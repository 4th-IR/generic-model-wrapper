{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://mis-app-service.mangobeach-c18b898d.switzerlandnorth.azurecontainerapps.io'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/api/v1/models?limit=100'\n",
    "\n",
    "response = re.get(URL+path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'model_identifier': 'sobamchan/st5-base-mean-8000',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:07:37',\n",
       "  'last_modified': '2025-02-27T17:08:25',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['sentence-transformers',\n",
       "   'safetensors',\n",
       "   't5',\n",
       "   'sentence-similarity',\n",
       "   'feature-extraction',\n",
       "   'generated_from_trainer',\n",
       "   'dataset_size:557850',\n",
       "   'loss:MultipleNegativesRankingLoss',\n",
       "   'en',\n",
       "   'dataset:sentence-transformers/all-nli',\n",
       "   'arxiv:1908.10084',\n",
       "   'arxiv:1705.00652',\n",
       "   'base_model:google-t5/t5-base',\n",
       "   'base_model:finetune:google-t5/t5-base',\n",
       "   'autotrain_compatible',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['sentence-similarity', 'feature-extraction'],\n",
       "  'architecture': 'T5EncoderModel',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.4083981513977051},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'description': '---\\nlanguage:\\n- en\\ntags:\\n- sentence-transformers\\n- sentence-similarity\\n- feature-extraction\\n- generated_from_trainer\\n- dataset_size:557850\\n- loss:MultipleNegativesRankingLoss\\nbase_model: google-t5/t5-base\\nwidget:\\n- source_sentence: A man is jumping unto his filthy bed.\\n  sentences:\\n  - A young male is looking at a newspaper while 2 females walks past him.\\n  - The bed is dirty.\\n  - The man is on the moon.\\n- source_sentence: A carefully balanced male stands on one foot near a clean ocean\\n    beach area.\\n  sentences:\\n  - A man is ouside near the beach.\\n  - Three policemen patrol the streets on bikes\\n  - A man is sitting on his couch.\\n- source_sentence: The man is wearing a blue shirt.\\n  sentences:\\n  - Near the trashcan the man stood and smoked\\n  - A man in a blue shirt leans on a wall beside a road with a blue van and red car\\n    with water in the background.\\n  - A man in a black shirt is playing a guitar.\\n- source_sentence: The girls are outdoors.\\n  sentences:\\n  - Two girls riding on an amusement part ride.\\n  - a guy laughs while doing laundry\\n  - Three girls are standing together in a room, one is listening, one is writing\\n    on a wall and the third is talking to them.\\n- source_sentence: A construction worker peeking out of a manhole while his coworker\\n    sits on the sidewalk smiling.\\n  sentences:\\n  - A worker is looking out of a manhole.\\n  - A man is giving a presentation.\\n  - The workers are both inside the manhole.\\ndatasets:\\n- sentence-transformers/all-nli\\npipeline_tag: sentence-similarity\\nlibrary_name: sentence-transformers\\n---\\n\\n# SentenceTransformer based on google-t5/t5-base\\n\\nThis is a [sentence-transformers](https://www.SBERT.net) model finetuned from [google-t5/t5-base](https://huggingface.co/google-t5/t5-base) on the [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli) dataset. It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.\\n\\n## Model Details\\n\\n### Model Description\\n- **Model Type:** Sentence Transformer\\n- **Base model:** [google-t5/t5-base](https://huggingface.co/google-t5/t5-base) <!-- at revision a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1 -->\\n- **Maximum Sequence Length:** 256 tokens\\n- **Output Dimensionality:** 768 dimensions\\n- **Similarity Function:** Cosine Similarity\\n- **Training Dataset:**\\n    - [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli)\\n- **Language:** en\\n<!-- - **License:** Unknown -->\\n\\n### Model Sources\\n\\n- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)\\n- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)\\n- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)\\n\\n### Full Model Architecture\\n\\n```\\nSentenceTransformer(\\n  (0): Transformer({\\'max_seq_length\\': 256, \\'do_lower_case\\': False}) with Transformer model: T5EncoderModel \\n  (1): Pooling({\\'word_embedding_dimension\\': 768, \\'pooling_mode_cls_token\\': False, \\'pooling_mode_mean_tokens\\': True, \\'pooling_mode_max_tokens\\': False, \\'pooling_mode_mean_sqrt_len_tokens\\': False, \\'pooling_mode_weightedmean_tokens\\': False, \\'pooling_mode_lasttoken\\': False, \\'include_prompt\\': True})\\n  (2): Normalize()\\n)\\n```\\n\\n## Usage\\n\\n### Direct Usage (Sentence Transformers)\\n\\nFirst install the Sentence Transformers library:\\n\\n```bash\\npip install -U sentence-transformers\\n```\\n\\nThen you can load this model and run inference.\\n```python\\nfrom sentence_transformers import SentenceTransformer\\n\\n# Download from the 🤗 Hub\\nmodel = SentenceTransformer(\"sentence_transformers_model_id\")\\n# Run inference\\nsentences = [\\n    \\'A construction worker peeking out of a manhole while his coworker sits on the sidewalk smiling.\\',\\n    \\'A worker is looking out of a manhole.\\',\\n    \\'The workers are both inside the manhole.\\',\\n]\\nembeddings = model.encode(sentences)\\nprint(embeddings.shape)\\n# [3, 768]\\n\\n# Get the similarity scores for the embeddings\\nsimilarities = model.similarity(embeddings, embeddings)\\nprint(similarities.shape)\\n# [3, 3]\\n```\\n\\n<!--\\n### Direct Usage (Transformers)\\n\\n<details><summary>Click to see the direct usage in Transformers</summary>\\n\\n</details>\\n-->\\n\\n<!--\\n### Downstream Usage (Sentence Transformers)\\n\\nYou can finetune this model on your own dataset.\\n\\n<details><summary>Click to expand</summary>\\n\\n</details>\\n-->\\n\\n<!--\\n### Out-of-Scope Use\\n\\n*List how the model may foreseeably be misused and address what users ought not to do with the model.*\\n-->\\n\\n<!--\\n## Bias, Risks and Limitations\\n\\n*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*\\n-->\\n\\n<!--\\n### Recommendations\\n\\n*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*\\n-->\\n\\n## Training Details\\n\\n### Training Dataset\\n\\n#### all-nli\\n\\n* Dataset: [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli) at [d482672](https://huggingface.co/datasets/sentence-transformers/all-nli/tree/d482672c8e74ce18da116f430137434ba2e52fab)\\n* Size: 557,850 training samples\\n* Columns: <code>anchor</code>, <code>positive</code>, and <code>negative</code>\\n* Approximate statistics based on the first 1000 samples:\\n  |         | anchor                                                                           | positive                                                                          | negative                                                                          |\\n  |:--------|:---------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|\\n  | type    | string                                                                           | string                                                                            | string                                                                            |\\n  | details | <ul><li>min: 6 tokens</li><li>mean: 9.96 tokens</li><li>max: 52 tokens</li></ul> | <ul><li>min: 5 tokens</li><li>mean: 12.79 tokens</li><li>max: 44 tokens</li></ul> | <ul><li>min: 4 tokens</li><li>mean: 14.02 tokens</li><li>max: 57 tokens</li></ul> |\\n* Samples:\\n  | anchor                                                                     | positive                                         | negative                                                   |\\n  |:---------------------------------------------------------------------------|:-------------------------------------------------|:-----------------------------------------------------------|\\n  | <code>A person on a horse jumps over a broken down airplane.</code>        | <code>A person is outdoors, on a horse.</code>   | <code>A person is at a diner, ordering an omelette.</code> |\\n  | <code>Children smiling and waving at camera</code>                         | <code>There are children present</code>          | <code>The kids are frowning</code>                         |\\n  | <code>A boy is jumping on skateboard in the middle of a red bridge.</code> | <code>The boy does a skateboarding trick.</code> | <code>The boy skates down the sidewalk.</code>             |\\n* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:\\n  ```json\\n  {\\n      \"scale\": 20.0,\\n      \"similarity_fct\": \"cos_sim\"\\n  }\\n  ```\\n\\n### Evaluation Dataset\\n\\n#### all-nli\\n\\n* Dataset: [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli) at [d482672](https://huggingface.co/datasets/sentence-transformers/all-nli/tree/d482672c8e74ce18da116f430137434ba2e52fab)\\n* Size: 6,584 evaluation samples\\n* Columns: <code>anchor</code>, <code>positive</code>, and <code>negative</code>\\n* Approximate statistics based on the first 1000 samples:\\n  |         | anchor                                                                            | positive                                                                         | negative                                                                          |\\n  |:--------|:----------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|\\n  | type    | string                                                                            | string                                                                           | string                                                                            |\\n  | details | <ul><li>min: 5 tokens</li><li>mean: 19.41 tokens</li><li>max: 79 tokens</li></ul> | <ul><li>min: 4 tokens</li><li>mean: 9.69 tokens</li><li>max: 35 tokens</li></ul> | <ul><li>min: 4 tokens</li><li>mean: 10.35 tokens</li><li>max: 30 tokens</li></ul> |\\n* Samples:\\n  | anchor                                                                                                                                                                         | positive                                                    | negative                                                |\\n  |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------|:--------------------------------------------------------|\\n  | <code>Two women are embracing while holding to go packages.</code>                                                                                                             | <code>Two woman are holding packages.</code>                | <code>The men are fighting outside a deli.</code>       |\\n  | <code>Two young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink.</code> | <code>Two kids in numbered jerseys wash their hands.</code> | <code>Two kids in jackets walk to school.</code>        |\\n  | <code>A man selling donuts to a customer during a world exhibition event held in the city of Angeles</code>                                                                    | <code>A man selling donuts to a customer.</code>            | <code>A woman drinks her coffee in a small cafe.</code> |\\n* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:\\n  ```json\\n  {\\n      \"scale\": 20.0,\\n      \"similarity_fct\": \"cos_sim\"\\n  }\\n  ```\\n\\n### Training Hyperparameters\\n#### Non-Default Hyperparameters\\n\\n- `eval_strategy`: steps\\n- `per_device_train_batch_size`: 64\\n- `per_device_eval_batch_size`: 64\\n- `learning_rate`: 1e-05\\n- `warmup_ratio`: 0.1\\n- `batch_sampler`: no_duplicates\\n\\n#### All Hyperparameters\\n<details><summary>Click to expand</summary>\\n\\n- `overwrite_output_dir`: False\\n- `do_predict`: False\\n- `eval_strategy`: steps\\n- `prediction_loss_only`: True\\n- `per_device_train_batch_size`: 64\\n- `per_device_eval_batch_size`: 64\\n- `per_gpu_train_batch_size`: None\\n- `per_gpu_eval_batch_size`: None\\n- `gradient_accumulation_steps`: 1\\n- `eval_accumulation_steps`: None\\n- `torch_empty_cache_steps`: None\\n- `learning_rate`: 1e-05\\n- `weight_decay`: 0.0\\n- `adam_beta1`: 0.9\\n- `adam_beta2`: 0.999\\n- `adam_epsilon`: 1e-08\\n- `max_grad_norm`: 1.0\\n- `num_train_epochs`: 3\\n- `max_steps`: -1\\n- `lr_scheduler_type`: linear\\n- `lr_scheduler_kwargs`: {}\\n- `warmup_ratio`: 0.1\\n- `warmup_steps`: 0\\n- `log_level`: passive\\n- `log_level_replica`: warning\\n- `log_on_each_node`: True\\n- `logging_nan_inf_filter`: True\\n- `save_safetensors`: True\\n- `save_on_each_node`: False\\n- `save_only_model`: False\\n- `restore_callback_states_from_checkpoint`: False\\n- `no_cuda`: False\\n- `use_cpu`: False\\n- `use_mps_device`: False\\n- `seed`: 42\\n- `data_seed`: None\\n- `jit_mode_eval`: False\\n- `use_ipex`: False\\n- `bf16`: False\\n- `fp16`: False\\n- `fp16_opt_level`: O1\\n- `half_precision_backend`: auto\\n- `bf16_full_eval`: False\\n- `fp16_full_eval`: False\\n- `tf32`: None\\n- `local_rank`: 0\\n- `ddp_backend`: None\\n- `tpu_num_cores`: None\\n- `tpu_metrics_debug`: False\\n- `debug`: []\\n- `dataloader_drop_last`: False\\n- `dataloader_num_workers`: 0\\n- `dataloader_prefetch_factor`: None\\n- `past_index`: -1\\n- `disable_tqdm`: False\\n- `remove_unused_columns`: True\\n- `label_names`: None\\n- `load_best_model_at_end`: False\\n- `ignore_data_skip`: False\\n- `fsdp`: []\\n- `fsdp_min_num_params`: 0\\n- `fsdp_config`: {\\'min_num_params\\': 0, \\'xla\\': False, \\'xla_fsdp_v2\\': False, \\'xla_fsdp_grad_ckpt\\': False}\\n- `fsdp_transformer_layer_cls_to_wrap`: None\\n- `accelerator_config`: {\\'split_batches\\': False, \\'dispatch_batches\\': None, \\'even_batches\\': True, \\'use_seedable_sampler\\': True, \\'non_blocking\\': False, \\'gradient_accumulation_kwargs\\': None}\\n- `deepspeed`: None\\n- `label_smoothing_factor`: 0.0\\n- `optim`: adamw_torch\\n- `optim_args`: None\\n- `adafactor`: False\\n- `group_by_length`: False\\n- `length_column_name`: length\\n- `ddp_find_unused_parameters`: None\\n- `ddp_bucket_cap_mb`: None\\n- `ddp_broadcast_buffers`: False\\n- `dataloader_pin_memory`: True\\n- `dataloader_persistent_workers`: False\\n- `skip_memory_metrics`: True\\n- `use_legacy_prediction_loop`: False\\n- `push_to_hub`: False\\n- `resume_from_checkpoint`: None\\n- `hub_model_id`: None\\n- `hub_strategy`: every_save\\n- `hub_private_repo`: None\\n- `hub_always_push`: False\\n- `gradient_checkpointing`: False\\n- `gradient_checkpointing_kwargs`: None\\n- `include_inputs_for_metrics`: False\\n- `include_for_metrics`: []\\n- `eval_do_concat_batches`: True\\n- `fp16_backend`: auto\\n- `push_to_hub_model_id`: None\\n- `push_to_hub_organization`: None\\n- `mp_parameters`: \\n- `auto_find_batch_size`: False\\n- `full_determinism`: False\\n- `torchdynamo`: None\\n- `ray_scope`: last\\n- `ddp_timeout`: 1800\\n- `torch_compile`: False\\n- `torch_compile_backend`: None\\n- `torch_compile_mode`: None\\n- `dispatch_batches`: None\\n- `split_batches`: None\\n- `include_tokens_per_second`: False\\n- `include_num_input_tokens_seen`: False\\n- `neftune_noise_alpha`: None\\n- `optim_target_modules`: None\\n- `batch_eval_metrics`: False\\n- `eval_on_start`: False\\n- `use_liger_kernel`: False\\n- `eval_use_gather_object`: False\\n- `average_tokens_across_devices`: False\\n- `prompts`: None\\n- `batch_sampler`: no_duplicates\\n- `multi_dataset_batch_sampler`: proportional\\n\\n</details>\\n\\n### Training Logs\\n<details><summary>Click to expand</summary>\\n\\n| Epoch  | Step | Training Loss | Validation Loss |\\n|:------:|:----:|:-------------:|:---------------:|\\n| 0.0011 | 10   | -             | 1.8733          |\\n| 0.0023 | 20   | -             | 1.8726          |\\n| 0.0034 | 30   | -             | 1.8714          |\\n| 0.0046 | 40   | -             | 1.8697          |\\n| 0.0057 | 50   | -             | 1.8675          |\\n| 0.0069 | 60   | -             | 1.8649          |\\n| 0.0080 | 70   | -             | 1.8619          |\\n| 0.0092 | 80   | -             | 1.8584          |\\n| 0.0103 | 90   | -             | 1.8544          |\\n| 0.0115 | 100  | 3.1046        | 1.8499          |\\n| 0.0126 | 110  | -             | 1.8451          |\\n| 0.0138 | 120  | -             | 1.8399          |\\n| 0.0149 | 130  | -             | 1.8343          |\\n| 0.0161 | 140  | -             | 1.8283          |\\n| 0.0172 | 150  | -             | 1.8223          |\\n| 0.0184 | 160  | -             | 1.8159          |\\n| 0.0195 | 170  | -             | 1.8091          |\\n| 0.0206 | 180  | -             | 1.8016          |\\n| 0.0218 | 190  | -             | 1.7938          |\\n| 0.0229 | 200  | 3.0303        | 1.7858          |\\n| 0.0241 | 210  | -             | 1.7775          |\\n| 0.0252 | 220  | -             | 1.7693          |\\n| 0.0264 | 230  | -             | 1.7605          |\\n| 0.0275 | 240  | -             | 1.7514          |\\n| 0.0287 | 250  | -             | 1.7417          |\\n| 0.0298 | 260  | -             | 1.7320          |\\n| 0.0310 | 270  | -             | 1.7227          |\\n| 0.0321 | 280  | -             | 1.7134          |\\n| 0.0333 | 290  | -             | 1.7040          |\\n| 0.0344 | 300  | 2.9459        | 1.6941          |\\n| 0.0356 | 310  | -             | 1.6833          |\\n| 0.0367 | 320  | -             | 1.6725          |\\n| 0.0379 | 330  | -             | 1.6614          |\\n| 0.0390 | 340  | -             | 1.6510          |\\n| 0.0402 | 350  | -             | 1.6402          |\\n| 0.0413 | 360  | -             | 1.6296          |\\n| 0.0424 | 370  | -             | 1.6187          |\\n| 0.0436 | 380  | -             | 1.6073          |\\n| 0.0447 | 390  | -             | 1.5962          |\\n| 0.0459 | 400  | 2.7813        | 1.5848          |\\n| 0.0470 | 410  | -             | 1.5735          |\\n| 0.0482 | 420  | -             | 1.5620          |\\n| 0.0493 | 430  | -             | 1.5495          |\\n| 0.0505 | 440  | -             | 1.5375          |\\n| 0.0516 | 450  | -             | 1.5256          |\\n| 0.0528 | 460  | -             | 1.5133          |\\n| 0.0539 | 470  | -             | 1.5012          |\\n| 0.0551 | 480  | -             | 1.4892          |\\n| 0.0562 | 490  | -             | 1.4769          |\\n| 0.0574 | 500  | 2.6308        | 1.4640          |\\n| 0.0585 | 510  | -             | 1.4513          |\\n| 0.0597 | 520  | -             | 1.4391          |\\n| 0.0608 | 530  | -             | 1.4262          |\\n| 0.0619 | 540  | -             | 1.4130          |\\n| 0.0631 | 550  | -             | 1.3998          |\\n| 0.0642 | 560  | -             | 1.3874          |\\n| 0.0654 | 570  | -             | 1.3752          |\\n| 0.0665 | 580  | -             | 1.3620          |\\n| 0.0677 | 590  | -             | 1.3485          |\\n| 0.0688 | 600  | 2.4452        | 1.3350          |\\n| 0.0700 | 610  | -             | 1.3213          |\\n| 0.0711 | 620  | -             | 1.3088          |\\n| 0.0723 | 630  | -             | 1.2965          |\\n| 0.0734 | 640  | -             | 1.2839          |\\n| 0.0746 | 650  | -             | 1.2713          |\\n| 0.0757 | 660  | -             | 1.2592          |\\n| 0.0769 | 670  | -             | 1.2466          |\\n| 0.0780 | 680  | -             | 1.2332          |\\n| 0.0792 | 690  | -             | 1.2203          |\\n| 0.0803 | 700  | 2.2626        | 1.2077          |\\n| 0.0815 | 710  | -             | 1.1959          |\\n| 0.0826 | 720  | -             | 1.1841          |\\n| 0.0837 | 730  | -             | 1.1725          |\\n| 0.0849 | 740  | -             | 1.1619          |\\n| 0.0860 | 750  | -             | 1.1516          |\\n| 0.0872 | 760  | -             | 1.1416          |\\n| 0.0883 | 770  | -             | 1.1320          |\\n| 0.0895 | 780  | -             | 1.1227          |\\n| 0.0906 | 790  | -             | 1.1138          |\\n| 0.0918 | 800  | 2.0044        | 1.1053          |\\n| 0.0929 | 810  | -             | 1.0965          |\\n| 0.0941 | 820  | -             | 1.0879          |\\n| 0.0952 | 830  | -             | 1.0796          |\\n| 0.0964 | 840  | -             | 1.0718          |\\n| 0.0975 | 850  | -             | 1.0644          |\\n| 0.0987 | 860  | -             | 1.0564          |\\n| 0.0998 | 870  | -             | 1.0490          |\\n| 0.1010 | 880  | -             | 1.0417          |\\n| 0.1021 | 890  | -             | 1.0354          |\\n| 0.1032 | 900  | 1.8763        | 1.0296          |\\n| 0.1044 | 910  | -             | 1.0239          |\\n| 0.1055 | 920  | -             | 1.0180          |\\n| 0.1067 | 930  | -             | 1.0123          |\\n| 0.1078 | 940  | -             | 1.0065          |\\n| 0.1090 | 950  | -             | 1.0008          |\\n| 0.1101 | 960  | -             | 0.9950          |\\n| 0.1113 | 970  | -             | 0.9894          |\\n| 0.1124 | 980  | -             | 0.9840          |\\n| 0.1136 | 990  | -             | 0.9793          |\\n| 0.1147 | 1000 | 1.7287        | 0.9752          |\\n| 0.1159 | 1010 | -             | 0.9706          |\\n| 0.1170 | 1020 | -             | 0.9659          |\\n| 0.1182 | 1030 | -             | 0.9615          |\\n| 0.1193 | 1040 | -             | 0.9572          |\\n| 0.1205 | 1050 | -             | 0.9531          |\\n| 0.1216 | 1060 | -             | 0.9494          |\\n| 0.1227 | 1070 | -             | 0.9456          |\\n| 0.1239 | 1080 | -             | 0.9415          |\\n| 0.1250 | 1090 | -             | 0.9377          |\\n| 0.1262 | 1100 | 1.6312        | 0.9339          |\\n| 0.1273 | 1110 | -             | 0.9303          |\\n| 0.1285 | 1120 | -             | 0.9267          |\\n| 0.1296 | 1130 | -             | 0.9232          |\\n| 0.1308 | 1140 | -             | 0.9197          |\\n| 0.1319 | 1150 | -             | 0.9162          |\\n| 0.1331 | 1160 | -             | 0.9128          |\\n| 0.1342 | 1170 | -             | 0.9097          |\\n| 0.1354 | 1180 | -             | 0.9069          |\\n| 0.1365 | 1190 | -             | 0.9040          |\\n| 0.1377 | 1200 | 1.5316        | 0.9010          |\\n| 0.1388 | 1210 | -             | 0.8979          |\\n| 0.1400 | 1220 | -             | 0.8947          |\\n| 0.1411 | 1230 | -             | 0.8915          |\\n| 0.1423 | 1240 | -             | 0.8888          |\\n| 0.1434 | 1250 | -             | 0.8861          |\\n| 0.1445 | 1260 | -             | 0.8833          |\\n| 0.1457 | 1270 | -             | 0.8806          |\\n| 0.1468 | 1280 | -             | 0.8779          |\\n| 0.1480 | 1290 | -             | 0.8748          |\\n| 0.1491 | 1300 | 1.4961        | 0.8718          |\\n| 0.1503 | 1310 | -             | 0.8690          |\\n| 0.1514 | 1320 | -             | 0.8664          |\\n| 0.1526 | 1330 | -             | 0.8635          |\\n| 0.1537 | 1340 | -             | 0.8603          |\\n| 0.1549 | 1350 | -             | 0.8574          |\\n| 0.1560 | 1360 | -             | 0.8545          |\\n| 0.1572 | 1370 | -             | 0.8521          |\\n| 0.1583 | 1380 | -             | 0.8497          |\\n| 0.1595 | 1390 | -             | 0.8474          |\\n| 0.1606 | 1400 | 1.451         | 0.8453          |\\n| 0.1618 | 1410 | -             | 0.8429          |\\n| 0.1629 | 1420 | -             | 0.8404          |\\n| 0.1640 | 1430 | -             | 0.8380          |\\n| 0.1652 | 1440 | -             | 0.8357          |\\n| 0.1663 | 1450 | -             | 0.8336          |\\n| 0.1675 | 1460 | -             | 0.8312          |\\n| 0.1686 | 1470 | -             | 0.8289          |\\n| 0.1698 | 1480 | -             | 0.8262          |\\n| 0.1709 | 1490 | -             | 0.8236          |\\n| 0.1721 | 1500 | 1.4177        | 0.8213          |\\n| 0.1732 | 1510 | -             | 0.8189          |\\n| 0.1744 | 1520 | -             | 0.8168          |\\n| 0.1755 | 1530 | -             | 0.8147          |\\n| 0.1767 | 1540 | -             | 0.8127          |\\n| 0.1778 | 1550 | -             | 0.8107          |\\n| 0.1790 | 1560 | -             | 0.8082          |\\n| 0.1801 | 1570 | -             | 0.8059          |\\n| 0.1813 | 1580 | -             | 0.8036          |\\n| 0.1824 | 1590 | -             | 0.8015          |\\n| 0.1835 | 1600 | 1.3734        | 0.7993          |\\n| 0.1847 | 1610 | -             | 0.7970          |\\n| 0.1858 | 1620 | -             | 0.7948          |\\n| 0.1870 | 1630 | -             | 0.7922          |\\n| 0.1881 | 1640 | -             | 0.7900          |\\n| 0.1893 | 1650 | -             | 0.7877          |\\n| 0.1904 | 1660 | -             | 0.7852          |\\n| 0.1916 | 1670 | -             | 0.7829          |\\n| 0.1927 | 1680 | -             | 0.7804          |\\n| 0.1939 | 1690 | -             | 0.7779          |\\n| 0.1950 | 1700 | 1.3327        | 0.7757          |\\n| 0.1962 | 1710 | -             | 0.7738          |\\n| 0.1973 | 1720 | -             | 0.7719          |\\n| 0.1985 | 1730 | -             | 0.7700          |\\n| 0.1996 | 1740 | -             | 0.7679          |\\n| 0.2008 | 1750 | -             | 0.7658          |\\n| 0.2019 | 1760 | -             | 0.7641          |\\n| 0.2031 | 1770 | -             | 0.7621          |\\n| 0.2042 | 1780 | -             | 0.7601          |\\n| 0.2053 | 1790 | -             | 0.7580          |\\n| 0.2065 | 1800 | 1.2804        | 0.7558          |\\n| 0.2076 | 1810 | -             | 0.7536          |\\n| 0.2088 | 1820 | -             | 0.7514          |\\n| 0.2099 | 1830 | -             | 0.7493          |\\n| 0.2111 | 1840 | -             | 0.7473          |\\n| 0.2122 | 1850 | -             | 0.7451          |\\n| 0.2134 | 1860 | -             | 0.7429          |\\n| 0.2145 | 1870 | -             | 0.7408          |\\n| 0.2157 | 1880 | -             | 0.7389          |\\n| 0.2168 | 1890 | -             | 0.7368          |\\n| 0.2180 | 1900 | 1.2255        | 0.7349          |\\n| 0.2191 | 1910 | -             | 0.7328          |\\n| 0.2203 | 1920 | -             | 0.7310          |\\n| 0.2214 | 1930 | -             | 0.7293          |\\n| 0.2226 | 1940 | -             | 0.7277          |\\n| 0.2237 | 1950 | -             | 0.7259          |\\n| 0.2248 | 1960 | -             | 0.7240          |\\n| 0.2260 | 1970 | -             | 0.7221          |\\n| 0.2271 | 1980 | -             | 0.7203          |\\n| 0.2283 | 1990 | -             | 0.7184          |\\n| 0.2294 | 2000 | 1.2635        | 0.7165          |\\n| 0.2306 | 2010 | -             | 0.7150          |\\n| 0.2317 | 2020 | -             | 0.7135          |\\n| 0.2329 | 2030 | -             | 0.7117          |\\n| 0.2340 | 2040 | -             | 0.7099          |\\n| 0.2352 | 2050 | -             | 0.7084          |\\n| 0.2363 | 2060 | -             | 0.7068          |\\n| 0.2375 | 2070 | -             | 0.7054          |\\n| 0.2386 | 2080 | -             | 0.7037          |\\n| 0.2398 | 2090 | -             | 0.7023          |\\n| 0.2409 | 2100 | 1.1912        | 0.7009          |\\n| 0.2421 | 2110 | -             | 0.6991          |\\n| 0.2432 | 2120 | -             | 0.6974          |\\n| 0.2444 | 2130 | -             | 0.6962          |\\n| 0.2455 | 2140 | -             | 0.6950          |\\n| 0.2466 | 2150 | -             | 0.6938          |\\n| 0.2478 | 2160 | -             | 0.6922          |\\n| 0.2489 | 2170 | -             | 0.6909          |\\n| 0.2501 | 2180 | -             | 0.6897          |\\n| 0.2512 | 2190 | -             | 0.6884          |\\n| 0.2524 | 2200 | 1.2144        | 0.6868          |\\n| 0.2535 | 2210 | -             | 0.6856          |\\n| 0.2547 | 2220 | -             | 0.6843          |\\n| 0.2558 | 2230 | -             | 0.6829          |\\n| 0.2570 | 2240 | -             | 0.6817          |\\n| 0.2581 | 2250 | -             | 0.6804          |\\n| 0.2593 | 2260 | -             | 0.6789          |\\n| 0.2604 | 2270 | -             | 0.6775          |\\n| 0.2616 | 2280 | -             | 0.6763          |\\n| 0.2627 | 2290 | -             | 0.6751          |\\n| 0.2639 | 2300 | 1.1498        | 0.6739          |\\n| 0.2650 | 2310 | -             | 0.6725          |\\n| 0.2661 | 2320 | -             | 0.6711          |\\n| 0.2673 | 2330 | -             | 0.6698          |\\n| 0.2684 | 2340 | -             | 0.6684          |\\n| 0.2696 | 2350 | -             | 0.6666          |\\n| 0.2707 | 2360 | -             | 0.6653          |\\n| 0.2719 | 2370 | -             | 0.6638          |\\n| 0.2730 | 2380 | -             | 0.6621          |\\n| 0.2742 | 2390 | -             | 0.6609          |\\n| 0.2753 | 2400 | 1.1446        | 0.6596          |\\n| 0.2765 | 2410 | -             | 0.6582          |\\n| 0.2776 | 2420 | -             | 0.6568          |\\n| 0.2788 | 2430 | -             | 0.6553          |\\n| 0.2799 | 2440 | -             | 0.6541          |\\n| 0.2811 | 2450 | -             | 0.6527          |\\n| 0.2822 | 2460 | -             | 0.6513          |\\n| 0.2834 | 2470 | -             | 0.6496          |\\n| 0.2845 | 2480 | -             | 0.6483          |\\n| 0.2856 | 2490 | -             | 0.6475          |\\n| 0.2868 | 2500 | 1.1309        | 0.6465          |\\n| 0.2879 | 2510 | -             | 0.6455          |\\n| 0.2891 | 2520 | -             | 0.6447          |\\n| 0.2902 | 2530 | -             | 0.6437          |\\n| 0.2914 | 2540 | -             | 0.6428          |\\n| 0.2925 | 2550 | -             | 0.6415          |\\n| 0.2937 | 2560 | -             | 0.6403          |\\n| 0.2948 | 2570 | -             | 0.6392          |\\n| 0.2960 | 2580 | -             | 0.6381          |\\n| 0.2971 | 2590 | -             | 0.6371          |\\n| 0.2983 | 2600 | 1.1006        | 0.6358          |\\n| 0.2994 | 2610 | -             | 0.6348          |\\n| 0.3006 | 2620 | -             | 0.6340          |\\n| 0.3017 | 2630 | -             | 0.6330          |\\n| 0.3029 | 2640 | -             | 0.6319          |\\n| 0.3040 | 2650 | -             | 0.6308          |\\n| 0.3052 | 2660 | -             | 0.6300          |\\n| 0.3063 | 2670 | -             | 0.6291          |\\n| 0.3074 | 2680 | -             | 0.6280          |\\n| 0.3086 | 2690 | -             | 0.6268          |\\n| 0.3097 | 2700 | 1.0772        | 0.6254          |\\n| 0.3109 | 2710 | -             | 0.6243          |\\n| 0.3120 | 2720 | -             | 0.6232          |\\n| 0.3132 | 2730 | -             | 0.6224          |\\n| 0.3143 | 2740 | -             | 0.6215          |\\n| 0.3155 | 2750 | -             | 0.6205          |\\n| 0.3166 | 2760 | -             | 0.6194          |\\n| 0.3178 | 2770 | -             | 0.6183          |\\n| 0.3189 | 2780 | -             | 0.6171          |\\n| 0.3201 | 2790 | -             | 0.6160          |\\n| 0.3212 | 2800 | 1.0648        | 0.6153          |\\n| 0.3224 | 2810 | -             | 0.6141          |\\n| 0.3235 | 2820 | -             | 0.6129          |\\n| 0.3247 | 2830 | -             | 0.6119          |\\n| 0.3258 | 2840 | -             | 0.6109          |\\n| 0.3269 | 2850 | -             | 0.6099          |\\n| 0.3281 | 2860 | -             | 0.6088          |\\n| 0.3292 | 2870 | -             | 0.6079          |\\n| 0.3304 | 2880 | -             | 0.6073          |\\n| 0.3315 | 2890 | -             | 0.6063          |\\n| 0.3327 | 2900 | 1.0398        | 0.6054          |\\n| 0.3338 | 2910 | -             | 0.6044          |\\n| 0.3350 | 2920 | -             | 0.6033          |\\n| 0.3361 | 2930 | -             | 0.6022          |\\n| 0.3373 | 2940 | -             | 0.6012          |\\n| 0.3384 | 2950 | -             | 0.6003          |\\n| 0.3396 | 2960 | -             | 0.5993          |\\n| 0.3407 | 2970 | -             | 0.5986          |\\n| 0.3419 | 2980 | -             | 0.5978          |\\n| 0.3430 | 2990 | -             | 0.5967          |\\n| 0.3442 | 3000 | 1.0256        | 0.5959          |\\n| 0.3453 | 3010 | -             | 0.5947          |\\n| 0.3464 | 3020 | -             | 0.5937          |\\n| 0.3476 | 3030 | -             | 0.5929          |\\n| 0.3487 | 3040 | -             | 0.5920          |\\n| 0.3499 | 3050 | -             | 0.5908          |\\n| 0.3510 | 3060 | -             | 0.5897          |\\n| 0.3522 | 3070 | -             | 0.5888          |\\n| 0.3533 | 3080 | -             | 0.5882          |\\n| 0.3545 | 3090 | -             | 0.5874          |\\n| 0.3556 | 3100 | 1.0489        | 0.5868          |\\n| 0.3568 | 3110 | -             | 0.5860          |\\n| 0.3579 | 3120 | -             | 0.5854          |\\n| 0.3591 | 3130 | -             | 0.5839          |\\n| 0.3602 | 3140 | -             | 0.5830          |\\n| 0.3614 | 3150 | -             | 0.5822          |\\n| 0.3625 | 3160 | -             | 0.5814          |\\n| 0.3637 | 3170 | -             | 0.5808          |\\n| 0.3648 | 3180 | -             | 0.5802          |\\n| 0.3660 | 3190 | -             | 0.5794          |\\n| 0.3671 | 3200 | 1.038         | 0.5788          |\\n| 0.3682 | 3210 | -             | 0.5778          |\\n| 0.3694 | 3220 | -             | 0.5770          |\\n| 0.3705 | 3230 | -             | 0.5763          |\\n| 0.3717 | 3240 | -             | 0.5752          |\\n| 0.3728 | 3250 | -             | 0.5745          |\\n| 0.3740 | 3260 | -             | 0.5737          |\\n| 0.3751 | 3270 | -             | 0.5728          |\\n| 0.3763 | 3280 | -             | 0.5720          |\\n| 0.3774 | 3290 | -             | 0.5713          |\\n| 0.3786 | 3300 | 1.0058        | 0.5707          |\\n| 0.3797 | 3310 | -             | 0.5700          |\\n| 0.3809 | 3320 | -             | 0.5690          |\\n| 0.3820 | 3330 | -             | 0.5681          |\\n| 0.3832 | 3340 | -             | 0.5673          |\\n| 0.3843 | 3350 | -             | 0.5669          |\\n| 0.3855 | 3360 | -             | 0.5667          |\\n| 0.3866 | 3370 | -             | 0.5665          |\\n| 0.3877 | 3380 | -             | 0.5659          |\\n| 0.3889 | 3390 | -             | 0.5650          |\\n| 0.3900 | 3400 | 1.0413        | 0.5645          |\\n| 0.3912 | 3410 | -             | 0.5641          |\\n| 0.3923 | 3420 | -             | 0.5635          |\\n| 0.3935 | 3430 | -             | 0.5629          |\\n| 0.3946 | 3440 | -             | 0.5622          |\\n| 0.3958 | 3450 | -             | 0.5617          |\\n| 0.3969 | 3460 | -             | 0.5614          |\\n| 0.3981 | 3470 | -             | 0.5607          |\\n| 0.3992 | 3480 | -             | 0.5603          |\\n| 0.4004 | 3490 | -             | 0.5598          |\\n| 0.4015 | 3500 | 0.938         | 0.5596          |\\n| 0.4027 | 3510 | -             | 0.5589          |\\n| 0.4038 | 3520 | -             | 0.5581          |\\n| 0.4050 | 3530 | -             | 0.5571          |\\n| 0.4061 | 3540 | -             | 0.5563          |\\n| 0.4073 | 3550 | -             | 0.5557          |\\n| 0.4084 | 3560 | -             | 0.5551          |\\n| 0.4095 | 3570 | -             | 0.5546          |\\n| 0.4107 | 3580 | -             | 0.5541          |\\n| 0.4118 | 3590 | -             | 0.5535          |\\n| 0.4130 | 3600 | 0.955         | 0.5528          |\\n| 0.4141 | 3610 | -             | 0.5522          |\\n| 0.4153 | 3620 | -             | 0.5516          |\\n| 0.4164 | 3630 | -             | 0.5509          |\\n| 0.4176 | 3640 | -             | 0.5503          |\\n| 0.4187 | 3650 | -             | 0.5495          |\\n| 0.4199 | 3660 | -             | 0.5490          |\\n| 0.4210 | 3670 | -             | 0.5481          |\\n| 0.4222 | 3680 | -             | 0.5475          |\\n| 0.4233 | 3690 | -             | 0.5467          |\\n| 0.4245 | 3700 | 0.9387        | 0.5463          |\\n| 0.4256 | 3710 | -             | 0.5459          |\\n| 0.4268 | 3720 | -             | 0.5452          |\\n| 0.4279 | 3730 | -             | 0.5448          |\\n| 0.4290 | 3740 | -             | 0.5443          |\\n| 0.4302 | 3750 | -             | 0.5440          |\\n| 0.4313 | 3760 | -             | 0.5435          |\\n| 0.4325 | 3770 | -             | 0.5430          |\\n| 0.4336 | 3780 | -             | 0.5423          |\\n| 0.4348 | 3790 | -             | 0.5418          |\\n| 0.4359 | 3800 | 0.9672        | 0.5415          |\\n| 0.4371 | 3810 | -             | 0.5413          |\\n| 0.4382 | 3820 | -             | 0.5410          |\\n| 0.4394 | 3830 | -             | 0.5406          |\\n| 0.4405 | 3840 | -             | 0.5403          |\\n| 0.4417 | 3850 | -             | 0.5397          |\\n| 0.4428 | 3860 | -             | 0.5394          |\\n| 0.4440 | 3870 | -             | 0.5386          |\\n| 0.4451 | 3880 | -             | 0.5378          |\\n| 0.4463 | 3890 | -             | 0.5370          |\\n| 0.4474 | 3900 | 0.926         | 0.5360          |\\n| 0.4485 | 3910 | -             | 0.5351          |\\n| 0.4497 | 3920 | -             | 0.5346          |\\n| 0.4508 | 3930 | -             | 0.5343          |\\n| 0.4520 | 3940 | -             | 0.5339          |\\n| 0.4531 | 3950 | -             | 0.5337          |\\n| 0.4543 | 3960 | -             | 0.5334          |\\n| 0.4554 | 3970 | -             | 0.5330          |\\n| 0.4566 | 3980 | -             | 0.5327          |\\n| 0.4577 | 3990 | -             | 0.5324          |\\n| 0.4589 | 4000 | 0.867         | 0.5319          |\\n| 0.4600 | 4010 | -             | 0.5313          |\\n| 0.4612 | 4020 | -             | 0.5308          |\\n| 0.4623 | 4030 | -             | 0.5300          |\\n| 0.4635 | 4040 | -             | 0.5293          |\\n| 0.4646 | 4050 | -             | 0.5287          |\\n| 0.4658 | 4060 | -             | 0.5284          |\\n| 0.4669 | 4070 | -             | 0.5281          |\\n| 0.4681 | 4080 | -             | 0.5277          |\\n| 0.4692 | 4090 | -             | 0.5272          |\\n| 0.4703 | 4100 | 0.916         | 0.5267          |\\n| 0.4715 | 4110 | -             | 0.5260          |\\n| 0.4726 | 4120 | -             | 0.5252          |\\n| 0.4738 | 4130 | -             | 0.5246          |\\n| 0.4749 | 4140 | -             | 0.5239          |\\n| 0.4761 | 4150 | -             | 0.5232          |\\n| 0.4772 | 4160 | -             | 0.5225          |\\n| 0.4784 | 4170 | -             | 0.5221          |\\n| 0.4795 | 4180 | -             | 0.5216          |\\n| 0.4807 | 4190 | -             | 0.5211          |\\n| 0.4818 | 4200 | 0.9667        | 0.5206          |\\n| 0.4830 | 4210 | -             | 0.5204          |\\n| 0.4841 | 4220 | -             | 0.5200          |\\n| 0.4853 | 4230 | -             | 0.5192          |\\n| 0.4864 | 4240 | -             | 0.5187          |\\n| 0.4876 | 4250 | -             | 0.5185          |\\n| 0.4887 | 4260 | -             | 0.5179          |\\n| 0.4898 | 4270 | -             | 0.5173          |\\n| 0.4910 | 4280 | -             | 0.5170          |\\n| 0.4921 | 4290 | -             | 0.5165          |\\n| 0.4933 | 4300 | 0.9276        | 0.5160          |\\n| 0.4944 | 4310 | -             | 0.5154          |\\n| 0.4956 | 4320 | -             | 0.5150          |\\n| 0.4967 | 4330 | -             | 0.5144          |\\n| 0.4979 | 4340 | -             | 0.5141          |\\n| 0.4990 | 4350 | -             | 0.5139          |\\n| 0.5002 | 4360 | -             | 0.5138          |\\n| 0.5013 | 4370 | -             | 0.5136          |\\n| 0.5025 | 4380 | -             | 0.5133          |\\n| 0.5036 | 4390 | -             | 0.5129          |\\n| 0.5048 | 4400 | 0.9331        | 0.5126          |\\n| 0.5059 | 4410 | -             | 0.5123          |\\n| 0.5071 | 4420 | -             | 0.5117          |\\n| 0.5082 | 4430 | -             | 0.5113          |\\n| 0.5093 | 4440 | -             | 0.5108          |\\n| 0.5105 | 4450 | -             | 0.5106          |\\n| 0.5116 | 4460 | -             | 0.5106          |\\n| 0.5128 | 4470 | -             | 0.5106          |\\n| 0.5139 | 4480 | -             | 0.5104          |\\n| 0.5151 | 4490 | -             | 0.5102          |\\n| 0.5162 | 4500 | 0.907         | 0.5097          |\\n| 0.5174 | 4510 | -             | 0.5092          |\\n| 0.5185 | 4520 | -             | 0.5086          |\\n| 0.5197 | 4530 | -             | 0.5082          |\\n| 0.5208 | 4540 | -             | 0.5079          |\\n| 0.5220 | 4550 | -             | 0.5075          |\\n| 0.5231 | 4560 | -             | 0.5071          |\\n| 0.5243 | 4570 | -             | 0.5067          |\\n| 0.5254 | 4580 | -             | 0.5066          |\\n| 0.5266 | 4590 | -             | 0.5062          |\\n| 0.5277 | 4600 | 0.913         | 0.5059          |\\n| 0.5289 | 4610 | -             | 0.5056          |\\n| 0.5300 | 4620 | -             | 0.5052          |\\n| 0.5311 | 4630 | -             | 0.5046          |\\n| 0.5323 | 4640 | -             | 0.5039          |\\n| 0.5334 | 4650 | -             | 0.5033          |\\n| 0.5346 | 4660 | -             | 0.5030          |\\n| 0.5357 | 4670 | -             | 0.5028          |\\n| 0.5369 | 4680 | -             | 0.5027          |\\n| 0.5380 | 4690 | -             | 0.5023          |\\n| 0.5392 | 4700 | 0.9047        | 0.5020          |\\n| 0.5403 | 4710 | -             | 0.5018          |\\n| 0.5415 | 4720 | -             | 0.5015          |\\n| 0.5426 | 4730 | -             | 0.5009          |\\n| 0.5438 | 4740 | -             | 0.5003          |\\n| 0.5449 | 4750 | -             | 0.4997          |\\n| 0.5461 | 4760 | -             | 0.4991          |\\n| 0.5472 | 4770 | -             | 0.4984          |\\n| 0.5484 | 4780 | -             | 0.4980          |\\n| 0.5495 | 4790 | -             | 0.4980          |\\n| 0.5506 | 4800 | 0.887         | 0.4979          |\\n| 0.5518 | 4810 | -             | 0.4975          |\\n| 0.5529 | 4820 | -             | 0.4973          |\\n| 0.5541 | 4830 | -             | 0.4969          |\\n| 0.5552 | 4840 | -             | 0.4966          |\\n| 0.5564 | 4850 | -             | 0.4964          |\\n| 0.5575 | 4860 | -             | 0.4964          |\\n| 0.5587 | 4870 | -             | 0.4960          |\\n| 0.5598 | 4880 | -             | 0.4957          |\\n| 0.5610 | 4890 | -             | 0.4955          |\\n| 0.5621 | 4900 | 0.8645        | 0.4952          |\\n| 0.5633 | 4910 | -             | 0.4950          |\\n| 0.5644 | 4920 | -             | 0.4952          |\\n| 0.5656 | 4930 | -             | 0.4949          |\\n| 0.5667 | 4940 | -             | 0.4943          |\\n| 0.5679 | 4950 | -             | 0.4938          |\\n| 0.5690 | 4960 | -             | 0.4936          |\\n| 0.5702 | 4970 | -             | 0.4933          |\\n| 0.5713 | 4980 | -             | 0.4931          |\\n| 0.5724 | 4990 | -             | 0.4929          |\\n| 0.5736 | 5000 | 0.8348        | 0.4924          |\\n| 0.5747 | 5010 | -             | 0.4921          |\\n| 0.5759 | 5020 | -             | 0.4915          |\\n| 0.5770 | 5030 | -             | 0.4911          |\\n| 0.5782 | 5040 | -             | 0.4909          |\\n| 0.5793 | 5050 | -             | 0.4905          |\\n| 0.5805 | 5060 | -             | 0.4900          |\\n| 0.5816 | 5070 | -             | 0.4892          |\\n| 0.5828 | 5080 | -             | 0.4886          |\\n| 0.5839 | 5090 | -             | 0.4883          |\\n| 0.5851 | 5100 | 0.871         | 0.4879          |\\n| 0.5862 | 5110 | -             | 0.4877          |\\n| 0.5874 | 5120 | -             | 0.4874          |\\n| 0.5885 | 5130 | -             | 0.4870          |\\n| 0.5897 | 5140 | -             | 0.4867          |\\n| 0.5908 | 5150 | -             | 0.4864          |\\n| 0.5919 | 5160 | -             | 0.4862          |\\n| 0.5931 | 5170 | -             | 0.4860          |\\n| 0.5942 | 5180 | -             | 0.4857          |\\n| 0.5954 | 5190 | -             | 0.4855          |\\n| 0.5965 | 5200 | 0.8522        | 0.4850          |\\n| 0.5977 | 5210 | -             | 0.4846          |\\n| 0.5988 | 5220 | -             | 0.4844          |\\n| 0.6000 | 5230 | -             | 0.4842          |\\n| 0.6011 | 5240 | -             | 0.4837          |\\n| 0.6023 | 5250 | -             | 0.4835          |\\n| 0.6034 | 5260 | -             | 0.4831          |\\n| 0.6046 | 5270 | -             | 0.4826          |\\n| 0.6057 | 5280 | -             | 0.4822          |\\n| 0.6069 | 5290 | -             | 0.4822          |\\n| 0.6080 | 5300 | 0.869         | 0.4820          |\\n| 0.6092 | 5310 | -             | 0.4818          |\\n| 0.6103 | 5320 | -             | 0.4819          |\\n| 0.6114 | 5330 | -             | 0.4819          |\\n| 0.6126 | 5340 | -             | 0.4815          |\\n| 0.6137 | 5350 | -             | 0.4813          |\\n| 0.6149 | 5360 | -             | 0.4812          |\\n| 0.6160 | 5370 | -             | 0.4810          |\\n| 0.6172 | 5380 | -             | 0.4809          |\\n| 0.6183 | 5390 | -             | 0.4806          |\\n| 0.6195 | 5400 | 0.8548        | 0.4805          |\\n| 0.6206 | 5410 | -             | 0.4800          |\\n| 0.6218 | 5420 | -             | 0.4798          |\\n| 0.6229 | 5430 | -             | 0.4795          |\\n| 0.6241 | 5440 | -             | 0.4792          |\\n| 0.6252 | 5450 | -             | 0.4790          |\\n| 0.6264 | 5460 | -             | 0.4790          |\\n| 0.6275 | 5470 | -             | 0.4791          |\\n| 0.6287 | 5480 | -             | 0.4794          |\\n| 0.6298 | 5490 | -             | 0.4792          |\\n| 0.6310 | 5500 | 0.8366        | 0.4790          |\\n| 0.6321 | 5510 | -             | 0.4786          |\\n| 0.6332 | 5520 | -             | 0.4780          |\\n| 0.6344 | 5530 | -             | 0.4773          |\\n| 0.6355 | 5540 | -             | 0.4768          |\\n| 0.6367 | 5550 | -             | 0.4767          |\\n| 0.6378 | 5560 | -             | 0.4765          |\\n| 0.6390 | 5570 | -             | 0.4765          |\\n| 0.6401 | 5580 | -             | 0.4763          |\\n| 0.6413 | 5590 | -             | 0.4760          |\\n| 0.6424 | 5600 | 0.8696        | 0.4757          |\\n| 0.6436 | 5610 | -             | 0.4754          |\\n| 0.6447 | 5620 | -             | 0.4752          |\\n| 0.6459 | 5630 | -             | 0.4751          |\\n| 0.6470 | 5640 | -             | 0.4747          |\\n| 0.6482 | 5650 | -             | 0.4747          |\\n| 0.6493 | 5660 | -             | 0.4742          |\\n| 0.6505 | 5670 | -             | 0.4740          |\\n| 0.6516 | 5680 | -             | 0.4736          |\\n| 0.6527 | 5690 | -             | 0.4730          |\\n| 0.6539 | 5700 | 0.8302        | 0.4725          |\\n| 0.6550 | 5710 | -             | 0.4723          |\\n| 0.6562 | 5720 | -             | 0.4720          |\\n| 0.6573 | 5730 | -             | 0.4718          |\\n| 0.6585 | 5740 | -             | 0.4715          |\\n| 0.6596 | 5750 | -             | 0.4714          |\\n| 0.6608 | 5760 | -             | 0.4711          |\\n| 0.6619 | 5770 | -             | 0.4707          |\\n| 0.6631 | 5780 | -             | 0.4707          |\\n| 0.6642 | 5790 | -             | 0.4703          |\\n| 0.6654 | 5800 | 0.8128        | 0.4703          |\\n| 0.6665 | 5810 | -             | 0.4701          |\\n| 0.6677 | 5820 | -             | 0.4699          |\\n| 0.6688 | 5830 | -             | 0.4697          |\\n| 0.6700 | 5840 | -             | 0.4698          |\\n| 0.6711 | 5850 | -             | 0.4695          |\\n| 0.6722 | 5860 | -             | 0.4691          |\\n| 0.6734 | 5870 | -             | 0.4689          |\\n| 0.6745 | 5880 | -             | 0.4689          |\\n| 0.6757 | 5890 | -             | 0.4688          |\\n| 0.6768 | 5900 | 0.8437        | 0.4683          |\\n| 0.6780 | 5910 | -             | 0.4683          |\\n| 0.6791 | 5920 | -             | 0.4681          |\\n| 0.6803 | 5930 | -             | 0.4678          |\\n| 0.6814 | 5940 | -             | 0.4677          |\\n| 0.6826 | 5950 | -             | 0.4676          |\\n| 0.6837 | 5960 | -             | 0.4673          |\\n| 0.6849 | 5970 | -             | 0.4668          |\\n| 0.6860 | 5980 | -             | 0.4667          |\\n| 0.6872 | 5990 | -             | 0.4661          |\\n| 0.6883 | 6000 | 0.7774        | 0.4657          |\\n| 0.6895 | 6010 | -             | 0.4654          |\\n| 0.6906 | 6020 | -             | 0.4650          |\\n| 0.6918 | 6030 | -             | 0.4648          |\\n| 0.6929 | 6040 | -             | 0.4646          |\\n| 0.6940 | 6050 | -             | 0.4644          |\\n| 0.6952 | 6060 | -             | 0.4643          |\\n| 0.6963 | 6070 | -             | 0.4641          |\\n| 0.6975 | 6080 | -             | 0.4640          |\\n| 0.6986 | 6090 | -             | 0.4638          |\\n| 0.6998 | 6100 | 0.834         | 0.4637          |\\n| 0.7009 | 6110 | -             | 0.4633          |\\n| 0.7021 | 6120 | -             | 0.4632          |\\n| 0.7032 | 6130 | -             | 0.4631          |\\n| 0.7044 | 6140 | -             | 0.4628          |\\n| 0.7055 | 6150 | -             | 0.4627          |\\n| 0.7067 | 6160 | -             | 0.4623          |\\n| 0.7078 | 6170 | -             | 0.4617          |\\n| 0.7090 | 6180 | -             | 0.4615          |\\n| 0.7101 | 6190 | -             | 0.4614          |\\n| 0.7113 | 6200 | 0.8118        | 0.4612          |\\n| 0.7124 | 6210 | -             | 0.4612          |\\n| 0.7135 | 6220 | -             | 0.4612          |\\n| 0.7147 | 6230 | -             | 0.4610          |\\n| 0.7158 | 6240 | -             | 0.4609          |\\n| 0.7170 | 6250 | -             | 0.4610          |\\n| 0.7181 | 6260 | -             | 0.4611          |\\n| 0.7193 | 6270 | -             | 0.4607          |\\n| 0.7204 | 6280 | -             | 0.4599          |\\n| 0.7216 | 6290 | -             | 0.4598          |\\n| 0.7227 | 6300 | 0.7884        | 0.4600          |\\n| 0.7239 | 6310 | -             | 0.4599          |\\n| 0.7250 | 6320 | -             | 0.4600          |\\n| 0.7262 | 6330 | -             | 0.4601          |\\n| 0.7273 | 6340 | -             | 0.4603          |\\n| 0.7285 | 6350 | -             | 0.4603          |\\n| 0.7296 | 6360 | -             | 0.4598          |\\n| 0.7308 | 6370 | -             | 0.4597          |\\n| 0.7319 | 6380 | -             | 0.4596          |\\n| 0.7331 | 6390 | -             | 0.4594          |\\n| 0.7342 | 6400 | 0.8092        | 0.4590          |\\n| 0.7353 | 6410 | -             | 0.4588          |\\n| 0.7365 | 6420 | -             | 0.4585          |\\n| 0.7376 | 6430 | -             | 0.4584          |\\n| 0.7388 | 6440 | -             | 0.4580          |\\n| 0.7399 | 6450 | -             | 0.4574          |\\n| 0.7411 | 6460 | -             | 0.4570          |\\n| 0.7422 | 6470 | -             | 0.4566          |\\n| 0.7434 | 6480 | -             | 0.4563          |\\n| 0.7445 | 6490 | -             | 0.4560          |\\n| 0.7457 | 6500 | 0.8195        | 0.4557          |\\n| 0.7468 | 6510 | -             | 0.4556          |\\n| 0.7480 | 6520 | -             | 0.4554          |\\n| 0.7491 | 6530 | -             | 0.4551          |\\n| 0.7503 | 6540 | -             | 0.4548          |\\n| 0.7514 | 6550 | -             | 0.4545          |\\n| 0.7526 | 6560 | -             | 0.4543          |\\n| 0.7537 | 6570 | -             | 0.4541          |\\n| 0.7548 | 6580 | -             | 0.4540          |\\n| 0.7560 | 6590 | -             | 0.4538          |\\n| 0.7571 | 6600 | 0.8163        | 0.4535          |\\n| 0.7583 | 6610 | -             | 0.4533          |\\n| 0.7594 | 6620 | -             | 0.4536          |\\n| 0.7606 | 6630 | -             | 0.4535          |\\n| 0.7617 | 6640 | -             | 0.4533          |\\n| 0.7629 | 6650 | -             | 0.4532          |\\n| 0.7640 | 6660 | -             | 0.4531          |\\n| 0.7652 | 6670 | -             | 0.4531          |\\n| 0.7663 | 6680 | -             | 0.4530          |\\n| 0.7675 | 6690 | -             | 0.4528          |\\n| 0.7686 | 6700 | 0.8091        | 0.4527          |\\n| 0.7698 | 6710 | -             | 0.4527          |\\n| 0.7709 | 6720 | -             | 0.4526          |\\n| 0.7721 | 6730 | -             | 0.4525          |\\n| 0.7732 | 6740 | -             | 0.4524          |\\n| 0.7743 | 6750 | -             | 0.4521          |\\n| 0.7755 | 6760 | -             | 0.4517          |\\n| 0.7766 | 6770 | -             | 0.4514          |\\n| 0.7778 | 6780 | -             | 0.4512          |\\n| 0.7789 | 6790 | -             | 0.4514          |\\n| 0.7801 | 6800 | 0.8098        | 0.4515          |\\n| 0.7812 | 6810 | -             | 0.4514          |\\n| 0.7824 | 6820 | -             | 0.4511          |\\n| 0.7835 | 6830 | -             | 0.4507          |\\n| 0.7847 | 6840 | -             | 0.4505          |\\n| 0.7858 | 6850 | -             | 0.4504          |\\n| 0.7870 | 6860 | -             | 0.4503          |\\n| 0.7881 | 6870 | -             | 0.4500          |\\n| 0.7893 | 6880 | -             | 0.4498          |\\n| 0.7904 | 6890 | -             | 0.4495          |\\n| 0.7916 | 6900 | 0.7857        | 0.4491          |\\n| 0.7927 | 6910 | -             | 0.4490          |\\n| 0.7939 | 6920 | -             | 0.4488          |\\n| 0.7950 | 6930 | -             | 0.4488          |\\n| 0.7961 | 6940 | -             | 0.4488          |\\n| 0.7973 | 6950 | -             | 0.4487          |\\n| 0.7984 | 6960 | -             | 0.4484          |\\n| 0.7996 | 6970 | -             | 0.4482          |\\n| 0.8007 | 6980 | -             | 0.4483          |\\n| 0.8019 | 6990 | -             | 0.4481          |\\n| 0.8030 | 7000 | 0.7817        | 0.4477          |\\n| 0.8042 | 7010 | -             | 0.4476          |\\n| 0.8053 | 7020 | -             | 0.4471          |\\n| 0.8065 | 7030 | -             | 0.4469          |\\n| 0.8076 | 7040 | -             | 0.4468          |\\n| 0.8088 | 7050 | -             | 0.4465          |\\n| 0.8099 | 7060 | -             | 0.4460          |\\n| 0.8111 | 7070 | -             | 0.4458          |\\n| 0.8122 | 7080 | -             | 0.4458          |\\n| 0.8134 | 7090 | -             | 0.4454          |\\n| 0.8145 | 7100 | 0.779         | 0.4452          |\\n| 0.8156 | 7110 | -             | 0.4449          |\\n| 0.8168 | 7120 | -             | 0.4448          |\\n| 0.8179 | 7130 | -             | 0.4446          |\\n| 0.8191 | 7140 | -             | 0.4442          |\\n| 0.8202 | 7150 | -             | 0.4442          |\\n| 0.8214 | 7160 | -             | 0.4441          |\\n| 0.8225 | 7170 | -             | 0.4440          |\\n| 0.8237 | 7180 | -             | 0.4437          |\\n| 0.8248 | 7190 | -             | 0.4434          |\\n| 0.8260 | 7200 | 0.7807        | 0.4434          |\\n| 0.8271 | 7210 | -             | 0.4435          |\\n| 0.8283 | 7220 | -             | 0.4433          |\\n| 0.8294 | 7230 | -             | 0.4431          |\\n| 0.8306 | 7240 | -             | 0.4430          |\\n| 0.8317 | 7250 | -             | 0.4428          |\\n| 0.8329 | 7260 | -             | 0.4426          |\\n| 0.8340 | 7270 | -             | 0.4424          |\\n| 0.8351 | 7280 | -             | 0.4428          |\\n| 0.8363 | 7290 | -             | 0.4426          |\\n| 0.8374 | 7300 | 0.7724        | 0.4423          |\\n| 0.8386 | 7310 | -             | 0.4419          |\\n| 0.8397 | 7320 | -             | 0.4418          |\\n| 0.8409 | 7330 | -             | 0.4417          |\\n| 0.8420 | 7340 | -             | 0.4415          |\\n| 0.8432 | 7350 | -             | 0.4413          |\\n| 0.8443 | 7360 | -             | 0.4409          |\\n| 0.8455 | 7370 | -             | 0.4406          |\\n| 0.8466 | 7380 | -             | 0.4405          |\\n| 0.8478 | 7390 | -             | 0.4400          |\\n| 0.8489 | 7400 | 0.7898        | 0.4393          |\\n| 0.8501 | 7410 | -             | 0.4389          |\\n| 0.8512 | 7420 | -             | 0.4384          |\\n| 0.8524 | 7430 | -             | 0.4381          |\\n| 0.8535 | 7440 | -             | 0.4380          |\\n| 0.8547 | 7450 | -             | 0.4380          |\\n| 0.8558 | 7460 | -             | 0.4379          |\\n| 0.8569 | 7470 | -             | 0.4377          |\\n| 0.8581 | 7480 | -             | 0.4377          |\\n| 0.8592 | 7490 | -             | 0.4376          |\\n| 0.8604 | 7500 | 0.8009        | 0.4375          |\\n| 0.8615 | 7510 | -             | 0.4371          |\\n| 0.8627 | 7520 | -             | 0.4369          |\\n| 0.8638 | 7530 | -             | 0.4365          |\\n| 0.8650 | 7540 | -             | 0.4362          |\\n| 0.8661 | 7550 | -             | 0.4359          |\\n| 0.8673 | 7560 | -             | 0.4357          |\\n| 0.8684 | 7570 | -             | 0.4355          |\\n| 0.8696 | 7580 | -             | 0.4351          |\\n| 0.8707 | 7590 | -             | 0.4347          |\\n| 0.8719 | 7600 | 0.7847        | 0.4346          |\\n| 0.8730 | 7610 | -             | 0.4346          |\\n| 0.8742 | 7620 | -             | 0.4344          |\\n| 0.8753 | 7630 | -             | 0.4343          |\\n| 0.8764 | 7640 | -             | 0.4338          |\\n| 0.8776 | 7650 | -             | 0.4336          |\\n| 0.8787 | 7660 | -             | 0.4332          |\\n| 0.8799 | 7670 | -             | 0.4331          |\\n| 0.8810 | 7680 | -             | 0.4329          |\\n| 0.8822 | 7690 | -             | 0.4326          |\\n| 0.8833 | 7700 | 0.7668        | 0.4324          |\\n| 0.8845 | 7710 | -             | 0.4325          |\\n| 0.8856 | 7720 | -             | 0.4327          |\\n| 0.8868 | 7730 | -             | 0.4329          |\\n| 0.8879 | 7740 | -             | 0.4328          |\\n| 0.8891 | 7750 | -             | 0.4325          |\\n| 0.8902 | 7760 | -             | 0.4325          |\\n| 0.8914 | 7770 | -             | 0.4326          |\\n| 0.8925 | 7780 | -             | 0.4324          |\\n| 0.8937 | 7790 | -             | 0.4322          |\\n| 0.8948 | 7800 | 0.7987        | 0.4320          |\\n| 0.8960 | 7810 | -             | 0.4319          |\\n| 0.8971 | 7820 | -             | 0.4318          |\\n| 0.8982 | 7830 | -             | 0.4315          |\\n| 0.8994 | 7840 | -             | 0.4312          |\\n| 0.9005 | 7850 | -             | 0.4308          |\\n| 0.9017 | 7860 | -             | 0.4308          |\\n| 0.9028 | 7870 | -             | 0.4309          |\\n| 0.9040 | 7880 | -             | 0.4306          |\\n| 0.9051 | 7890 | -             | 0.4305          |\\n| 0.9063 | 7900 | 0.7691        | 0.4305          |\\n| 0.9074 | 7910 | -             | 0.4305          |\\n| 0.9086 | 7920 | -             | 0.4308          |\\n| 0.9097 | 7930 | -             | 0.4309          |\\n| 0.9109 | 7940 | -             | 0.4309          |\\n| 0.9120 | 7950 | -             | 0.4305          |\\n| 0.9132 | 7960 | -             | 0.4297          |\\n| 0.9143 | 7970 | -             | 0.4294          |\\n| 0.9155 | 7980 | -             | 0.4292          |\\n| 0.9166 | 7990 | -             | 0.4292          |\\n| 0.9177 | 8000 | 0.7828        | 0.4289          |\\n\\n</details>\\n\\n### Framework Versions\\n- Python: 3.12.8\\n- Sentence Transformers: 3.4.1\\n- Transformers: 4.49.0\\n- PyTorch: 2.2.0+cu121\\n- Accelerate: 1.4.0\\n- Datasets: 3.3.2\\n- Tokenizers: 0.21.0\\n\\n## Citation\\n\\n### BibTeX\\n\\n#### Sentence Transformers\\n```bibtex\\n@inproceedings{reimers-2019-sentence-bert,\\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\\n    author = \"Reimers, Nils and Gurevych, Iryna\",\\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\\n    month = \"11\",\\n    year = \"2019\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://arxiv.org/abs/1908.10084\",\\n}\\n```\\n\\n#### MultipleNegativesRankingLoss\\n```bibtex\\n@misc{henderson2017efficient,\\n    title={Efficient Natural Language Response Suggestion for Smart Reply},\\n    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\\n    year={2017},\\n    eprint={1705.00652},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.CL}\\n}\\n```\\n\\n<!--\\n## Glossary\\n\\n*Clearly define terms in order to be accessible across audiences.*\\n-->\\n\\n<!--\\n## Model Card Authors\\n\\n*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*\\n-->\\n\\n<!--\\n## Model Card Contact\\n\\n*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*\\n-->',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c283fcad08a0d12ccabd',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:14.509000',\n",
       "  'date_modified': '2025-02-27T19:18:14.509000'},\n",
       " {'model_identifier': 'AltEinstein/emc2',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:02:04',\n",
       "  'last_modified': '2025-02-27T17:09:00',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'llama', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 14.957527160644531},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c280fcad08a0d12ccabb',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:14.508000',\n",
       "  'date_modified': '2025-02-27T19:18:14.508000'},\n",
       " {'model_identifier': 'sobamchan/st5-base-mean-12000',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:08:35',\n",
       "  'last_modified': '2025-02-27T17:09:18',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['sentence-transformers',\n",
       "   'safetensors',\n",
       "   't5',\n",
       "   'sentence-similarity',\n",
       "   'feature-extraction',\n",
       "   'generated_from_trainer',\n",
       "   'dataset_size:557850',\n",
       "   'loss:MultipleNegativesRankingLoss',\n",
       "   'en',\n",
       "   'dataset:sentence-transformers/all-nli',\n",
       "   'arxiv:1908.10084',\n",
       "   'arxiv:1705.00652',\n",
       "   'base_model:google-t5/t5-base',\n",
       "   'base_model:finetune:google-t5/t5-base',\n",
       "   'autotrain_compatible',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['sentence-similarity', 'feature-extraction'],\n",
       "  'architecture': 'T5EncoderModel',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.4083981513977051},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'description': '---\\nlanguage:\\n- en\\ntags:\\n- sentence-transformers\\n- sentence-similarity\\n- feature-extraction\\n- generated_from_trainer\\n- dataset_size:557850\\n- loss:MultipleNegativesRankingLoss\\nbase_model: google-t5/t5-base\\nwidget:\\n- source_sentence: A man is jumping unto his filthy bed.\\n  sentences:\\n  - A young male is looking at a newspaper while 2 females walks past him.\\n  - The bed is dirty.\\n  - The man is on the moon.\\n- source_sentence: A carefully balanced male stands on one foot near a clean ocean\\n    beach area.\\n  sentences:\\n  - A man is ouside near the beach.\\n  - Three policemen patrol the streets on bikes\\n  - A man is sitting on his couch.\\n- source_sentence: The man is wearing a blue shirt.\\n  sentences:\\n  - Near the trashcan the man stood and smoked\\n  - A man in a blue shirt leans on a wall beside a road with a blue van and red car\\n    with water in the background.\\n  - A man in a black shirt is playing a guitar.\\n- source_sentence: The girls are outdoors.\\n  sentences:\\n  - Two girls riding on an amusement part ride.\\n  - a guy laughs while doing laundry\\n  - Three girls are standing together in a room, one is listening, one is writing\\n    on a wall and the third is talking to them.\\n- source_sentence: A construction worker peeking out of a manhole while his coworker\\n    sits on the sidewalk smiling.\\n  sentences:\\n  - A worker is looking out of a manhole.\\n  - A man is giving a presentation.\\n  - The workers are both inside the manhole.\\ndatasets:\\n- sentence-transformers/all-nli\\npipeline_tag: sentence-similarity\\nlibrary_name: sentence-transformers\\n---\\n\\n# SentenceTransformer based on google-t5/t5-base\\n\\nThis is a [sentence-transformers](https://www.SBERT.net) model finetuned from [google-t5/t5-base](https://huggingface.co/google-t5/t5-base) on the [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli) dataset. It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.\\n\\n## Model Details\\n\\n### Model Description\\n- **Model Type:** Sentence Transformer\\n- **Base model:** [google-t5/t5-base](https://huggingface.co/google-t5/t5-base) <!-- at revision a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1 -->\\n- **Maximum Sequence Length:** 256 tokens\\n- **Output Dimensionality:** 768 dimensions\\n- **Similarity Function:** Cosine Similarity\\n- **Training Dataset:**\\n    - [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli)\\n- **Language:** en\\n<!-- - **License:** Unknown -->\\n\\n### Model Sources\\n\\n- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)\\n- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)\\n- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)\\n\\n### Full Model Architecture\\n\\n```\\nSentenceTransformer(\\n  (0): Transformer({\\'max_seq_length\\': 256, \\'do_lower_case\\': False}) with Transformer model: T5EncoderModel \\n  (1): Pooling({\\'word_embedding_dimension\\': 768, \\'pooling_mode_cls_token\\': False, \\'pooling_mode_mean_tokens\\': True, \\'pooling_mode_max_tokens\\': False, \\'pooling_mode_mean_sqrt_len_tokens\\': False, \\'pooling_mode_weightedmean_tokens\\': False, \\'pooling_mode_lasttoken\\': False, \\'include_prompt\\': True})\\n  (2): Normalize()\\n)\\n```\\n\\n## Usage\\n\\n### Direct Usage (Sentence Transformers)\\n\\nFirst install the Sentence Transformers library:\\n\\n```bash\\npip install -U sentence-transformers\\n```\\n\\nThen you can load this model and run inference.\\n```python\\nfrom sentence_transformers import SentenceTransformer\\n\\n# Download from the 🤗 Hub\\nmodel = SentenceTransformer(\"sentence_transformers_model_id\")\\n# Run inference\\nsentences = [\\n    \\'A construction worker peeking out of a manhole while his coworker sits on the sidewalk smiling.\\',\\n    \\'A worker is looking out of a manhole.\\',\\n    \\'The workers are both inside the manhole.\\',\\n]\\nembeddings = model.encode(sentences)\\nprint(embeddings.shape)\\n# [3, 768]\\n\\n# Get the similarity scores for the embeddings\\nsimilarities = model.similarity(embeddings, embeddings)\\nprint(similarities.shape)\\n# [3, 3]\\n```\\n\\n<!--\\n### Direct Usage (Transformers)\\n\\n<details><summary>Click to see the direct usage in Transformers</summary>\\n\\n</details>\\n-->\\n\\n<!--\\n### Downstream Usage (Sentence Transformers)\\n\\nYou can finetune this model on your own dataset.\\n\\n<details><summary>Click to expand</summary>\\n\\n</details>\\n-->\\n\\n<!--\\n### Out-of-Scope Use\\n\\n*List how the model may foreseeably be misused and address what users ought not to do with the model.*\\n-->\\n\\n<!--\\n## Bias, Risks and Limitations\\n\\n*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*\\n-->\\n\\n<!--\\n### Recommendations\\n\\n*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*\\n-->\\n\\n## Training Details\\n\\n### Training Dataset\\n\\n#### all-nli\\n\\n* Dataset: [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli) at [d482672](https://huggingface.co/datasets/sentence-transformers/all-nli/tree/d482672c8e74ce18da116f430137434ba2e52fab)\\n* Size: 557,850 training samples\\n* Columns: <code>anchor</code>, <code>positive</code>, and <code>negative</code>\\n* Approximate statistics based on the first 1000 samples:\\n  |         | anchor                                                                           | positive                                                                          | negative                                                                          |\\n  |:--------|:---------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|\\n  | type    | string                                                                           | string                                                                            | string                                                                            |\\n  | details | <ul><li>min: 6 tokens</li><li>mean: 9.96 tokens</li><li>max: 52 tokens</li></ul> | <ul><li>min: 5 tokens</li><li>mean: 12.79 tokens</li><li>max: 44 tokens</li></ul> | <ul><li>min: 4 tokens</li><li>mean: 14.02 tokens</li><li>max: 57 tokens</li></ul> |\\n* Samples:\\n  | anchor                                                                     | positive                                         | negative                                                   |\\n  |:---------------------------------------------------------------------------|:-------------------------------------------------|:-----------------------------------------------------------|\\n  | <code>A person on a horse jumps over a broken down airplane.</code>        | <code>A person is outdoors, on a horse.</code>   | <code>A person is at a diner, ordering an omelette.</code> |\\n  | <code>Children smiling and waving at camera</code>                         | <code>There are children present</code>          | <code>The kids are frowning</code>                         |\\n  | <code>A boy is jumping on skateboard in the middle of a red bridge.</code> | <code>The boy does a skateboarding trick.</code> | <code>The boy skates down the sidewalk.</code>             |\\n* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:\\n  ```json\\n  {\\n      \"scale\": 20.0,\\n      \"similarity_fct\": \"cos_sim\"\\n  }\\n  ```\\n\\n### Evaluation Dataset\\n\\n#### all-nli\\n\\n* Dataset: [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli) at [d482672](https://huggingface.co/datasets/sentence-transformers/all-nli/tree/d482672c8e74ce18da116f430137434ba2e52fab)\\n* Size: 6,584 evaluation samples\\n* Columns: <code>anchor</code>, <code>positive</code>, and <code>negative</code>\\n* Approximate statistics based on the first 1000 samples:\\n  |         | anchor                                                                            | positive                                                                         | negative                                                                          |\\n  |:--------|:----------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|\\n  | type    | string                                                                            | string                                                                           | string                                                                            |\\n  | details | <ul><li>min: 5 tokens</li><li>mean: 19.41 tokens</li><li>max: 79 tokens</li></ul> | <ul><li>min: 4 tokens</li><li>mean: 9.69 tokens</li><li>max: 35 tokens</li></ul> | <ul><li>min: 4 tokens</li><li>mean: 10.35 tokens</li><li>max: 30 tokens</li></ul> |\\n* Samples:\\n  | anchor                                                                                                                                                                         | positive                                                    | negative                                                |\\n  |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------|:--------------------------------------------------------|\\n  | <code>Two women are embracing while holding to go packages.</code>                                                                                                             | <code>Two woman are holding packages.</code>                | <code>The men are fighting outside a deli.</code>       |\\n  | <code>Two young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink.</code> | <code>Two kids in numbered jerseys wash their hands.</code> | <code>Two kids in jackets walk to school.</code>        |\\n  | <code>A man selling donuts to a customer during a world exhibition event held in the city of Angeles</code>                                                                    | <code>A man selling donuts to a customer.</code>            | <code>A woman drinks her coffee in a small cafe.</code> |\\n* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:\\n  ```json\\n  {\\n      \"scale\": 20.0,\\n      \"similarity_fct\": \"cos_sim\"\\n  }\\n  ```\\n\\n### Training Hyperparameters\\n#### Non-Default Hyperparameters\\n\\n- `eval_strategy`: steps\\n- `per_device_train_batch_size`: 64\\n- `per_device_eval_batch_size`: 64\\n- `learning_rate`: 1e-05\\n- `warmup_ratio`: 0.1\\n- `batch_sampler`: no_duplicates\\n\\n#### All Hyperparameters\\n<details><summary>Click to expand</summary>\\n\\n- `overwrite_output_dir`: False\\n- `do_predict`: False\\n- `eval_strategy`: steps\\n- `prediction_loss_only`: True\\n- `per_device_train_batch_size`: 64\\n- `per_device_eval_batch_size`: 64\\n- `per_gpu_train_batch_size`: None\\n- `per_gpu_eval_batch_size`: None\\n- `gradient_accumulation_steps`: 1\\n- `eval_accumulation_steps`: None\\n- `torch_empty_cache_steps`: None\\n- `learning_rate`: 1e-05\\n- `weight_decay`: 0.0\\n- `adam_beta1`: 0.9\\n- `adam_beta2`: 0.999\\n- `adam_epsilon`: 1e-08\\n- `max_grad_norm`: 1.0\\n- `num_train_epochs`: 3\\n- `max_steps`: -1\\n- `lr_scheduler_type`: linear\\n- `lr_scheduler_kwargs`: {}\\n- `warmup_ratio`: 0.1\\n- `warmup_steps`: 0\\n- `log_level`: passive\\n- `log_level_replica`: warning\\n- `log_on_each_node`: True\\n- `logging_nan_inf_filter`: True\\n- `save_safetensors`: True\\n- `save_on_each_node`: False\\n- `save_only_model`: False\\n- `restore_callback_states_from_checkpoint`: False\\n- `no_cuda`: False\\n- `use_cpu`: False\\n- `use_mps_device`: False\\n- `seed`: 42\\n- `data_seed`: None\\n- `jit_mode_eval`: False\\n- `use_ipex`: False\\n- `bf16`: False\\n- `fp16`: False\\n- `fp16_opt_level`: O1\\n- `half_precision_backend`: auto\\n- `bf16_full_eval`: False\\n- `fp16_full_eval`: False\\n- `tf32`: None\\n- `local_rank`: 0\\n- `ddp_backend`: None\\n- `tpu_num_cores`: None\\n- `tpu_metrics_debug`: False\\n- `debug`: []\\n- `dataloader_drop_last`: False\\n- `dataloader_num_workers`: 0\\n- `dataloader_prefetch_factor`: None\\n- `past_index`: -1\\n- `disable_tqdm`: False\\n- `remove_unused_columns`: True\\n- `label_names`: None\\n- `load_best_model_at_end`: False\\n- `ignore_data_skip`: False\\n- `fsdp`: []\\n- `fsdp_min_num_params`: 0\\n- `fsdp_config`: {\\'min_num_params\\': 0, \\'xla\\': False, \\'xla_fsdp_v2\\': False, \\'xla_fsdp_grad_ckpt\\': False}\\n- `fsdp_transformer_layer_cls_to_wrap`: None\\n- `accelerator_config`: {\\'split_batches\\': False, \\'dispatch_batches\\': None, \\'even_batches\\': True, \\'use_seedable_sampler\\': True, \\'non_blocking\\': False, \\'gradient_accumulation_kwargs\\': None}\\n- `deepspeed`: None\\n- `label_smoothing_factor`: 0.0\\n- `optim`: adamw_torch\\n- `optim_args`: None\\n- `adafactor`: False\\n- `group_by_length`: False\\n- `length_column_name`: length\\n- `ddp_find_unused_parameters`: None\\n- `ddp_bucket_cap_mb`: None\\n- `ddp_broadcast_buffers`: False\\n- `dataloader_pin_memory`: True\\n- `dataloader_persistent_workers`: False\\n- `skip_memory_metrics`: True\\n- `use_legacy_prediction_loop`: False\\n- `push_to_hub`: False\\n- `resume_from_checkpoint`: None\\n- `hub_model_id`: None\\n- `hub_strategy`: every_save\\n- `hub_private_repo`: None\\n- `hub_always_push`: False\\n- `gradient_checkpointing`: False\\n- `gradient_checkpointing_kwargs`: None\\n- `include_inputs_for_metrics`: False\\n- `include_for_metrics`: []\\n- `eval_do_concat_batches`: True\\n- `fp16_backend`: auto\\n- `push_to_hub_model_id`: None\\n- `push_to_hub_organization`: None\\n- `mp_parameters`: \\n- `auto_find_batch_size`: False\\n- `full_determinism`: False\\n- `torchdynamo`: None\\n- `ray_scope`: last\\n- `ddp_timeout`: 1800\\n- `torch_compile`: False\\n- `torch_compile_backend`: None\\n- `torch_compile_mode`: None\\n- `dispatch_batches`: None\\n- `split_batches`: None\\n- `include_tokens_per_second`: False\\n- `include_num_input_tokens_seen`: False\\n- `neftune_noise_alpha`: None\\n- `optim_target_modules`: None\\n- `batch_eval_metrics`: False\\n- `eval_on_start`: False\\n- `use_liger_kernel`: False\\n- `eval_use_gather_object`: False\\n- `average_tokens_across_devices`: False\\n- `prompts`: None\\n- `batch_sampler`: no_duplicates\\n- `multi_dataset_batch_sampler`: proportional\\n\\n</details>\\n\\n### Training Logs\\n<details><summary>Click to expand</summary>\\n\\n| Epoch  | Step  | Training Loss | Validation Loss |\\n|:------:|:-----:|:-------------:|:---------------:|\\n| 0.0011 | 10    | -             | 1.8733          |\\n| 0.0023 | 20    | -             | 1.8726          |\\n| 0.0034 | 30    | -             | 1.8714          |\\n| 0.0046 | 40    | -             | 1.8697          |\\n| 0.0057 | 50    | -             | 1.8675          |\\n| 0.0069 | 60    | -             | 1.8649          |\\n| 0.0080 | 70    | -             | 1.8619          |\\n| 0.0092 | 80    | -             | 1.8584          |\\n| 0.0103 | 90    | -             | 1.8544          |\\n| 0.0115 | 100   | 3.1046        | 1.8499          |\\n| 0.0126 | 110   | -             | 1.8451          |\\n| 0.0138 | 120   | -             | 1.8399          |\\n| 0.0149 | 130   | -             | 1.8343          |\\n| 0.0161 | 140   | -             | 1.8283          |\\n| 0.0172 | 150   | -             | 1.8223          |\\n| 0.0184 | 160   | -             | 1.8159          |\\n| 0.0195 | 170   | -             | 1.8091          |\\n| 0.0206 | 180   | -             | 1.8016          |\\n| 0.0218 | 190   | -             | 1.7938          |\\n| 0.0229 | 200   | 3.0303        | 1.7858          |\\n| 0.0241 | 210   | -             | 1.7775          |\\n| 0.0252 | 220   | -             | 1.7693          |\\n| 0.0264 | 230   | -             | 1.7605          |\\n| 0.0275 | 240   | -             | 1.7514          |\\n| 0.0287 | 250   | -             | 1.7417          |\\n| 0.0298 | 260   | -             | 1.7320          |\\n| 0.0310 | 270   | -             | 1.7227          |\\n| 0.0321 | 280   | -             | 1.7134          |\\n| 0.0333 | 290   | -             | 1.7040          |\\n| 0.0344 | 300   | 2.9459        | 1.6941          |\\n| 0.0356 | 310   | -             | 1.6833          |\\n| 0.0367 | 320   | -             | 1.6725          |\\n| 0.0379 | 330   | -             | 1.6614          |\\n| 0.0390 | 340   | -             | 1.6510          |\\n| 0.0402 | 350   | -             | 1.6402          |\\n| 0.0413 | 360   | -             | 1.6296          |\\n| 0.0424 | 370   | -             | 1.6187          |\\n| 0.0436 | 380   | -             | 1.6073          |\\n| 0.0447 | 390   | -             | 1.5962          |\\n| 0.0459 | 400   | 2.7813        | 1.5848          |\\n| 0.0470 | 410   | -             | 1.5735          |\\n| 0.0482 | 420   | -             | 1.5620          |\\n| 0.0493 | 430   | -             | 1.5495          |\\n| 0.0505 | 440   | -             | 1.5375          |\\n| 0.0516 | 450   | -             | 1.5256          |\\n| 0.0528 | 460   | -             | 1.5133          |\\n| 0.0539 | 470   | -             | 1.5012          |\\n| 0.0551 | 480   | -             | 1.4892          |\\n| 0.0562 | 490   | -             | 1.4769          |\\n| 0.0574 | 500   | 2.6308        | 1.4640          |\\n| 0.0585 | 510   | -             | 1.4513          |\\n| 0.0597 | 520   | -             | 1.4391          |\\n| 0.0608 | 530   | -             | 1.4262          |\\n| 0.0619 | 540   | -             | 1.4130          |\\n| 0.0631 | 550   | -             | 1.3998          |\\n| 0.0642 | 560   | -             | 1.3874          |\\n| 0.0654 | 570   | -             | 1.3752          |\\n| 0.0665 | 580   | -             | 1.3620          |\\n| 0.0677 | 590   | -             | 1.3485          |\\n| 0.0688 | 600   | 2.4452        | 1.3350          |\\n| 0.0700 | 610   | -             | 1.3213          |\\n| 0.0711 | 620   | -             | 1.3088          |\\n| 0.0723 | 630   | -             | 1.2965          |\\n| 0.0734 | 640   | -             | 1.2839          |\\n| 0.0746 | 650   | -             | 1.2713          |\\n| 0.0757 | 660   | -             | 1.2592          |\\n| 0.0769 | 670   | -             | 1.2466          |\\n| 0.0780 | 680   | -             | 1.2332          |\\n| 0.0792 | 690   | -             | 1.2203          |\\n| 0.0803 | 700   | 2.2626        | 1.2077          |\\n| 0.0815 | 710   | -             | 1.1959          |\\n| 0.0826 | 720   | -             | 1.1841          |\\n| 0.0837 | 730   | -             | 1.1725          |\\n| 0.0849 | 740   | -             | 1.1619          |\\n| 0.0860 | 750   | -             | 1.1516          |\\n| 0.0872 | 760   | -             | 1.1416          |\\n| 0.0883 | 770   | -             | 1.1320          |\\n| 0.0895 | 780   | -             | 1.1227          |\\n| 0.0906 | 790   | -             | 1.1138          |\\n| 0.0918 | 800   | 2.0044        | 1.1053          |\\n| 0.0929 | 810   | -             | 1.0965          |\\n| 0.0941 | 820   | -             | 1.0879          |\\n| 0.0952 | 830   | -             | 1.0796          |\\n| 0.0964 | 840   | -             | 1.0718          |\\n| 0.0975 | 850   | -             | 1.0644          |\\n| 0.0987 | 860   | -             | 1.0564          |\\n| 0.0998 | 870   | -             | 1.0490          |\\n| 0.1010 | 880   | -             | 1.0417          |\\n| 0.1021 | 890   | -             | 1.0354          |\\n| 0.1032 | 900   | 1.8763        | 1.0296          |\\n| 0.1044 | 910   | -             | 1.0239          |\\n| 0.1055 | 920   | -             | 1.0180          |\\n| 0.1067 | 930   | -             | 1.0123          |\\n| 0.1078 | 940   | -             | 1.0065          |\\n| 0.1090 | 950   | -             | 1.0008          |\\n| 0.1101 | 960   | -             | 0.9950          |\\n| 0.1113 | 970   | -             | 0.9894          |\\n| 0.1124 | 980   | -             | 0.9840          |\\n| 0.1136 | 990   | -             | 0.9793          |\\n| 0.1147 | 1000  | 1.7287        | 0.9752          |\\n| 0.1159 | 1010  | -             | 0.9706          |\\n| 0.1170 | 1020  | -             | 0.9659          |\\n| 0.1182 | 1030  | -             | 0.9615          |\\n| 0.1193 | 1040  | -             | 0.9572          |\\n| 0.1205 | 1050  | -             | 0.9531          |\\n| 0.1216 | 1060  | -             | 0.9494          |\\n| 0.1227 | 1070  | -             | 0.9456          |\\n| 0.1239 | 1080  | -             | 0.9415          |\\n| 0.1250 | 1090  | -             | 0.9377          |\\n| 0.1262 | 1100  | 1.6312        | 0.9339          |\\n| 0.1273 | 1110  | -             | 0.9303          |\\n| 0.1285 | 1120  | -             | 0.9267          |\\n| 0.1296 | 1130  | -             | 0.9232          |\\n| 0.1308 | 1140  | -             | 0.9197          |\\n| 0.1319 | 1150  | -             | 0.9162          |\\n| 0.1331 | 1160  | -             | 0.9128          |\\n| 0.1342 | 1170  | -             | 0.9097          |\\n| 0.1354 | 1180  | -             | 0.9069          |\\n| 0.1365 | 1190  | -             | 0.9040          |\\n| 0.1377 | 1200  | 1.5316        | 0.9010          |\\n| 0.1388 | 1210  | -             | 0.8979          |\\n| 0.1400 | 1220  | -             | 0.8947          |\\n| 0.1411 | 1230  | -             | 0.8915          |\\n| 0.1423 | 1240  | -             | 0.8888          |\\n| 0.1434 | 1250  | -             | 0.8861          |\\n| 0.1445 | 1260  | -             | 0.8833          |\\n| 0.1457 | 1270  | -             | 0.8806          |\\n| 0.1468 | 1280  | -             | 0.8779          |\\n| 0.1480 | 1290  | -             | 0.8748          |\\n| 0.1491 | 1300  | 1.4961        | 0.8718          |\\n| 0.1503 | 1310  | -             | 0.8690          |\\n| 0.1514 | 1320  | -             | 0.8664          |\\n| 0.1526 | 1330  | -             | 0.8635          |\\n| 0.1537 | 1340  | -             | 0.8603          |\\n| 0.1549 | 1350  | -             | 0.8574          |\\n| 0.1560 | 1360  | -             | 0.8545          |\\n| 0.1572 | 1370  | -             | 0.8521          |\\n| 0.1583 | 1380  | -             | 0.8497          |\\n| 0.1595 | 1390  | -             | 0.8474          |\\n| 0.1606 | 1400  | 1.451         | 0.8453          |\\n| 0.1618 | 1410  | -             | 0.8429          |\\n| 0.1629 | 1420  | -             | 0.8404          |\\n| 0.1640 | 1430  | -             | 0.8380          |\\n| 0.1652 | 1440  | -             | 0.8357          |\\n| 0.1663 | 1450  | -             | 0.8336          |\\n| 0.1675 | 1460  | -             | 0.8312          |\\n| 0.1686 | 1470  | -             | 0.8289          |\\n| 0.1698 | 1480  | -             | 0.8262          |\\n| 0.1709 | 1490  | -             | 0.8236          |\\n| 0.1721 | 1500  | 1.4177        | 0.8213          |\\n| 0.1732 | 1510  | -             | 0.8189          |\\n| 0.1744 | 1520  | -             | 0.8168          |\\n| 0.1755 | 1530  | -             | 0.8147          |\\n| 0.1767 | 1540  | -             | 0.8127          |\\n| 0.1778 | 1550  | -             | 0.8107          |\\n| 0.1790 | 1560  | -             | 0.8082          |\\n| 0.1801 | 1570  | -             | 0.8059          |\\n| 0.1813 | 1580  | -             | 0.8036          |\\n| 0.1824 | 1590  | -             | 0.8015          |\\n| 0.1835 | 1600  | 1.3734        | 0.7993          |\\n| 0.1847 | 1610  | -             | 0.7970          |\\n| 0.1858 | 1620  | -             | 0.7948          |\\n| 0.1870 | 1630  | -             | 0.7922          |\\n| 0.1881 | 1640  | -             | 0.7900          |\\n| 0.1893 | 1650  | -             | 0.7877          |\\n| 0.1904 | 1660  | -             | 0.7852          |\\n| 0.1916 | 1670  | -             | 0.7829          |\\n| 0.1927 | 1680  | -             | 0.7804          |\\n| 0.1939 | 1690  | -             | 0.7779          |\\n| 0.1950 | 1700  | 1.3327        | 0.7757          |\\n| 0.1962 | 1710  | -             | 0.7738          |\\n| 0.1973 | 1720  | -             | 0.7719          |\\n| 0.1985 | 1730  | -             | 0.7700          |\\n| 0.1996 | 1740  | -             | 0.7679          |\\n| 0.2008 | 1750  | -             | 0.7658          |\\n| 0.2019 | 1760  | -             | 0.7641          |\\n| 0.2031 | 1770  | -             | 0.7621          |\\n| 0.2042 | 1780  | -             | 0.7601          |\\n| 0.2053 | 1790  | -             | 0.7580          |\\n| 0.2065 | 1800  | 1.2804        | 0.7558          |\\n| 0.2076 | 1810  | -             | 0.7536          |\\n| 0.2088 | 1820  | -             | 0.7514          |\\n| 0.2099 | 1830  | -             | 0.7493          |\\n| 0.2111 | 1840  | -             | 0.7473          |\\n| 0.2122 | 1850  | -             | 0.7451          |\\n| 0.2134 | 1860  | -             | 0.7429          |\\n| 0.2145 | 1870  | -             | 0.7408          |\\n| 0.2157 | 1880  | -             | 0.7389          |\\n| 0.2168 | 1890  | -             | 0.7368          |\\n| 0.2180 | 1900  | 1.2255        | 0.7349          |\\n| 0.2191 | 1910  | -             | 0.7328          |\\n| 0.2203 | 1920  | -             | 0.7310          |\\n| 0.2214 | 1930  | -             | 0.7293          |\\n| 0.2226 | 1940  | -             | 0.7277          |\\n| 0.2237 | 1950  | -             | 0.7259          |\\n| 0.2248 | 1960  | -             | 0.7240          |\\n| 0.2260 | 1970  | -             | 0.7221          |\\n| 0.2271 | 1980  | -             | 0.7203          |\\n| 0.2283 | 1990  | -             | 0.7184          |\\n| 0.2294 | 2000  | 1.2635        | 0.7165          |\\n| 0.2306 | 2010  | -             | 0.7150          |\\n| 0.2317 | 2020  | -             | 0.7135          |\\n| 0.2329 | 2030  | -             | 0.7117          |\\n| 0.2340 | 2040  | -             | 0.7099          |\\n| 0.2352 | 2050  | -             | 0.7084          |\\n| 0.2363 | 2060  | -             | 0.7068          |\\n| 0.2375 | 2070  | -             | 0.7054          |\\n| 0.2386 | 2080  | -             | 0.7037          |\\n| 0.2398 | 2090  | -             | 0.7023          |\\n| 0.2409 | 2100  | 1.1912        | 0.7009          |\\n| 0.2421 | 2110  | -             | 0.6991          |\\n| 0.2432 | 2120  | -             | 0.6974          |\\n| 0.2444 | 2130  | -             | 0.6962          |\\n| 0.2455 | 2140  | -             | 0.6950          |\\n| 0.2466 | 2150  | -             | 0.6938          |\\n| 0.2478 | 2160  | -             | 0.6922          |\\n| 0.2489 | 2170  | -             | 0.6909          |\\n| 0.2501 | 2180  | -             | 0.6897          |\\n| 0.2512 | 2190  | -             | 0.6884          |\\n| 0.2524 | 2200  | 1.2144        | 0.6868          |\\n| 0.2535 | 2210  | -             | 0.6856          |\\n| 0.2547 | 2220  | -             | 0.6843          |\\n| 0.2558 | 2230  | -             | 0.6829          |\\n| 0.2570 | 2240  | -             | 0.6817          |\\n| 0.2581 | 2250  | -             | 0.6804          |\\n| 0.2593 | 2260  | -             | 0.6789          |\\n| 0.2604 | 2270  | -             | 0.6775          |\\n| 0.2616 | 2280  | -             | 0.6763          |\\n| 0.2627 | 2290  | -             | 0.6751          |\\n| 0.2639 | 2300  | 1.1498        | 0.6739          |\\n| 0.2650 | 2310  | -             | 0.6725          |\\n| 0.2661 | 2320  | -             | 0.6711          |\\n| 0.2673 | 2330  | -             | 0.6698          |\\n| 0.2684 | 2340  | -             | 0.6684          |\\n| 0.2696 | 2350  | -             | 0.6666          |\\n| 0.2707 | 2360  | -             | 0.6653          |\\n| 0.2719 | 2370  | -             | 0.6638          |\\n| 0.2730 | 2380  | -             | 0.6621          |\\n| 0.2742 | 2390  | -             | 0.6609          |\\n| 0.2753 | 2400  | 1.1446        | 0.6596          |\\n| 0.2765 | 2410  | -             | 0.6582          |\\n| 0.2776 | 2420  | -             | 0.6568          |\\n| 0.2788 | 2430  | -             | 0.6553          |\\n| 0.2799 | 2440  | -             | 0.6541          |\\n| 0.2811 | 2450  | -             | 0.6527          |\\n| 0.2822 | 2460  | -             | 0.6513          |\\n| 0.2834 | 2470  | -             | 0.6496          |\\n| 0.2845 | 2480  | -             | 0.6483          |\\n| 0.2856 | 2490  | -             | 0.6475          |\\n| 0.2868 | 2500  | 1.1309        | 0.6465          |\\n| 0.2879 | 2510  | -             | 0.6455          |\\n| 0.2891 | 2520  | -             | 0.6447          |\\n| 0.2902 | 2530  | -             | 0.6437          |\\n| 0.2914 | 2540  | -             | 0.6428          |\\n| 0.2925 | 2550  | -             | 0.6415          |\\n| 0.2937 | 2560  | -             | 0.6403          |\\n| 0.2948 | 2570  | -             | 0.6392          |\\n| 0.2960 | 2580  | -             | 0.6381          |\\n| 0.2971 | 2590  | -             | 0.6371          |\\n| 0.2983 | 2600  | 1.1006        | 0.6358          |\\n| 0.2994 | 2610  | -             | 0.6348          |\\n| 0.3006 | 2620  | -             | 0.6340          |\\n| 0.3017 | 2630  | -             | 0.6330          |\\n| 0.3029 | 2640  | -             | 0.6319          |\\n| 0.3040 | 2650  | -             | 0.6308          |\\n| 0.3052 | 2660  | -             | 0.6300          |\\n| 0.3063 | 2670  | -             | 0.6291          |\\n| 0.3074 | 2680  | -             | 0.6280          |\\n| 0.3086 | 2690  | -             | 0.6268          |\\n| 0.3097 | 2700  | 1.0772        | 0.6254          |\\n| 0.3109 | 2710  | -             | 0.6243          |\\n| 0.3120 | 2720  | -             | 0.6232          |\\n| 0.3132 | 2730  | -             | 0.6224          |\\n| 0.3143 | 2740  | -             | 0.6215          |\\n| 0.3155 | 2750  | -             | 0.6205          |\\n| 0.3166 | 2760  | -             | 0.6194          |\\n| 0.3178 | 2770  | -             | 0.6183          |\\n| 0.3189 | 2780  | -             | 0.6171          |\\n| 0.3201 | 2790  | -             | 0.6160          |\\n| 0.3212 | 2800  | 1.0648        | 0.6153          |\\n| 0.3224 | 2810  | -             | 0.6141          |\\n| 0.3235 | 2820  | -             | 0.6129          |\\n| 0.3247 | 2830  | -             | 0.6119          |\\n| 0.3258 | 2840  | -             | 0.6109          |\\n| 0.3269 | 2850  | -             | 0.6099          |\\n| 0.3281 | 2860  | -             | 0.6088          |\\n| 0.3292 | 2870  | -             | 0.6079          |\\n| 0.3304 | 2880  | -             | 0.6073          |\\n| 0.3315 | 2890  | -             | 0.6063          |\\n| 0.3327 | 2900  | 1.0398        | 0.6054          |\\n| 0.3338 | 2910  | -             | 0.6044          |\\n| 0.3350 | 2920  | -             | 0.6033          |\\n| 0.3361 | 2930  | -             | 0.6022          |\\n| 0.3373 | 2940  | -             | 0.6012          |\\n| 0.3384 | 2950  | -             | 0.6003          |\\n| 0.3396 | 2960  | -             | 0.5993          |\\n| 0.3407 | 2970  | -             | 0.5986          |\\n| 0.3419 | 2980  | -             | 0.5978          |\\n| 0.3430 | 2990  | -             | 0.5967          |\\n| 0.3442 | 3000  | 1.0256        | 0.5959          |\\n| 0.3453 | 3010  | -             | 0.5947          |\\n| 0.3464 | 3020  | -             | 0.5937          |\\n| 0.3476 | 3030  | -             | 0.5929          |\\n| 0.3487 | 3040  | -             | 0.5920          |\\n| 0.3499 | 3050  | -             | 0.5908          |\\n| 0.3510 | 3060  | -             | 0.5897          |\\n| 0.3522 | 3070  | -             | 0.5888          |\\n| 0.3533 | 3080  | -             | 0.5882          |\\n| 0.3545 | 3090  | -             | 0.5874          |\\n| 0.3556 | 3100  | 1.0489        | 0.5868          |\\n| 0.3568 | 3110  | -             | 0.5860          |\\n| 0.3579 | 3120  | -             | 0.5854          |\\n| 0.3591 | 3130  | -             | 0.5839          |\\n| 0.3602 | 3140  | -             | 0.5830          |\\n| 0.3614 | 3150  | -             | 0.5822          |\\n| 0.3625 | 3160  | -             | 0.5814          |\\n| 0.3637 | 3170  | -             | 0.5808          |\\n| 0.3648 | 3180  | -             | 0.5802          |\\n| 0.3660 | 3190  | -             | 0.5794          |\\n| 0.3671 | 3200  | 1.038         | 0.5788          |\\n| 0.3682 | 3210  | -             | 0.5778          |\\n| 0.3694 | 3220  | -             | 0.5770          |\\n| 0.3705 | 3230  | -             | 0.5763          |\\n| 0.3717 | 3240  | -             | 0.5752          |\\n| 0.3728 | 3250  | -             | 0.5745          |\\n| 0.3740 | 3260  | -             | 0.5737          |\\n| 0.3751 | 3270  | -             | 0.5728          |\\n| 0.3763 | 3280  | -             | 0.5720          |\\n| 0.3774 | 3290  | -             | 0.5713          |\\n| 0.3786 | 3300  | 1.0058        | 0.5707          |\\n| 0.3797 | 3310  | -             | 0.5700          |\\n| 0.3809 | 3320  | -             | 0.5690          |\\n| 0.3820 | 3330  | -             | 0.5681          |\\n| 0.3832 | 3340  | -             | 0.5673          |\\n| 0.3843 | 3350  | -             | 0.5669          |\\n| 0.3855 | 3360  | -             | 0.5667          |\\n| 0.3866 | 3370  | -             | 0.5665          |\\n| 0.3877 | 3380  | -             | 0.5659          |\\n| 0.3889 | 3390  | -             | 0.5650          |\\n| 0.3900 | 3400  | 1.0413        | 0.5645          |\\n| 0.3912 | 3410  | -             | 0.5641          |\\n| 0.3923 | 3420  | -             | 0.5635          |\\n| 0.3935 | 3430  | -             | 0.5629          |\\n| 0.3946 | 3440  | -             | 0.5622          |\\n| 0.3958 | 3450  | -             | 0.5617          |\\n| 0.3969 | 3460  | -             | 0.5614          |\\n| 0.3981 | 3470  | -             | 0.5607          |\\n| 0.3992 | 3480  | -             | 0.5603          |\\n| 0.4004 | 3490  | -             | 0.5598          |\\n| 0.4015 | 3500  | 0.938         | 0.5596          |\\n| 0.4027 | 3510  | -             | 0.5589          |\\n| 0.4038 | 3520  | -             | 0.5581          |\\n| 0.4050 | 3530  | -             | 0.5571          |\\n| 0.4061 | 3540  | -             | 0.5563          |\\n| 0.4073 | 3550  | -             | 0.5557          |\\n| 0.4084 | 3560  | -             | 0.5551          |\\n| 0.4095 | 3570  | -             | 0.5546          |\\n| 0.4107 | 3580  | -             | 0.5541          |\\n| 0.4118 | 3590  | -             | 0.5535          |\\n| 0.4130 | 3600  | 0.955         | 0.5528          |\\n| 0.4141 | 3610  | -             | 0.5522          |\\n| 0.4153 | 3620  | -             | 0.5516          |\\n| 0.4164 | 3630  | -             | 0.5509          |\\n| 0.4176 | 3640  | -             | 0.5503          |\\n| 0.4187 | 3650  | -             | 0.5495          |\\n| 0.4199 | 3660  | -             | 0.5490          |\\n| 0.4210 | 3670  | -             | 0.5481          |\\n| 0.4222 | 3680  | -             | 0.5475          |\\n| 0.4233 | 3690  | -             | 0.5467          |\\n| 0.4245 | 3700  | 0.9387        | 0.5463          |\\n| 0.4256 | 3710  | -             | 0.5459          |\\n| 0.4268 | 3720  | -             | 0.5452          |\\n| 0.4279 | 3730  | -             | 0.5448          |\\n| 0.4290 | 3740  | -             | 0.5443          |\\n| 0.4302 | 3750  | -             | 0.5440          |\\n| 0.4313 | 3760  | -             | 0.5435          |\\n| 0.4325 | 3770  | -             | 0.5430          |\\n| 0.4336 | 3780  | -             | 0.5423          |\\n| 0.4348 | 3790  | -             | 0.5418          |\\n| 0.4359 | 3800  | 0.9672        | 0.5415          |\\n| 0.4371 | 3810  | -             | 0.5413          |\\n| 0.4382 | 3820  | -             | 0.5410          |\\n| 0.4394 | 3830  | -             | 0.5406          |\\n| 0.4405 | 3840  | -             | 0.5403          |\\n| 0.4417 | 3850  | -             | 0.5397          |\\n| 0.4428 | 3860  | -             | 0.5394          |\\n| 0.4440 | 3870  | -             | 0.5386          |\\n| 0.4451 | 3880  | -             | 0.5378          |\\n| 0.4463 | 3890  | -             | 0.5370          |\\n| 0.4474 | 3900  | 0.926         | 0.5360          |\\n| 0.4485 | 3910  | -             | 0.5351          |\\n| 0.4497 | 3920  | -             | 0.5346          |\\n| 0.4508 | 3930  | -             | 0.5343          |\\n| 0.4520 | 3940  | -             | 0.5339          |\\n| 0.4531 | 3950  | -             | 0.5337          |\\n| 0.4543 | 3960  | -             | 0.5334          |\\n| 0.4554 | 3970  | -             | 0.5330          |\\n| 0.4566 | 3980  | -             | 0.5327          |\\n| 0.4577 | 3990  | -             | 0.5324          |\\n| 0.4589 | 4000  | 0.867         | 0.5319          |\\n| 0.4600 | 4010  | -             | 0.5313          |\\n| 0.4612 | 4020  | -             | 0.5308          |\\n| 0.4623 | 4030  | -             | 0.5300          |\\n| 0.4635 | 4040  | -             | 0.5293          |\\n| 0.4646 | 4050  | -             | 0.5287          |\\n| 0.4658 | 4060  | -             | 0.5284          |\\n| 0.4669 | 4070  | -             | 0.5281          |\\n| 0.4681 | 4080  | -             | 0.5277          |\\n| 0.4692 | 4090  | -             | 0.5272          |\\n| 0.4703 | 4100  | 0.916         | 0.5267          |\\n| 0.4715 | 4110  | -             | 0.5260          |\\n| 0.4726 | 4120  | -             | 0.5252          |\\n| 0.4738 | 4130  | -             | 0.5246          |\\n| 0.4749 | 4140  | -             | 0.5239          |\\n| 0.4761 | 4150  | -             | 0.5232          |\\n| 0.4772 | 4160  | -             | 0.5225          |\\n| 0.4784 | 4170  | -             | 0.5221          |\\n| 0.4795 | 4180  | -             | 0.5216          |\\n| 0.4807 | 4190  | -             | 0.5211          |\\n| 0.4818 | 4200  | 0.9667        | 0.5206          |\\n| 0.4830 | 4210  | -             | 0.5204          |\\n| 0.4841 | 4220  | -             | 0.5200          |\\n| 0.4853 | 4230  | -             | 0.5192          |\\n| 0.4864 | 4240  | -             | 0.5187          |\\n| 0.4876 | 4250  | -             | 0.5185          |\\n| 0.4887 | 4260  | -             | 0.5179          |\\n| 0.4898 | 4270  | -             | 0.5173          |\\n| 0.4910 | 4280  | -             | 0.5170          |\\n| 0.4921 | 4290  | -             | 0.5165          |\\n| 0.4933 | 4300  | 0.9276        | 0.5160          |\\n| 0.4944 | 4310  | -             | 0.5154          |\\n| 0.4956 | 4320  | -             | 0.5150          |\\n| 0.4967 | 4330  | -             | 0.5144          |\\n| 0.4979 | 4340  | -             | 0.5141          |\\n| 0.4990 | 4350  | -             | 0.5139          |\\n| 0.5002 | 4360  | -             | 0.5138          |\\n| 0.5013 | 4370  | -             | 0.5136          |\\n| 0.5025 | 4380  | -             | 0.5133          |\\n| 0.5036 | 4390  | -             | 0.5129          |\\n| 0.5048 | 4400  | 0.9331        | 0.5126          |\\n| 0.5059 | 4410  | -             | 0.5123          |\\n| 0.5071 | 4420  | -             | 0.5117          |\\n| 0.5082 | 4430  | -             | 0.5113          |\\n| 0.5093 | 4440  | -             | 0.5108          |\\n| 0.5105 | 4450  | -             | 0.5106          |\\n| 0.5116 | 4460  | -             | 0.5106          |\\n| 0.5128 | 4470  | -             | 0.5106          |\\n| 0.5139 | 4480  | -             | 0.5104          |\\n| 0.5151 | 4490  | -             | 0.5102          |\\n| 0.5162 | 4500  | 0.907         | 0.5097          |\\n| 0.5174 | 4510  | -             | 0.5092          |\\n| 0.5185 | 4520  | -             | 0.5086          |\\n| 0.5197 | 4530  | -             | 0.5082          |\\n| 0.5208 | 4540  | -             | 0.5079          |\\n| 0.5220 | 4550  | -             | 0.5075          |\\n| 0.5231 | 4560  | -             | 0.5071          |\\n| 0.5243 | 4570  | -             | 0.5067          |\\n| 0.5254 | 4580  | -             | 0.5066          |\\n| 0.5266 | 4590  | -             | 0.5062          |\\n| 0.5277 | 4600  | 0.913         | 0.5059          |\\n| 0.5289 | 4610  | -             | 0.5056          |\\n| 0.5300 | 4620  | -             | 0.5052          |\\n| 0.5311 | 4630  | -             | 0.5046          |\\n| 0.5323 | 4640  | -             | 0.5039          |\\n| 0.5334 | 4650  | -             | 0.5033          |\\n| 0.5346 | 4660  | -             | 0.5030          |\\n| 0.5357 | 4670  | -             | 0.5028          |\\n| 0.5369 | 4680  | -             | 0.5027          |\\n| 0.5380 | 4690  | -             | 0.5023          |\\n| 0.5392 | 4700  | 0.9047        | 0.5020          |\\n| 0.5403 | 4710  | -             | 0.5018          |\\n| 0.5415 | 4720  | -             | 0.5015          |\\n| 0.5426 | 4730  | -             | 0.5009          |\\n| 0.5438 | 4740  | -             | 0.5003          |\\n| 0.5449 | 4750  | -             | 0.4997          |\\n| 0.5461 | 4760  | -             | 0.4991          |\\n| 0.5472 | 4770  | -             | 0.4984          |\\n| 0.5484 | 4780  | -             | 0.4980          |\\n| 0.5495 | 4790  | -             | 0.4980          |\\n| 0.5506 | 4800  | 0.887         | 0.4979          |\\n| 0.5518 | 4810  | -             | 0.4975          |\\n| 0.5529 | 4820  | -             | 0.4973          |\\n| 0.5541 | 4830  | -             | 0.4969          |\\n| 0.5552 | 4840  | -             | 0.4966          |\\n| 0.5564 | 4850  | -             | 0.4964          |\\n| 0.5575 | 4860  | -             | 0.4964          |\\n| 0.5587 | 4870  | -             | 0.4960          |\\n| 0.5598 | 4880  | -             | 0.4957          |\\n| 0.5610 | 4890  | -             | 0.4955          |\\n| 0.5621 | 4900  | 0.8645        | 0.4952          |\\n| 0.5633 | 4910  | -             | 0.4950          |\\n| 0.5644 | 4920  | -             | 0.4952          |\\n| 0.5656 | 4930  | -             | 0.4949          |\\n| 0.5667 | 4940  | -             | 0.4943          |\\n| 0.5679 | 4950  | -             | 0.4938          |\\n| 0.5690 | 4960  | -             | 0.4936          |\\n| 0.5702 | 4970  | -             | 0.4933          |\\n| 0.5713 | 4980  | -             | 0.4931          |\\n| 0.5724 | 4990  | -             | 0.4929          |\\n| 0.5736 | 5000  | 0.8348        | 0.4924          |\\n| 0.5747 | 5010  | -             | 0.4921          |\\n| 0.5759 | 5020  | -             | 0.4915          |\\n| 0.5770 | 5030  | -             | 0.4911          |\\n| 0.5782 | 5040  | -             | 0.4909          |\\n| 0.5793 | 5050  | -             | 0.4905          |\\n| 0.5805 | 5060  | -             | 0.4900          |\\n| 0.5816 | 5070  | -             | 0.4892          |\\n| 0.5828 | 5080  | -             | 0.4886          |\\n| 0.5839 | 5090  | -             | 0.4883          |\\n| 0.5851 | 5100  | 0.871         | 0.4879          |\\n| 0.5862 | 5110  | -             | 0.4877          |\\n| 0.5874 | 5120  | -             | 0.4874          |\\n| 0.5885 | 5130  | -             | 0.4870          |\\n| 0.5897 | 5140  | -             | 0.4867          |\\n| 0.5908 | 5150  | -             | 0.4864          |\\n| 0.5919 | 5160  | -             | 0.4862          |\\n| 0.5931 | 5170  | -             | 0.4860          |\\n| 0.5942 | 5180  | -             | 0.4857          |\\n| 0.5954 | 5190  | -             | 0.4855          |\\n| 0.5965 | 5200  | 0.8522        | 0.4850          |\\n| 0.5977 | 5210  | -             | 0.4846          |\\n| 0.5988 | 5220  | -             | 0.4844          |\\n| 0.6000 | 5230  | -             | 0.4842          |\\n| 0.6011 | 5240  | -             | 0.4837          |\\n| 0.6023 | 5250  | -             | 0.4835          |\\n| 0.6034 | 5260  | -             | 0.4831          |\\n| 0.6046 | 5270  | -             | 0.4826          |\\n| 0.6057 | 5280  | -             | 0.4822          |\\n| 0.6069 | 5290  | -             | 0.4822          |\\n| 0.6080 | 5300  | 0.869         | 0.4820          |\\n| 0.6092 | 5310  | -             | 0.4818          |\\n| 0.6103 | 5320  | -             | 0.4819          |\\n| 0.6114 | 5330  | -             | 0.4819          |\\n| 0.6126 | 5340  | -             | 0.4815          |\\n| 0.6137 | 5350  | -             | 0.4813          |\\n| 0.6149 | 5360  | -             | 0.4812          |\\n| 0.6160 | 5370  | -             | 0.4810          |\\n| 0.6172 | 5380  | -             | 0.4809          |\\n| 0.6183 | 5390  | -             | 0.4806          |\\n| 0.6195 | 5400  | 0.8548        | 0.4805          |\\n| 0.6206 | 5410  | -             | 0.4800          |\\n| 0.6218 | 5420  | -             | 0.4798          |\\n| 0.6229 | 5430  | -             | 0.4795          |\\n| 0.6241 | 5440  | -             | 0.4792          |\\n| 0.6252 | 5450  | -             | 0.4790          |\\n| 0.6264 | 5460  | -             | 0.4790          |\\n| 0.6275 | 5470  | -             | 0.4791          |\\n| 0.6287 | 5480  | -             | 0.4794          |\\n| 0.6298 | 5490  | -             | 0.4792          |\\n| 0.6310 | 5500  | 0.8366        | 0.4790          |\\n| 0.6321 | 5510  | -             | 0.4786          |\\n| 0.6332 | 5520  | -             | 0.4780          |\\n| 0.6344 | 5530  | -             | 0.4773          |\\n| 0.6355 | 5540  | -             | 0.4768          |\\n| 0.6367 | 5550  | -             | 0.4767          |\\n| 0.6378 | 5560  | -             | 0.4765          |\\n| 0.6390 | 5570  | -             | 0.4765          |\\n| 0.6401 | 5580  | -             | 0.4763          |\\n| 0.6413 | 5590  | -             | 0.4760          |\\n| 0.6424 | 5600  | 0.8696        | 0.4757          |\\n| 0.6436 | 5610  | -             | 0.4754          |\\n| 0.6447 | 5620  | -             | 0.4752          |\\n| 0.6459 | 5630  | -             | 0.4751          |\\n| 0.6470 | 5640  | -             | 0.4747          |\\n| 0.6482 | 5650  | -             | 0.4747          |\\n| 0.6493 | 5660  | -             | 0.4742          |\\n| 0.6505 | 5670  | -             | 0.4740          |\\n| 0.6516 | 5680  | -             | 0.4736          |\\n| 0.6527 | 5690  | -             | 0.4730          |\\n| 0.6539 | 5700  | 0.8302        | 0.4725          |\\n| 0.6550 | 5710  | -             | 0.4723          |\\n| 0.6562 | 5720  | -             | 0.4720          |\\n| 0.6573 | 5730  | -             | 0.4718          |\\n| 0.6585 | 5740  | -             | 0.4715          |\\n| 0.6596 | 5750  | -             | 0.4714          |\\n| 0.6608 | 5760  | -             | 0.4711          |\\n| 0.6619 | 5770  | -             | 0.4707          |\\n| 0.6631 | 5780  | -             | 0.4707          |\\n| 0.6642 | 5790  | -             | 0.4703          |\\n| 0.6654 | 5800  | 0.8128        | 0.4703          |\\n| 0.6665 | 5810  | -             | 0.4701          |\\n| 0.6677 | 5820  | -             | 0.4699          |\\n| 0.6688 | 5830  | -             | 0.4697          |\\n| 0.6700 | 5840  | -             | 0.4698          |\\n| 0.6711 | 5850  | -             | 0.4695          |\\n| 0.6722 | 5860  | -             | 0.4691          |\\n| 0.6734 | 5870  | -             | 0.4689          |\\n| 0.6745 | 5880  | -             | 0.4689          |\\n| 0.6757 | 5890  | -             | 0.4688          |\\n| 0.6768 | 5900  | 0.8437        | 0.4683          |\\n| 0.6780 | 5910  | -             | 0.4683          |\\n| 0.6791 | 5920  | -             | 0.4681          |\\n| 0.6803 | 5930  | -             | 0.4678          |\\n| 0.6814 | 5940  | -             | 0.4677          |\\n| 0.6826 | 5950  | -             | 0.4676          |\\n| 0.6837 | 5960  | -             | 0.4673          |\\n| 0.6849 | 5970  | -             | 0.4668          |\\n| 0.6860 | 5980  | -             | 0.4667          |\\n| 0.6872 | 5990  | -             | 0.4661          |\\n| 0.6883 | 6000  | 0.7774        | 0.4657          |\\n| 0.6895 | 6010  | -             | 0.4654          |\\n| 0.6906 | 6020  | -             | 0.4650          |\\n| 0.6918 | 6030  | -             | 0.4648          |\\n| 0.6929 | 6040  | -             | 0.4646          |\\n| 0.6940 | 6050  | -             | 0.4644          |\\n| 0.6952 | 6060  | -             | 0.4643          |\\n| 0.6963 | 6070  | -             | 0.4641          |\\n| 0.6975 | 6080  | -             | 0.4640          |\\n| 0.6986 | 6090  | -             | 0.4638          |\\n| 0.6998 | 6100  | 0.834         | 0.4637          |\\n| 0.7009 | 6110  | -             | 0.4633          |\\n| 0.7021 | 6120  | -             | 0.4632          |\\n| 0.7032 | 6130  | -             | 0.4631          |\\n| 0.7044 | 6140  | -             | 0.4628          |\\n| 0.7055 | 6150  | -             | 0.4627          |\\n| 0.7067 | 6160  | -             | 0.4623          |\\n| 0.7078 | 6170  | -             | 0.4617          |\\n| 0.7090 | 6180  | -             | 0.4615          |\\n| 0.7101 | 6190  | -             | 0.4614          |\\n| 0.7113 | 6200  | 0.8118        | 0.4612          |\\n| 0.7124 | 6210  | -             | 0.4612          |\\n| 0.7135 | 6220  | -             | 0.4612          |\\n| 0.7147 | 6230  | -             | 0.4610          |\\n| 0.7158 | 6240  | -             | 0.4609          |\\n| 0.7170 | 6250  | -             | 0.4610          |\\n| 0.7181 | 6260  | -             | 0.4611          |\\n| 0.7193 | 6270  | -             | 0.4607          |\\n| 0.7204 | 6280  | -             | 0.4599          |\\n| 0.7216 | 6290  | -             | 0.4598          |\\n| 0.7227 | 6300  | 0.7884        | 0.4600          |\\n| 0.7239 | 6310  | -             | 0.4599          |\\n| 0.7250 | 6320  | -             | 0.4600          |\\n| 0.7262 | 6330  | -             | 0.4601          |\\n| 0.7273 | 6340  | -             | 0.4603          |\\n| 0.7285 | 6350  | -             | 0.4603          |\\n| 0.7296 | 6360  | -             | 0.4598          |\\n| 0.7308 | 6370  | -             | 0.4597          |\\n| 0.7319 | 6380  | -             | 0.4596          |\\n| 0.7331 | 6390  | -             | 0.4594          |\\n| 0.7342 | 6400  | 0.8092        | 0.4590          |\\n| 0.7353 | 6410  | -             | 0.4588          |\\n| 0.7365 | 6420  | -             | 0.4585          |\\n| 0.7376 | 6430  | -             | 0.4584          |\\n| 0.7388 | 6440  | -             | 0.4580          |\\n| 0.7399 | 6450  | -             | 0.4574          |\\n| 0.7411 | 6460  | -             | 0.4570          |\\n| 0.7422 | 6470  | -             | 0.4566          |\\n| 0.7434 | 6480  | -             | 0.4563          |\\n| 0.7445 | 6490  | -             | 0.4560          |\\n| 0.7457 | 6500  | 0.8195        | 0.4557          |\\n| 0.7468 | 6510  | -             | 0.4556          |\\n| 0.7480 | 6520  | -             | 0.4554          |\\n| 0.7491 | 6530  | -             | 0.4551          |\\n| 0.7503 | 6540  | -             | 0.4548          |\\n| 0.7514 | 6550  | -             | 0.4545          |\\n| 0.7526 | 6560  | -             | 0.4543          |\\n| 0.7537 | 6570  | -             | 0.4541          |\\n| 0.7548 | 6580  | -             | 0.4540          |\\n| 0.7560 | 6590  | -             | 0.4538          |\\n| 0.7571 | 6600  | 0.8163        | 0.4535          |\\n| 0.7583 | 6610  | -             | 0.4533          |\\n| 0.7594 | 6620  | -             | 0.4536          |\\n| 0.7606 | 6630  | -             | 0.4535          |\\n| 0.7617 | 6640  | -             | 0.4533          |\\n| 0.7629 | 6650  | -             | 0.4532          |\\n| 0.7640 | 6660  | -             | 0.4531          |\\n| 0.7652 | 6670  | -             | 0.4531          |\\n| 0.7663 | 6680  | -             | 0.4530          |\\n| 0.7675 | 6690  | -             | 0.4528          |\\n| 0.7686 | 6700  | 0.8091        | 0.4527          |\\n| 0.7698 | 6710  | -             | 0.4527          |\\n| 0.7709 | 6720  | -             | 0.4526          |\\n| 0.7721 | 6730  | -             | 0.4525          |\\n| 0.7732 | 6740  | -             | 0.4524          |\\n| 0.7743 | 6750  | -             | 0.4521          |\\n| 0.7755 | 6760  | -             | 0.4517          |\\n| 0.7766 | 6770  | -             | 0.4514          |\\n| 0.7778 | 6780  | -             | 0.4512          |\\n| 0.7789 | 6790  | -             | 0.4514          |\\n| 0.7801 | 6800  | 0.8098        | 0.4515          |\\n| 0.7812 | 6810  | -             | 0.4514          |\\n| 0.7824 | 6820  | -             | 0.4511          |\\n| 0.7835 | 6830  | -             | 0.4507          |\\n| 0.7847 | 6840  | -             | 0.4505          |\\n| 0.7858 | 6850  | -             | 0.4504          |\\n| 0.7870 | 6860  | -             | 0.4503          |\\n| 0.7881 | 6870  | -             | 0.4500          |\\n| 0.7893 | 6880  | -             | 0.4498          |\\n| 0.7904 | 6890  | -             | 0.4495          |\\n| 0.7916 | 6900  | 0.7857        | 0.4491          |\\n| 0.7927 | 6910  | -             | 0.4490          |\\n| 0.7939 | 6920  | -             | 0.4488          |\\n| 0.7950 | 6930  | -             | 0.4488          |\\n| 0.7961 | 6940  | -             | 0.4488          |\\n| 0.7973 | 6950  | -             | 0.4487          |\\n| 0.7984 | 6960  | -             | 0.4484          |\\n| 0.7996 | 6970  | -             | 0.4482          |\\n| 0.8007 | 6980  | -             | 0.4483          |\\n| 0.8019 | 6990  | -             | 0.4481          |\\n| 0.8030 | 7000  | 0.7817        | 0.4477          |\\n| 0.8042 | 7010  | -             | 0.4476          |\\n| 0.8053 | 7020  | -             | 0.4471          |\\n| 0.8065 | 7030  | -             | 0.4469          |\\n| 0.8076 | 7040  | -             | 0.4468          |\\n| 0.8088 | 7050  | -             | 0.4465          |\\n| 0.8099 | 7060  | -             | 0.4460          |\\n| 0.8111 | 7070  | -             | 0.4458          |\\n| 0.8122 | 7080  | -             | 0.4458          |\\n| 0.8134 | 7090  | -             | 0.4454          |\\n| 0.8145 | 7100  | 0.779         | 0.4452          |\\n| 0.8156 | 7110  | -             | 0.4449          |\\n| 0.8168 | 7120  | -             | 0.4448          |\\n| 0.8179 | 7130  | -             | 0.4446          |\\n| 0.8191 | 7140  | -             | 0.4442          |\\n| 0.8202 | 7150  | -             | 0.4442          |\\n| 0.8214 | 7160  | -             | 0.4441          |\\n| 0.8225 | 7170  | -             | 0.4440          |\\n| 0.8237 | 7180  | -             | 0.4437          |\\n| 0.8248 | 7190  | -             | 0.4434          |\\n| 0.8260 | 7200  | 0.7807        | 0.4434          |\\n| 0.8271 | 7210  | -             | 0.4435          |\\n| 0.8283 | 7220  | -             | 0.4433          |\\n| 0.8294 | 7230  | -             | 0.4431          |\\n| 0.8306 | 7240  | -             | 0.4430          |\\n| 0.8317 | 7250  | -             | 0.4428          |\\n| 0.8329 | 7260  | -             | 0.4426          |\\n| 0.8340 | 7270  | -             | 0.4424          |\\n| 0.8351 | 7280  | -             | 0.4428          |\\n| 0.8363 | 7290  | -             | 0.4426          |\\n| 0.8374 | 7300  | 0.7724        | 0.4423          |\\n| 0.8386 | 7310  | -             | 0.4419          |\\n| 0.8397 | 7320  | -             | 0.4418          |\\n| 0.8409 | 7330  | -             | 0.4417          |\\n| 0.8420 | 7340  | -             | 0.4415          |\\n| 0.8432 | 7350  | -             | 0.4413          |\\n| 0.8443 | 7360  | -             | 0.4409          |\\n| 0.8455 | 7370  | -             | 0.4406          |\\n| 0.8466 | 7380  | -             | 0.4405          |\\n| 0.8478 | 7390  | -             | 0.4400          |\\n| 0.8489 | 7400  | 0.7898        | 0.4393          |\\n| 0.8501 | 7410  | -             | 0.4389          |\\n| 0.8512 | 7420  | -             | 0.4384          |\\n| 0.8524 | 7430  | -             | 0.4381          |\\n| 0.8535 | 7440  | -             | 0.4380          |\\n| 0.8547 | 7450  | -             | 0.4380          |\\n| 0.8558 | 7460  | -             | 0.4379          |\\n| 0.8569 | 7470  | -             | 0.4377          |\\n| 0.8581 | 7480  | -             | 0.4377          |\\n| 0.8592 | 7490  | -             | 0.4376          |\\n| 0.8604 | 7500  | 0.8009        | 0.4375          |\\n| 0.8615 | 7510  | -             | 0.4371          |\\n| 0.8627 | 7520  | -             | 0.4369          |\\n| 0.8638 | 7530  | -             | 0.4365          |\\n| 0.8650 | 7540  | -             | 0.4362          |\\n| 0.8661 | 7550  | -             | 0.4359          |\\n| 0.8673 | 7560  | -             | 0.4357          |\\n| 0.8684 | 7570  | -             | 0.4355          |\\n| 0.8696 | 7580  | -             | 0.4351          |\\n| 0.8707 | 7590  | -             | 0.4347          |\\n| 0.8719 | 7600  | 0.7847        | 0.4346          |\\n| 0.8730 | 7610  | -             | 0.4346          |\\n| 0.8742 | 7620  | -             | 0.4344          |\\n| 0.8753 | 7630  | -             | 0.4343          |\\n| 0.8764 | 7640  | -             | 0.4338          |\\n| 0.8776 | 7650  | -             | 0.4336          |\\n| 0.8787 | 7660  | -             | 0.4332          |\\n| 0.8799 | 7670  | -             | 0.4331          |\\n| 0.8810 | 7680  | -             | 0.4329          |\\n| 0.8822 | 7690  | -             | 0.4326          |\\n| 0.8833 | 7700  | 0.7668        | 0.4324          |\\n| 0.8845 | 7710  | -             | 0.4325          |\\n| 0.8856 | 7720  | -             | 0.4327          |\\n| 0.8868 | 7730  | -             | 0.4329          |\\n| 0.8879 | 7740  | -             | 0.4328          |\\n| 0.8891 | 7750  | -             | 0.4325          |\\n| 0.8902 | 7760  | -             | 0.4325          |\\n| 0.8914 | 7770  | -             | 0.4326          |\\n| 0.8925 | 7780  | -             | 0.4324          |\\n| 0.8937 | 7790  | -             | 0.4322          |\\n| 0.8948 | 7800  | 0.7987        | 0.4320          |\\n| 0.8960 | 7810  | -             | 0.4319          |\\n| 0.8971 | 7820  | -             | 0.4318          |\\n| 0.8982 | 7830  | -             | 0.4315          |\\n| 0.8994 | 7840  | -             | 0.4312          |\\n| 0.9005 | 7850  | -             | 0.4308          |\\n| 0.9017 | 7860  | -             | 0.4308          |\\n| 0.9028 | 7870  | -             | 0.4309          |\\n| 0.9040 | 7880  | -             | 0.4306          |\\n| 0.9051 | 7890  | -             | 0.4305          |\\n| 0.9063 | 7900  | 0.7691        | 0.4305          |\\n| 0.9074 | 7910  | -             | 0.4305          |\\n| 0.9086 | 7920  | -             | 0.4308          |\\n| 0.9097 | 7930  | -             | 0.4309          |\\n| 0.9109 | 7940  | -             | 0.4309          |\\n| 0.9120 | 7950  | -             | 0.4305          |\\n| 0.9132 | 7960  | -             | 0.4297          |\\n| 0.9143 | 7970  | -             | 0.4294          |\\n| 0.9155 | 7980  | -             | 0.4292          |\\n| 0.9166 | 7990  | -             | 0.4292          |\\n| 0.9177 | 8000  | 0.7828        | 0.4289          |\\n| 0.9189 | 8010  | -             | 0.4288          |\\n| 0.9200 | 8020  | -             | 0.4289          |\\n| 0.9212 | 8030  | -             | 0.4285          |\\n| 0.9223 | 8040  | -             | 0.4286          |\\n| 0.9235 | 8050  | -             | 0.4289          |\\n| 0.9246 | 8060  | -             | 0.4288          |\\n| 0.9258 | 8070  | -             | 0.4290          |\\n| 0.9269 | 8080  | -             | 0.4289          |\\n| 0.9281 | 8090  | -             | 0.4287          |\\n| 0.9292 | 8100  | 0.7544        | 0.4288          |\\n| 0.9304 | 8110  | -             | 0.4284          |\\n| 0.9315 | 8120  | -             | 0.4287          |\\n| 0.9327 | 8130  | -             | 0.4289          |\\n| 0.9338 | 8140  | -             | 0.4293          |\\n| 0.9350 | 8150  | -             | 0.4292          |\\n| 0.9361 | 8160  | -             | 0.4289          |\\n| 0.9372 | 8170  | -             | 0.4286          |\\n| 0.9384 | 8180  | -             | 0.4280          |\\n| 0.9395 | 8190  | -             | 0.4281          |\\n| 0.9407 | 8200  | 0.7502        | 0.4281          |\\n| 0.9418 | 8210  | -             | 0.4278          |\\n| 0.9430 | 8220  | -             | 0.4276          |\\n| 0.9441 | 8230  | -             | 0.4274          |\\n| 0.9453 | 8240  | -             | 0.4270          |\\n| 0.9464 | 8250  | -             | 0.4267          |\\n| 0.9476 | 8260  | -             | 0.4263          |\\n| 0.9487 | 8270  | -             | 0.4261          |\\n| 0.9499 | 8280  | -             | 0.4257          |\\n| 0.9510 | 8290  | -             | 0.4254          |\\n| 0.9522 | 8300  | 0.7818        | 0.4255          |\\n| 0.9533 | 8310  | -             | 0.4255          |\\n| 0.9545 | 8320  | -             | 0.4254          |\\n| 0.9556 | 8330  | -             | 0.4252          |\\n| 0.9568 | 8340  | -             | 0.4249          |\\n| 0.9579 | 8350  | -             | 0.4249          |\\n| 0.9590 | 8360  | -             | 0.4248          |\\n| 0.9602 | 8370  | -             | 0.4249          |\\n| 0.9613 | 8380  | -             | 0.4248          |\\n| 0.9625 | 8390  | -             | 0.4246          |\\n| 0.9636 | 8400  | 0.7606        | 0.4243          |\\n| 0.9648 | 8410  | -             | 0.4242          |\\n| 0.9659 | 8420  | -             | 0.4240          |\\n| 0.9671 | 8430  | -             | 0.4239          |\\n| 0.9682 | 8440  | -             | 0.4238          |\\n| 0.9694 | 8450  | -             | 0.4238          |\\n| 0.9705 | 8460  | -             | 0.4237          |\\n| 0.9717 | 8470  | -             | 0.4236          |\\n| 0.9728 | 8480  | -             | 0.4232          |\\n| 0.9740 | 8490  | -             | 0.4229          |\\n| 0.9751 | 8500  | 0.7416        | 0.4227          |\\n| 0.9763 | 8510  | -             | 0.4226          |\\n| 0.9774 | 8520  | -             | 0.4220          |\\n| 0.9785 | 8530  | -             | 0.4218          |\\n| 0.9797 | 8540  | -             | 0.4217          |\\n| 0.9808 | 8550  | -             | 0.4217          |\\n| 0.9820 | 8560  | -             | 0.4215          |\\n| 0.9831 | 8570  | -             | 0.4216          |\\n| 0.9843 | 8580  | -             | 0.4217          |\\n| 0.9854 | 8590  | -             | 0.4216          |\\n| 0.9866 | 8600  | 0.748         | 0.4217          |\\n| 0.9877 | 8610  | -             | 0.4215          |\\n| 0.9889 | 8620  | -             | 0.4216          |\\n| 0.9900 | 8630  | -             | 0.4218          |\\n| 0.9912 | 8640  | -             | 0.4218          |\\n| 0.9923 | 8650  | -             | 0.4219          |\\n| 0.9935 | 8660  | -             | 0.4217          |\\n| 0.9946 | 8670  | -             | 0.4217          |\\n| 0.9958 | 8680  | -             | 0.4214          |\\n| 0.9969 | 8690  | -             | 0.4210          |\\n| 0.9980 | 8700  | 0.7553        | 0.4205          |\\n| 0.9992 | 8710  | -             | 0.4200          |\\n| 1.0003 | 8720  | -             | 0.4199          |\\n| 1.0015 | 8730  | -             | 0.4199          |\\n| 1.0026 | 8740  | -             | 0.4199          |\\n| 1.0038 | 8750  | -             | 0.4198          |\\n| 1.0049 | 8760  | -             | 0.4200          |\\n| 1.0061 | 8770  | -             | 0.4198          |\\n| 1.0072 | 8780  | -             | 0.4195          |\\n| 1.0084 | 8790  | -             | 0.4194          |\\n| 1.0095 | 8800  | 0.7202        | 0.4191          |\\n| 1.0107 | 8810  | -             | 0.4190          |\\n| 1.0118 | 8820  | -             | 0.4188          |\\n| 1.0130 | 8830  | -             | 0.4188          |\\n| 1.0141 | 8840  | -             | 0.4192          |\\n| 1.0153 | 8850  | -             | 0.4190          |\\n| 1.0164 | 8860  | -             | 0.4191          |\\n| 1.0176 | 8870  | -             | 0.4190          |\\n| 1.0187 | 8880  | -             | 0.4192          |\\n| 1.0198 | 8890  | -             | 0.4190          |\\n| 1.0210 | 8900  | 0.7567        | 0.4189          |\\n| 1.0221 | 8910  | -             | 0.4188          |\\n| 1.0233 | 8920  | -             | 0.4189          |\\n| 1.0244 | 8930  | -             | 0.4188          |\\n| 1.0256 | 8940  | -             | 0.4187          |\\n| 1.0267 | 8950  | -             | 0.4183          |\\n| 1.0279 | 8960  | -             | 0.4182          |\\n| 1.0290 | 8970  | -             | 0.4182          |\\n| 1.0302 | 8980  | -             | 0.4184          |\\n| 1.0313 | 8990  | -             | 0.4181          |\\n| 1.0325 | 9000  | 0.7345        | 0.4177          |\\n| 1.0336 | 9010  | -             | 0.4173          |\\n| 1.0348 | 9020  | -             | 0.4171          |\\n| 1.0359 | 9030  | -             | 0.4172          |\\n| 1.0371 | 9040  | -             | 0.4171          |\\n| 1.0382 | 9050  | -             | 0.4172          |\\n| 1.0393 | 9060  | -             | 0.4172          |\\n| 1.0405 | 9070  | -             | 0.4170          |\\n| 1.0416 | 9080  | -             | 0.4165          |\\n| 1.0428 | 9090  | -             | 0.4162          |\\n| 1.0439 | 9100  | 0.7344        | 0.4162          |\\n| 1.0451 | 9110  | -             | 0.4160          |\\n| 1.0462 | 9120  | -             | 0.4158          |\\n| 1.0474 | 9130  | -             | 0.4157          |\\n| 1.0485 | 9140  | -             | 0.4157          |\\n| 1.0497 | 9150  | -             | 0.4156          |\\n| 1.0508 | 9160  | -             | 0.4153          |\\n| 1.0520 | 9170  | -             | 0.4153          |\\n| 1.0531 | 9180  | -             | 0.4154          |\\n| 1.0543 | 9190  | -             | 0.4154          |\\n| 1.0554 | 9200  | 0.7233        | 0.4157          |\\n| 1.0566 | 9210  | -             | 0.4157          |\\n| 1.0577 | 9220  | -             | 0.4156          |\\n| 1.0589 | 9230  | -             | 0.4155          |\\n| 1.0600 | 9240  | -             | 0.4153          |\\n| 1.0611 | 9250  | -             | 0.4154          |\\n| 1.0623 | 9260  | -             | 0.4155          |\\n| 1.0634 | 9270  | -             | 0.4154          |\\n| 1.0646 | 9280  | -             | 0.4151          |\\n| 1.0657 | 9290  | -             | 0.4149          |\\n| 1.0669 | 9300  | 0.7442        | 0.4148          |\\n| 1.0680 | 9310  | -             | 0.4144          |\\n| 1.0692 | 9320  | -             | 0.4143          |\\n| 1.0703 | 9330  | -             | 0.4141          |\\n| 1.0715 | 9340  | -             | 0.4140          |\\n| 1.0726 | 9350  | -             | 0.4138          |\\n| 1.0738 | 9360  | -             | 0.4136          |\\n| 1.0749 | 9370  | -             | 0.4133          |\\n| 1.0761 | 9380  | -             | 0.4132          |\\n| 1.0772 | 9390  | -             | 0.4130          |\\n| 1.0784 | 9400  | 0.722         | 0.4129          |\\n| 1.0795 | 9410  | -             | 0.4131          |\\n| 1.0806 | 9420  | -             | 0.4132          |\\n| 1.0818 | 9430  | -             | 0.4133          |\\n| 1.0829 | 9440  | -             | 0.4134          |\\n| 1.0841 | 9450  | -             | 0.4134          |\\n| 1.0852 | 9460  | -             | 0.4133          |\\n| 1.0864 | 9470  | -             | 0.4132          |\\n| 1.0875 | 9480  | -             | 0.4132          |\\n| 1.0887 | 9490  | -             | 0.4134          |\\n| 1.0898 | 9500  | 0.7433        | 0.4133          |\\n| 1.0910 | 9510  | -             | 0.4133          |\\n| 1.0921 | 9520  | -             | 0.4133          |\\n| 1.0933 | 9530  | -             | 0.4132          |\\n| 1.0944 | 9540  | -             | 0.4131          |\\n| 1.0956 | 9550  | -             | 0.4130          |\\n| 1.0967 | 9560  | -             | 0.4130          |\\n| 1.0979 | 9570  | -             | 0.4126          |\\n| 1.0990 | 9580  | -             | 0.4125          |\\n| 1.1001 | 9590  | -             | 0.4121          |\\n| 1.1013 | 9600  | 0.746         | 0.4119          |\\n| 1.1024 | 9610  | -             | 0.4117          |\\n| 1.1036 | 9620  | -             | 0.4112          |\\n| 1.1047 | 9630  | -             | 0.4109          |\\n| 1.1059 | 9640  | -             | 0.4106          |\\n| 1.1070 | 9650  | -             | 0.4101          |\\n| 1.1082 | 9660  | -             | 0.4101          |\\n| 1.1093 | 9670  | -             | 0.4102          |\\n| 1.1105 | 9680  | -             | 0.4102          |\\n| 1.1116 | 9690  | -             | 0.4101          |\\n| 1.1128 | 9700  | 0.7447        | 0.4099          |\\n| 1.1139 | 9710  | -             | 0.4100          |\\n| 1.1151 | 9720  | -             | 0.4098          |\\n| 1.1162 | 9730  | -             | 0.4097          |\\n| 1.1174 | 9740  | -             | 0.4094          |\\n| 1.1185 | 9750  | -             | 0.4097          |\\n| 1.1197 | 9760  | -             | 0.4096          |\\n| 1.1208 | 9770  | -             | 0.4096          |\\n| 1.1219 | 9780  | -             | 0.4097          |\\n| 1.1231 | 9790  | -             | 0.4097          |\\n| 1.1242 | 9800  | 0.7234        | 0.4094          |\\n| 1.1254 | 9810  | -             | 0.4090          |\\n| 1.1265 | 9820  | -             | 0.4090          |\\n| 1.1277 | 9830  | -             | 0.4091          |\\n| 1.1288 | 9840  | -             | 0.4091          |\\n| 1.1300 | 9850  | -             | 0.4090          |\\n| 1.1311 | 9860  | -             | 0.4088          |\\n| 1.1323 | 9870  | -             | 0.4088          |\\n| 1.1334 | 9880  | -             | 0.4085          |\\n| 1.1346 | 9890  | -             | 0.4085          |\\n| 1.1357 | 9900  | 0.7054        | 0.4084          |\\n| 1.1369 | 9910  | -             | 0.4087          |\\n| 1.1380 | 9920  | -             | 0.4089          |\\n| 1.1392 | 9930  | -             | 0.4089          |\\n| 1.1403 | 9940  | -             | 0.4088          |\\n| 1.1414 | 9950  | -             | 0.4091          |\\n| 1.1426 | 9960  | -             | 0.4088          |\\n| 1.1437 | 9970  | -             | 0.4086          |\\n| 1.1449 | 9980  | -             | 0.4084          |\\n| 1.1460 | 9990  | -             | 0.4089          |\\n| 1.1472 | 10000 | 0.7071        | 0.4088          |\\n| 1.1483 | 10010 | -             | 0.4086          |\\n| 1.1495 | 10020 | -             | 0.4081          |\\n| 1.1506 | 10030 | -             | 0.4079          |\\n| 1.1518 | 10040 | -             | 0.4079          |\\n| 1.1529 | 10050 | -             | 0.4081          |\\n| 1.1541 | 10060 | -             | 0.4081          |\\n| 1.1552 | 10070 | -             | 0.4080          |\\n| 1.1564 | 10080 | -             | 0.4079          |\\n| 1.1575 | 10090 | -             | 0.4078          |\\n| 1.1587 | 10100 | 0.7289        | 0.4075          |\\n| 1.1598 | 10110 | -             | 0.4072          |\\n| 1.1609 | 10120 | -             | 0.4070          |\\n| 1.1621 | 10130 | -             | 0.4070          |\\n| 1.1632 | 10140 | -             | 0.4074          |\\n| 1.1644 | 10150 | -             | 0.4074          |\\n| 1.1655 | 10160 | -             | 0.4073          |\\n| 1.1667 | 10170 | -             | 0.4073          |\\n| 1.1678 | 10180 | -             | 0.4072          |\\n| 1.1690 | 10190 | -             | 0.4073          |\\n| 1.1701 | 10200 | 0.758         | 0.4071          |\\n| 1.1713 | 10210 | -             | 0.4071          |\\n| 1.1724 | 10220 | -             | 0.4071          |\\n| 1.1736 | 10230 | -             | 0.4068          |\\n| 1.1747 | 10240 | -             | 0.4063          |\\n| 1.1759 | 10250 | -             | 0.4062          |\\n| 1.1770 | 10260 | -             | 0.4064          |\\n| 1.1782 | 10270 | -             | 0.4065          |\\n| 1.1793 | 10280 | -             | 0.4063          |\\n| 1.1805 | 10290 | -             | 0.4065          |\\n| 1.1816 | 10300 | 0.7322        | 0.4066          |\\n| 1.1827 | 10310 | -             | 0.4065          |\\n| 1.1839 | 10320 | -             | 0.4065          |\\n| 1.1850 | 10330 | -             | 0.4061          |\\n| 1.1862 | 10340 | -             | 0.4060          |\\n| 1.1873 | 10350 | -             | 0.4057          |\\n| 1.1885 | 10360 | -             | 0.4056          |\\n| 1.1896 | 10370 | -             | 0.4056          |\\n| 1.1908 | 10380 | -             | 0.4059          |\\n| 1.1919 | 10390 | -             | 0.4061          |\\n| 1.1931 | 10400 | 0.6948        | 0.4059          |\\n| 1.1942 | 10410 | -             | 0.4059          |\\n| 1.1954 | 10420 | -             | 0.4060          |\\n| 1.1965 | 10430 | -             | 0.4058          |\\n| 1.1977 | 10440 | -             | 0.4057          |\\n| 1.1988 | 10450 | -             | 0.4056          |\\n| 1.2000 | 10460 | -             | 0.4056          |\\n| 1.2011 | 10470 | -             | 0.4056          |\\n| 1.2022 | 10480 | -             | 0.4057          |\\n| 1.2034 | 10490 | -             | 0.4056          |\\n| 1.2045 | 10500 | 0.7185        | 0.4055          |\\n| 1.2057 | 10510 | -             | 0.4056          |\\n| 1.2068 | 10520 | -             | 0.4054          |\\n| 1.2080 | 10530 | -             | 0.4053          |\\n| 1.2091 | 10540 | -             | 0.4051          |\\n| 1.2103 | 10550 | -             | 0.4050          |\\n| 1.2114 | 10560 | -             | 0.4051          |\\n| 1.2126 | 10570 | -             | 0.4052          |\\n| 1.2137 | 10580 | -             | 0.4053          |\\n| 1.2149 | 10590 | -             | 0.4053          |\\n| 1.2160 | 10600 | 0.7039        | 0.4053          |\\n| 1.2172 | 10610 | -             | 0.4054          |\\n| 1.2183 | 10620 | -             | 0.4051          |\\n| 1.2195 | 10630 | -             | 0.4050          |\\n| 1.2206 | 10640 | -             | 0.4048          |\\n| 1.2218 | 10650 | -             | 0.4044          |\\n| 1.2229 | 10660 | -             | 0.4046          |\\n| 1.2240 | 10670 | -             | 0.4044          |\\n| 1.2252 | 10680 | -             | 0.4041          |\\n| 1.2263 | 10690 | -             | 0.4039          |\\n| 1.2275 | 10700 | 0.6969        | 0.4037          |\\n| 1.2286 | 10710 | -             | 0.4037          |\\n| 1.2298 | 10720 | -             | 0.4035          |\\n| 1.2309 | 10730 | -             | 0.4036          |\\n| 1.2321 | 10740 | -             | 0.4035          |\\n| 1.2332 | 10750 | -             | 0.4038          |\\n| 1.2344 | 10760 | -             | 0.4038          |\\n| 1.2355 | 10770 | -             | 0.4037          |\\n| 1.2367 | 10780 | -             | 0.4037          |\\n| 1.2378 | 10790 | -             | 0.4037          |\\n| 1.2390 | 10800 | 0.6921        | 0.4038          |\\n| 1.2401 | 10810 | -             | 0.4039          |\\n| 1.2413 | 10820 | -             | 0.4038          |\\n| 1.2424 | 10830 | -             | 0.4037          |\\n| 1.2435 | 10840 | -             | 0.4040          |\\n| 1.2447 | 10850 | -             | 0.4042          |\\n| 1.2458 | 10860 | -             | 0.4044          |\\n| 1.2470 | 10870 | -             | 0.4043          |\\n| 1.2481 | 10880 | -             | 0.4043          |\\n| 1.2493 | 10890 | -             | 0.4044          |\\n| 1.2504 | 10900 | 0.728         | 0.4042          |\\n| 1.2516 | 10910 | -             | 0.4044          |\\n| 1.2527 | 10920 | -             | 0.4043          |\\n| 1.2539 | 10930 | -             | 0.4039          |\\n| 1.2550 | 10940 | -             | 0.4038          |\\n| 1.2562 | 10950 | -             | 0.4037          |\\n| 1.2573 | 10960 | -             | 0.4035          |\\n| 1.2585 | 10970 | -             | 0.4032          |\\n| 1.2596 | 10980 | -             | 0.4024          |\\n| 1.2608 | 10990 | -             | 0.4019          |\\n| 1.2619 | 11000 | 0.713         | 0.4018          |\\n| 1.2630 | 11010 | -             | 0.4015          |\\n| 1.2642 | 11020 | -             | 0.4015          |\\n| 1.2653 | 11030 | -             | 0.4014          |\\n| 1.2665 | 11040 | -             | 0.4015          |\\n| 1.2676 | 11050 | -             | 0.4014          |\\n| 1.2688 | 11060 | -             | 0.4013          |\\n| 1.2699 | 11070 | -             | 0.4015          |\\n| 1.2711 | 11080 | -             | 0.4016          |\\n| 1.2722 | 11090 | -             | 0.4017          |\\n| 1.2734 | 11100 | 0.668         | 0.4017          |\\n| 1.2745 | 11110 | -             | 0.4016          |\\n| 1.2757 | 11120 | -             | 0.4016          |\\n| 1.2768 | 11130 | -             | 0.4019          |\\n| 1.2780 | 11140 | -             | 0.4021          |\\n| 1.2791 | 11150 | -             | 0.4019          |\\n| 1.2803 | 11160 | -             | 0.4017          |\\n| 1.2814 | 11170 | -             | 0.4017          |\\n| 1.2826 | 11180 | -             | 0.4018          |\\n| 1.2837 | 11190 | -             | 0.4013          |\\n| 1.2848 | 11200 | 0.7101        | 0.4011          |\\n| 1.2860 | 11210 | -             | 0.4011          |\\n| 1.2871 | 11220 | -             | 0.4014          |\\n| 1.2883 | 11230 | -             | 0.4015          |\\n| 1.2894 | 11240 | -             | 0.4010          |\\n| 1.2906 | 11250 | -             | 0.4012          |\\n| 1.2917 | 11260 | -             | 0.4013          |\\n| 1.2929 | 11270 | -             | 0.4010          |\\n| 1.2940 | 11280 | -             | 0.4006          |\\n| 1.2952 | 11290 | -             | 0.4005          |\\n| 1.2963 | 11300 | 0.6963        | 0.4004          |\\n| 1.2975 | 11310 | -             | 0.4003          |\\n| 1.2986 | 11320 | -             | 0.4004          |\\n| 1.2998 | 11330 | -             | 0.4003          |\\n| 1.3009 | 11340 | -             | 0.3999          |\\n| 1.3021 | 11350 | -             | 0.3997          |\\n| 1.3032 | 11360 | -             | 0.3996          |\\n| 1.3043 | 11370 | -             | 0.3997          |\\n| 1.3055 | 11380 | -             | 0.3996          |\\n| 1.3066 | 11390 | -             | 0.3994          |\\n| 1.3078 | 11400 | 0.6706        | 0.3993          |\\n| 1.3089 | 11410 | -             | 0.3991          |\\n| 1.3101 | 11420 | -             | 0.3990          |\\n| 1.3112 | 11430 | -             | 0.3990          |\\n| 1.3124 | 11440 | -             | 0.3987          |\\n| 1.3135 | 11450 | -             | 0.3981          |\\n| 1.3147 | 11460 | -             | 0.3978          |\\n| 1.3158 | 11470 | -             | 0.3975          |\\n| 1.3170 | 11480 | -             | 0.3974          |\\n| 1.3181 | 11490 | -             | 0.3974          |\\n| 1.3193 | 11500 | 0.6962        | 0.3974          |\\n| 1.3204 | 11510 | -             | 0.3975          |\\n| 1.3216 | 11520 | -             | 0.3975          |\\n| 1.3227 | 11530 | -             | 0.3976          |\\n| 1.3238 | 11540 | -             | 0.3977          |\\n| 1.3250 | 11550 | -             | 0.3975          |\\n| 1.3261 | 11560 | -             | 0.3974          |\\n| 1.3273 | 11570 | -             | 0.3973          |\\n| 1.3284 | 11580 | -             | 0.3971          |\\n| 1.3296 | 11590 | -             | 0.3969          |\\n| 1.3307 | 11600 | 0.7083        | 0.3970          |\\n| 1.3319 | 11610 | -             | 0.3970          |\\n| 1.3330 | 11620 | -             | 0.3971          |\\n| 1.3342 | 11630 | -             | 0.3973          |\\n| 1.3353 | 11640 | -             | 0.3975          |\\n| 1.3365 | 11650 | -             | 0.3973          |\\n| 1.3376 | 11660 | -             | 0.3973          |\\n| 1.3388 | 11670 | -             | 0.3973          |\\n| 1.3399 | 11680 | -             | 0.3976          |\\n| 1.3411 | 11690 | -             | 0.3976          |\\n| 1.3422 | 11700 | 0.6757        | 0.3976          |\\n| 1.3434 | 11710 | -             | 0.3975          |\\n| 1.3445 | 11720 | -             | 0.3973          |\\n| 1.3456 | 11730 | -             | 0.3971          |\\n| 1.3468 | 11740 | -             | 0.3963          |\\n| 1.3479 | 11750 | -             | 0.3964          |\\n| 1.3491 | 11760 | -             | 0.3965          |\\n| 1.3502 | 11770 | -             | 0.3967          |\\n| 1.3514 | 11780 | -             | 0.3966          |\\n| 1.3525 | 11790 | -             | 0.3964          |\\n| 1.3537 | 11800 | 0.7091        | 0.3965          |\\n| 1.3548 | 11810 | -             | 0.3964          |\\n| 1.3560 | 11820 | -             | 0.3964          |\\n| 1.3571 | 11830 | -             | 0.3963          |\\n| 1.3583 | 11840 | -             | 0.3962          |\\n| 1.3594 | 11850 | -             | 0.3961          |\\n| 1.3606 | 11860 | -             | 0.3956          |\\n| 1.3617 | 11870 | -             | 0.3956          |\\n| 1.3629 | 11880 | -             | 0.3961          |\\n| 1.3640 | 11890 | -             | 0.3963          |\\n| 1.3651 | 11900 | 0.6977        | 0.3962          |\\n| 1.3663 | 11910 | -             | 0.3958          |\\n| 1.3674 | 11920 | -             | 0.3960          |\\n| 1.3686 | 11930 | -             | 0.3963          |\\n| 1.3697 | 11940 | -             | 0.3964          |\\n| 1.3709 | 11950 | -             | 0.3961          |\\n| 1.3720 | 11960 | -             | 0.3960          |\\n| 1.3732 | 11970 | -             | 0.3958          |\\n| 1.3743 | 11980 | -             | 0.3954          |\\n| 1.3755 | 11990 | -             | 0.3948          |\\n| 1.3766 | 12000 | 0.7003        | 0.3944          |\\n\\n</details>\\n\\n### Framework Versions\\n- Python: 3.12.8\\n- Sentence Transformers: 3.4.1\\n- Transformers: 4.49.0\\n- PyTorch: 2.2.0+cu121\\n- Accelerate: 1.4.0\\n- Datasets: 3.3.2\\n- Tokenizers: 0.21.0\\n\\n## Citation\\n\\n### BibTeX\\n\\n#### Sentence Transformers\\n```bibtex\\n@inproceedings{reimers-2019-sentence-bert,\\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\\n    author = \"Reimers, Nils and Gurevych, Iryna\",\\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\\n    month = \"11\",\\n    year = \"2019\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://arxiv.org/abs/1908.10084\",\\n}\\n```\\n\\n#### MultipleNegativesRankingLoss\\n```bibtex\\n@misc{henderson2017efficient,\\n    title={Efficient Natural Language Response Suggestion for Smart Reply},\\n    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\\n    year={2017},\\n    eprint={1705.00652},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.CL}\\n}\\n```\\n\\n<!--\\n## Glossary\\n\\n*Clearly define terms in order to be accessible across audiences.*\\n-->\\n\\n<!--\\n## Model Card Authors\\n\\n*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*\\n-->\\n\\n<!--\\n## Model Card Contact\\n\\n*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*\\n-->',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c27dfcad08a0d12ccab9',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:14.507000',\n",
       "  'date_modified': '2025-02-27T19:18:14.507000'},\n",
       " {'model_identifier': 'blowing-up-groundhogs/vatrpp',\n",
       "  'version': 7,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-26T15:12:39',\n",
       "  'last_modified': '2025-02-27T17:09:28',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors',\n",
       "   'vatrpp',\n",
       "   'image-generation',\n",
       "   'text-to-image',\n",
       "   'conditional-generation',\n",
       "   'generative-modeling',\n",
       "   'image-synthesis',\n",
       "   'image-manipulation',\n",
       "   'design-prototyping',\n",
       "   'research',\n",
       "   'educational',\n",
       "   'en',\n",
       "   'license:mit',\n",
       "   'region:us'],\n",
       "  'task': ['text-to-image'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.5223385691642761},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'mit'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'description': \"---\\nlanguage:\\n  - en\\ntags:\\n    - image-generation\\n    - text-to-image\\n    - conditional-generation\\n    - generative-modeling\\n    - image-synthesis\\n    - image-manipulation\\n    - design-prototyping\\n    - research\\n    - educational\\nlicense: mit\\nmetrics:\\n  - FID\\n  - KID\\n  - HWD\\n  - CER\\n---\\n\\n# Handwritten Text Generation from Visual Archetypes ++\\n\\nThis repository includes the code for training the VATr++ Styled Handwritten Text Generation model.\\n\\n## Installation\\n\\n```bash\\nconda create --name vatr python=3.9\\nconda activate vatr\\nconda install pytorch==1.13.1 torchvision==0.14.1 torchaudio==0.13.1 pytorch-cuda=11.7 -c pytorch -c nvidia\\ngit clone https://github.com/aimagelab/VATr.git && cd VATr\\npip install -r requirements.txt\\n```\\n\\n[This folder](https://drive.google.com/drive/folders/13rJhjl7VsyiXlPTBvnp1EKkKEhckLalr?usp=sharing) contains the regular IAM dataset `IAM-32.pickle` and the modified version with attached punctuation marks `IAM-32-pa.pickle`.\\nThe folder also contains the synthetically pretrained weights for the encoder `resnet_18_pretrained.pth`.\\nPlease download these files and place them into the `files` folder.\\n\\n## Training\\n\\nTo train the regular VATr model, use the following command. This uses the default settings from the paper.\\n\\n```bash\\npython train.py\\n```\\n\\nUseful arguments:\\n```bash\\npython train.py\\n        --feat_model_path PATH  # path to the pretrained resnet 18 checkpoint. By default this is the synthetically pretrained model\\n        --is_cycle              # use style cycle loss for training\\n        --dataset DATASET       # dataset to use. Default IAM\\n        --resume                # resume training from the last checkpoint with the same name\\n        --wandb                 # use wandb for logging\\n```\\n\\nUse the following arguments to apply full VATr++ training\\n```bash\\npython train.py\\n        --d-crop-size 64 128          # Randomly crop input to discriminator to width 64 to 128\\n        --text-augment-strength 0.4   # Text augmentation for adding more rare characters\\n        --file-suffix pa              # Use the punctuation attached version of IAM\\n        --augment-ocr                 # Augment the real images used to train the OCR model\\n```\\n\\n### Pretraining dataset\\nThe model `resnet_18_pretrained.pth` was pretrained by using this dataset: [Font Square](https://github.com/aimagelab/font_square)\\n\\n\\n## Generate Styled Handwritten Text Images\\n\\nWe added some utility to generate handwritten text images using the trained model. These are used as follows:\\n\\n```bash\\npython generate.py [ACTION] --checkpoint files/vatrpp.pth\\n```\\n\\nThe following actions are available with their respective arguments.\\n\\n### Custom Author\\n\\nGenerate the given text for a custom author.\\n\\n```bash\\ntext  --text STRING     # String to generate\\n      --text-path PATH  # Optional path to text file\\n      --output PATH     # Optional output location, default: files/output.png\\n      --style-folder PATH    # Optional style folder containing writer samples, default: 'files/style_samples/00'\\n```\\nStyle samples for the author are needed. These can be automatically generated from an image of a page using `create_style_sample.py`.\\n```bash\\npython create_style_sample.py  --input-image PATH     # Path of the image to extract the style samples from.\\n                               --output-folder PATH   # Folder where the style samples should be saved\\n```\\n\\n### All Authors\\n\\nGenerate some text for all authors of IAM. The output is saved to `saved_images/author_samples/`\\n\\n```bash\\nauthors --test-set        # Generate authors of test set, otherwise training set is generated\\n        --checkpoint PATH # Checkpoint used to generate text, files/vatr.pth by default\\n        --align           # Detect the bottom lines for each word and align them\\n        --at-once         # Generate the whole sentence at once instead of word-by-word\\n        --output-style    # Also save the style images used to generate the words\\n```\\n\\n### Evaluation Images\\n\\n```bash\\nfid --target_dataset_path PATH  # dataset file for which the test set will be generated\\n    --dataset-path PATH         # dataset file from which style samples will be taken, for example the attached punctuation\\n    --output PATH               # where to save the images, default is saved_images/fid\\n    --checkpoint PATH           # Checkpoint used to generate text, files/vatr.pth by default\\n    --all-epochs                # Generate evaluation images for all saved epochs available (checkpoint has to be a folder)\\n    --fake-only                 # Only output fake images, no ground truth\\n    --test-only                 # Only generate test set, not train set\\n    --long-tail                 # Only generate words containing long tail characters\\n```\",\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c27cfcad08a0d12ccab8',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:14.507000',\n",
       "  'date_modified': '2025-02-27T19:18:14.507000'},\n",
       " {'model_identifier': 'liamj16/fine_tuned_Qwen2.5-Code-3B-hq-and-synthetic',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:07:03',\n",
       "  'last_modified': '2025-02-27T17:09:34',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'arxiv:1910.09700',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 5.748008728027344},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nlibrary_name: transformers\\ntags: []\\n---\\n\\n# Model Card for Model ID\\n\\n<!-- Provide a quick summary of what the model is/does. -->\\n\\n\\n\\n## Model Details\\n\\n### Model Description\\n\\n<!-- Provide a longer summary of what this model is. -->\\n\\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\\n\\n- **Developed by:** [More Information Needed]\\n- **Funded by [optional]:** [More Information Needed]\\n- **Shared by [optional]:** [More Information Needed]\\n- **Model type:** [More Information Needed]\\n- **Language(s) (NLP):** [More Information Needed]\\n- **License:** [More Information Needed]\\n- **Finetuned from model [optional]:** [More Information Needed]\\n\\n### Model Sources [optional]\\n\\n<!-- Provide the basic links for the model. -->\\n\\n- **Repository:** [More Information Needed]\\n- **Paper [optional]:** [More Information Needed]\\n- **Demo [optional]:** [More Information Needed]\\n\\n## Uses\\n\\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\\n\\n### Direct Use\\n\\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\\n\\n[More Information Needed]\\n\\n### Downstream Use [optional]\\n\\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\\n\\n[More Information Needed]\\n\\n### Out-of-Scope Use\\n\\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\\n\\n[More Information Needed]\\n\\n## Bias, Risks, and Limitations\\n\\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\\n\\n[More Information Needed]\\n\\n### Recommendations\\n\\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\\n\\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n## How to Get Started with the Model\\n\\nUse the code below to get started with the model.\\n\\n[More Information Needed]\\n\\n## Training Details\\n\\n### Training Data\\n\\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\\n\\n[More Information Needed]\\n\\n### Training Procedure\\n\\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\\n\\n#### Preprocessing [optional]\\n\\n[More Information Needed]\\n\\n\\n#### Training Hyperparameters\\n\\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\\n\\n#### Speeds, Sizes, Times [optional]\\n\\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\\n\\n[More Information Needed]\\n\\n## Evaluation\\n\\n<!-- This section describes the evaluation protocols and provides the results. -->\\n\\n### Testing Data, Factors & Metrics\\n\\n#### Testing Data\\n\\n<!-- This should link to a Dataset Card if possible. -->\\n\\n[More Information Needed]\\n\\n#### Factors\\n\\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\\n\\n[More Information Needed]\\n\\n#### Metrics\\n\\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\\n\\n[More Information Needed]\\n\\n### Results\\n\\n[More Information Needed]\\n\\n#### Summary\\n\\n\\n\\n## Model Examination [optional]\\n\\n<!-- Relevant interpretability work for the model goes here -->\\n\\n[More Information Needed]\\n\\n## Environmental Impact\\n\\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\\n\\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n\\n- **Hardware Type:** [More Information Needed]\\n- **Hours used:** [More Information Needed]\\n- **Cloud Provider:** [More Information Needed]\\n- **Compute Region:** [More Information Needed]\\n- **Carbon Emitted:** [More Information Needed]\\n\\n## Technical Specifications [optional]\\n\\n### Model Architecture and Objective\\n\\n[More Information Needed]\\n\\n### Compute Infrastructure\\n\\n[More Information Needed]\\n\\n#### Hardware\\n\\n[More Information Needed]\\n\\n#### Software\\n\\n[More Information Needed]\\n\\n## Citation [optional]\\n\\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\\n\\n**BibTeX:**\\n\\n[More Information Needed]\\n\\n**APA:**\\n\\n[More Information Needed]\\n\\n## Glossary [optional]\\n\\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\\n\\n[More Information Needed]\\n\\n## More Information [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Authors [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Contact\\n\\n[More Information Needed]',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c27bfcad08a0d12ccab7',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:14.507000',\n",
       "  'date_modified': '2025-02-27T19:18:14.507000'},\n",
       " {'model_identifier': 'Yangyang127/imagefusion',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:09:53',\n",
       "  'last_modified': '2025-02-27T17:09:53',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c27afcad08a0d12ccab6',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:14.507000',\n",
       "  'date_modified': '2025-02-27T19:18:14.507000'},\n",
       " {'model_identifier': 'psalas/psalas_model',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:09:54',\n",
       "  'last_modified': '2025-02-27T17:09:54',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['license:wtfpl', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'wtfpl'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: wtfpl\\n---\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c279fcad08a0d12ccab5',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:14.131000',\n",
       "  'date_modified': '2025-02-27T19:18:14.131000'},\n",
       " {'model_identifier': 'mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF',\n",
       "  'version': 27,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T12:03:39',\n",
       "  'last_modified': '2025-02-27T17:10:02',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': True,\n",
       "  'tags': ['transformers',\n",
       "   'gguf',\n",
       "   'mergekit',\n",
       "   'merge',\n",
       "   'en',\n",
       "   'base_model:jojo00/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P',\n",
       "   'base_model:quantized:jojo00/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P',\n",
       "   'endpoints_compatible',\n",
       "   'region:us',\n",
       "   'imatrix',\n",
       "   'conversational'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 7.478763580322266},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': \"---\\nbase_model: jojo00/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P\\nlanguage:\\n- en\\nlibrary_name: transformers\\nquantized_by: mradermacher\\ntags:\\n- mergekit\\n- merge\\n---\\n## About\\n\\n<!-- ### quantize_version: 2 -->\\n<!-- ### output_tensor_quantised: 1 -->\\n<!-- ### convert_type: hf -->\\n<!-- ### vocab_type:  -->\\n<!-- ### tags: nicoboss -->\\nweighted/imatrix quants of https://huggingface.co/jojo00/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P\\n\\n<!-- provided-files -->\\nstatic quants are available at https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-GGUF\\n## Usage\\n\\nIf you are unsure how to use GGUF files, refer to one of [TheBloke's\\nREADMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for\\nmore details, including on how to concatenate multi-part files.\\n\\n## Provided Quants\\n\\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\\n\\n| Link | Type | Size/GB | Notes |\\n|:-----|:-----|--------:|:------|\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ1_S.gguf) | i1-IQ1_S | 2.1 | for the desperate |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ1_M.gguf) | i1-IQ1_M | 2.3 | mostly desperate |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ2_XXS.gguf) | i1-IQ2_XXS | 2.5 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ2_XS.gguf) | i1-IQ2_XS | 2.7 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ2_S.gguf) | i1-IQ2_S | 2.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ2_M.gguf) | i1-IQ2_M | 3.0 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q2_K_S.gguf) | i1-Q2_K_S | 3.1 | very low quality |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q2_K.gguf) | i1-Q2_K | 3.3 | IQ3_XXS probably better |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ3_XXS.gguf) | i1-IQ3_XXS | 3.4 | lower quality |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ3_XS.gguf) | i1-IQ3_XS | 3.6 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q3_K_S.gguf) | i1-Q3_K_S | 3.8 | IQ3_XS probably better |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ3_S.gguf) | i1-IQ3_S | 3.8 | beats Q3_K* |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ3_M.gguf) | i1-IQ3_M | 3.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q3_K_M.gguf) | i1-Q3_K_M | 4.1 | IQ3_S probably better |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q3_K_L.gguf) | i1-Q3_K_L | 4.4 | IQ3_M probably better |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ4_XS.gguf) | i1-IQ4_XS | 4.5 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q4_0.gguf) | i1-Q4_0 | 4.8 | fast, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-IQ4_NL.gguf) | i1-IQ4_NL | 4.8 | prefer IQ4_XS |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q4_K_S.gguf) | i1-Q4_K_S | 4.8 | optimal size/speed/quality |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q4_K_M.gguf) | i1-Q4_K_M | 5.0 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q4_1.gguf) | i1-Q4_1 | 5.2 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q5_K_S.gguf) | i1-Q5_K_S | 5.7 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q5_K_M.gguf) | i1-Q5_K_M | 5.8 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P-i1-GGUF/resolve/main/Lumimaid-v0.2-8B-SLERP-DeepHermes-3-Llama-3-8B-P.i1-Q6_K.gguf) | i1-Q6_K | 6.7 | practically like static Q6_K |\\n\\nHere is a handy graph by ikawrakow comparing some lower-quality quant\\ntypes (lower is better):\\n\\n![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)\\n\\nAnd here are Artefact2's thoughts on the matter:\\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\\n\\n## FAQ / Model Request\\n\\nSee https://huggingface.co/mradermacher/model_requests for some answers to\\nquestions you might have and/or if you want some other model quantized.\\n\\n## Thanks\\n\\nI thank my company, [nethype GmbH](https://www.nethype.de/), for letting\\nme use its servers and providing upgrades to my workstation to enable\\nthis work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.\\n\\n<!-- end -->\\n\",\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 1.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c276fcad08a0d12ccab2',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.999000',\n",
       "  'date_modified': '2025-02-27T19:18:13.999000'},\n",
       " {'model_identifier': 'sobamchan/st5-base-mean-16000',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:09:46',\n",
       "  'last_modified': '2025-02-27T17:10:34',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['sentence-transformers',\n",
       "   'safetensors',\n",
       "   't5',\n",
       "   'sentence-similarity',\n",
       "   'feature-extraction',\n",
       "   'generated_from_trainer',\n",
       "   'dataset_size:557850',\n",
       "   'loss:MultipleNegativesRankingLoss',\n",
       "   'en',\n",
       "   'dataset:sentence-transformers/all-nli',\n",
       "   'arxiv:1908.10084',\n",
       "   'arxiv:1705.00652',\n",
       "   'base_model:google-t5/t5-base',\n",
       "   'base_model:finetune:google-t5/t5-base',\n",
       "   'autotrain_compatible',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['sentence-similarity', 'feature-extraction'],\n",
       "  'architecture': 'T5EncoderModel',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.4083981513977051},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'sentence-similarity',\n",
       "  'description': '---\\nlanguage:\\n- en\\ntags:\\n- sentence-transformers\\n- sentence-similarity\\n- feature-extraction\\n- generated_from_trainer\\n- dataset_size:557850\\n- loss:MultipleNegativesRankingLoss\\nbase_model: google-t5/t5-base\\nwidget:\\n- source_sentence: A man is jumping unto his filthy bed.\\n  sentences:\\n  - A young male is looking at a newspaper while 2 females walks past him.\\n  - The bed is dirty.\\n  - The man is on the moon.\\n- source_sentence: A carefully balanced male stands on one foot near a clean ocean\\n    beach area.\\n  sentences:\\n  - A man is ouside near the beach.\\n  - Three policemen patrol the streets on bikes\\n  - A man is sitting on his couch.\\n- source_sentence: The man is wearing a blue shirt.\\n  sentences:\\n  - Near the trashcan the man stood and smoked\\n  - A man in a blue shirt leans on a wall beside a road with a blue van and red car\\n    with water in the background.\\n  - A man in a black shirt is playing a guitar.\\n- source_sentence: The girls are outdoors.\\n  sentences:\\n  - Two girls riding on an amusement part ride.\\n  - a guy laughs while doing laundry\\n  - Three girls are standing together in a room, one is listening, one is writing\\n    on a wall and the third is talking to them.\\n- source_sentence: A construction worker peeking out of a manhole while his coworker\\n    sits on the sidewalk smiling.\\n  sentences:\\n  - A worker is looking out of a manhole.\\n  - A man is giving a presentation.\\n  - The workers are both inside the manhole.\\ndatasets:\\n- sentence-transformers/all-nli\\npipeline_tag: sentence-similarity\\nlibrary_name: sentence-transformers\\n---\\n\\n# SentenceTransformer based on google-t5/t5-base\\n\\nThis is a [sentence-transformers](https://www.SBERT.net) model finetuned from [google-t5/t5-base](https://huggingface.co/google-t5/t5-base) on the [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli) dataset. It maps sentences & paragraphs to a 768-dimensional dense vector space and can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.\\n\\n## Model Details\\n\\n### Model Description\\n- **Model Type:** Sentence Transformer\\n- **Base model:** [google-t5/t5-base](https://huggingface.co/google-t5/t5-base) <!-- at revision a9723ea7f1b39c1eae772870f3b547bf6ef7e6c1 -->\\n- **Maximum Sequence Length:** 256 tokens\\n- **Output Dimensionality:** 768 dimensions\\n- **Similarity Function:** Cosine Similarity\\n- **Training Dataset:**\\n    - [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli)\\n- **Language:** en\\n<!-- - **License:** Unknown -->\\n\\n### Model Sources\\n\\n- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)\\n- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)\\n- **Hugging Face:** [Sentence Transformers on Hugging Face](https://huggingface.co/models?library=sentence-transformers)\\n\\n### Full Model Architecture\\n\\n```\\nSentenceTransformer(\\n  (0): Transformer({\\'max_seq_length\\': 256, \\'do_lower_case\\': False}) with Transformer model: T5EncoderModel \\n  (1): Pooling({\\'word_embedding_dimension\\': 768, \\'pooling_mode_cls_token\\': False, \\'pooling_mode_mean_tokens\\': True, \\'pooling_mode_max_tokens\\': False, \\'pooling_mode_mean_sqrt_len_tokens\\': False, \\'pooling_mode_weightedmean_tokens\\': False, \\'pooling_mode_lasttoken\\': False, \\'include_prompt\\': True})\\n  (2): Normalize()\\n)\\n```\\n\\n## Usage\\n\\n### Direct Usage (Sentence Transformers)\\n\\nFirst install the Sentence Transformers library:\\n\\n```bash\\npip install -U sentence-transformers\\n```\\n\\nThen you can load this model and run inference.\\n```python\\nfrom sentence_transformers import SentenceTransformer\\n\\n# Download from the 🤗 Hub\\nmodel = SentenceTransformer(\"sentence_transformers_model_id\")\\n# Run inference\\nsentences = [\\n    \\'A construction worker peeking out of a manhole while his coworker sits on the sidewalk smiling.\\',\\n    \\'A worker is looking out of a manhole.\\',\\n    \\'The workers are both inside the manhole.\\',\\n]\\nembeddings = model.encode(sentences)\\nprint(embeddings.shape)\\n# [3, 768]\\n\\n# Get the similarity scores for the embeddings\\nsimilarities = model.similarity(embeddings, embeddings)\\nprint(similarities.shape)\\n# [3, 3]\\n```\\n\\n<!--\\n### Direct Usage (Transformers)\\n\\n<details><summary>Click to see the direct usage in Transformers</summary>\\n\\n</details>\\n-->\\n\\n<!--\\n### Downstream Usage (Sentence Transformers)\\n\\nYou can finetune this model on your own dataset.\\n\\n<details><summary>Click to expand</summary>\\n\\n</details>\\n-->\\n\\n<!--\\n### Out-of-Scope Use\\n\\n*List how the model may foreseeably be misused and address what users ought not to do with the model.*\\n-->\\n\\n<!--\\n## Bias, Risks and Limitations\\n\\n*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*\\n-->\\n\\n<!--\\n### Recommendations\\n\\n*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*\\n-->\\n\\n## Training Details\\n\\n### Training Dataset\\n\\n#### all-nli\\n\\n* Dataset: [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli) at [d482672](https://huggingface.co/datasets/sentence-transformers/all-nli/tree/d482672c8e74ce18da116f430137434ba2e52fab)\\n* Size: 557,850 training samples\\n* Columns: <code>anchor</code>, <code>positive</code>, and <code>negative</code>\\n* Approximate statistics based on the first 1000 samples:\\n  |         | anchor                                                                           | positive                                                                          | negative                                                                          |\\n  |:--------|:---------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|\\n  | type    | string                                                                           | string                                                                            | string                                                                            |\\n  | details | <ul><li>min: 6 tokens</li><li>mean: 9.96 tokens</li><li>max: 52 tokens</li></ul> | <ul><li>min: 5 tokens</li><li>mean: 12.79 tokens</li><li>max: 44 tokens</li></ul> | <ul><li>min: 4 tokens</li><li>mean: 14.02 tokens</li><li>max: 57 tokens</li></ul> |\\n* Samples:\\n  | anchor                                                                     | positive                                         | negative                                                   |\\n  |:---------------------------------------------------------------------------|:-------------------------------------------------|:-----------------------------------------------------------|\\n  | <code>A person on a horse jumps over a broken down airplane.</code>        | <code>A person is outdoors, on a horse.</code>   | <code>A person is at a diner, ordering an omelette.</code> |\\n  | <code>Children smiling and waving at camera</code>                         | <code>There are children present</code>          | <code>The kids are frowning</code>                         |\\n  | <code>A boy is jumping on skateboard in the middle of a red bridge.</code> | <code>The boy does a skateboarding trick.</code> | <code>The boy skates down the sidewalk.</code>             |\\n* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:\\n  ```json\\n  {\\n      \"scale\": 20.0,\\n      \"similarity_fct\": \"cos_sim\"\\n  }\\n  ```\\n\\n### Evaluation Dataset\\n\\n#### all-nli\\n\\n* Dataset: [all-nli](https://huggingface.co/datasets/sentence-transformers/all-nli) at [d482672](https://huggingface.co/datasets/sentence-transformers/all-nli/tree/d482672c8e74ce18da116f430137434ba2e52fab)\\n* Size: 6,584 evaluation samples\\n* Columns: <code>anchor</code>, <code>positive</code>, and <code>negative</code>\\n* Approximate statistics based on the first 1000 samples:\\n  |         | anchor                                                                            | positive                                                                         | negative                                                                          |\\n  |:--------|:----------------------------------------------------------------------------------|:---------------------------------------------------------------------------------|:----------------------------------------------------------------------------------|\\n  | type    | string                                                                            | string                                                                           | string                                                                            |\\n  | details | <ul><li>min: 5 tokens</li><li>mean: 19.41 tokens</li><li>max: 79 tokens</li></ul> | <ul><li>min: 4 tokens</li><li>mean: 9.69 tokens</li><li>max: 35 tokens</li></ul> | <ul><li>min: 4 tokens</li><li>mean: 10.35 tokens</li><li>max: 30 tokens</li></ul> |\\n* Samples:\\n  | anchor                                                                                                                                                                         | positive                                                    | negative                                                |\\n  |:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------|:--------------------------------------------------------|\\n  | <code>Two women are embracing while holding to go packages.</code>                                                                                                             | <code>Two woman are holding packages.</code>                | <code>The men are fighting outside a deli.</code>       |\\n  | <code>Two young children in blue jerseys, one with the number 9 and one with the number 2 are standing on wooden steps in a bathroom and washing their hands in a sink.</code> | <code>Two kids in numbered jerseys wash their hands.</code> | <code>Two kids in jackets walk to school.</code>        |\\n  | <code>A man selling donuts to a customer during a world exhibition event held in the city of Angeles</code>                                                                    | <code>A man selling donuts to a customer.</code>            | <code>A woman drinks her coffee in a small cafe.</code> |\\n* Loss: [<code>MultipleNegativesRankingLoss</code>](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss) with these parameters:\\n  ```json\\n  {\\n      \"scale\": 20.0,\\n      \"similarity_fct\": \"cos_sim\"\\n  }\\n  ```\\n\\n### Training Hyperparameters\\n#### Non-Default Hyperparameters\\n\\n- `eval_strategy`: steps\\n- `per_device_train_batch_size`: 64\\n- `per_device_eval_batch_size`: 64\\n- `learning_rate`: 1e-05\\n- `warmup_ratio`: 0.1\\n- `batch_sampler`: no_duplicates\\n\\n#### All Hyperparameters\\n<details><summary>Click to expand</summary>\\n\\n- `overwrite_output_dir`: False\\n- `do_predict`: False\\n- `eval_strategy`: steps\\n- `prediction_loss_only`: True\\n- `per_device_train_batch_size`: 64\\n- `per_device_eval_batch_size`: 64\\n- `per_gpu_train_batch_size`: None\\n- `per_gpu_eval_batch_size`: None\\n- `gradient_accumulation_steps`: 1\\n- `eval_accumulation_steps`: None\\n- `torch_empty_cache_steps`: None\\n- `learning_rate`: 1e-05\\n- `weight_decay`: 0.0\\n- `adam_beta1`: 0.9\\n- `adam_beta2`: 0.999\\n- `adam_epsilon`: 1e-08\\n- `max_grad_norm`: 1.0\\n- `num_train_epochs`: 3\\n- `max_steps`: -1\\n- `lr_scheduler_type`: linear\\n- `lr_scheduler_kwargs`: {}\\n- `warmup_ratio`: 0.1\\n- `warmup_steps`: 0\\n- `log_level`: passive\\n- `log_level_replica`: warning\\n- `log_on_each_node`: True\\n- `logging_nan_inf_filter`: True\\n- `save_safetensors`: True\\n- `save_on_each_node`: False\\n- `save_only_model`: False\\n- `restore_callback_states_from_checkpoint`: False\\n- `no_cuda`: False\\n- `use_cpu`: False\\n- `use_mps_device`: False\\n- `seed`: 42\\n- `data_seed`: None\\n- `jit_mode_eval`: False\\n- `use_ipex`: False\\n- `bf16`: False\\n- `fp16`: False\\n- `fp16_opt_level`: O1\\n- `half_precision_backend`: auto\\n- `bf16_full_eval`: False\\n- `fp16_full_eval`: False\\n- `tf32`: None\\n- `local_rank`: 0\\n- `ddp_backend`: None\\n- `tpu_num_cores`: None\\n- `tpu_metrics_debug`: False\\n- `debug`: []\\n- `dataloader_drop_last`: False\\n- `dataloader_num_workers`: 0\\n- `dataloader_prefetch_factor`: None\\n- `past_index`: -1\\n- `disable_tqdm`: False\\n- `remove_unused_columns`: True\\n- `label_names`: None\\n- `load_best_model_at_end`: False\\n- `ignore_data_skip`: False\\n- `fsdp`: []\\n- `fsdp_min_num_params`: 0\\n- `fsdp_config`: {\\'min_num_params\\': 0, \\'xla\\': False, \\'xla_fsdp_v2\\': False, \\'xla_fsdp_grad_ckpt\\': False}\\n- `fsdp_transformer_layer_cls_to_wrap`: None\\n- `accelerator_config`: {\\'split_batches\\': False, \\'dispatch_batches\\': None, \\'even_batches\\': True, \\'use_seedable_sampler\\': True, \\'non_blocking\\': False, \\'gradient_accumulation_kwargs\\': None}\\n- `deepspeed`: None\\n- `label_smoothing_factor`: 0.0\\n- `optim`: adamw_torch\\n- `optim_args`: None\\n- `adafactor`: False\\n- `group_by_length`: False\\n- `length_column_name`: length\\n- `ddp_find_unused_parameters`: None\\n- `ddp_bucket_cap_mb`: None\\n- `ddp_broadcast_buffers`: False\\n- `dataloader_pin_memory`: True\\n- `dataloader_persistent_workers`: False\\n- `skip_memory_metrics`: True\\n- `use_legacy_prediction_loop`: False\\n- `push_to_hub`: False\\n- `resume_from_checkpoint`: None\\n- `hub_model_id`: None\\n- `hub_strategy`: every_save\\n- `hub_private_repo`: None\\n- `hub_always_push`: False\\n- `gradient_checkpointing`: False\\n- `gradient_checkpointing_kwargs`: None\\n- `include_inputs_for_metrics`: False\\n- `include_for_metrics`: []\\n- `eval_do_concat_batches`: True\\n- `fp16_backend`: auto\\n- `push_to_hub_model_id`: None\\n- `push_to_hub_organization`: None\\n- `mp_parameters`: \\n- `auto_find_batch_size`: False\\n- `full_determinism`: False\\n- `torchdynamo`: None\\n- `ray_scope`: last\\n- `ddp_timeout`: 1800\\n- `torch_compile`: False\\n- `torch_compile_backend`: None\\n- `torch_compile_mode`: None\\n- `dispatch_batches`: None\\n- `split_batches`: None\\n- `include_tokens_per_second`: False\\n- `include_num_input_tokens_seen`: False\\n- `neftune_noise_alpha`: None\\n- `optim_target_modules`: None\\n- `batch_eval_metrics`: False\\n- `eval_on_start`: False\\n- `use_liger_kernel`: False\\n- `eval_use_gather_object`: False\\n- `average_tokens_across_devices`: False\\n- `prompts`: None\\n- `batch_sampler`: no_duplicates\\n- `multi_dataset_batch_sampler`: proportional\\n\\n</details>\\n\\n### Training Logs\\n<details><summary>Click to expand</summary>\\n\\n| Epoch  | Step  | Training Loss | Validation Loss |\\n|:------:|:-----:|:-------------:|:---------------:|\\n| 0.0011 | 10    | -             | 1.8733          |\\n| 0.0023 | 20    | -             | 1.8726          |\\n| 0.0034 | 30    | -             | 1.8714          |\\n| 0.0046 | 40    | -             | 1.8697          |\\n| 0.0057 | 50    | -             | 1.8675          |\\n| 0.0069 | 60    | -             | 1.8649          |\\n| 0.0080 | 70    | -             | 1.8619          |\\n| 0.0092 | 80    | -             | 1.8584          |\\n| 0.0103 | 90    | -             | 1.8544          |\\n| 0.0115 | 100   | 3.1046        | 1.8499          |\\n| 0.0126 | 110   | -             | 1.8451          |\\n| 0.0138 | 120   | -             | 1.8399          |\\n| 0.0149 | 130   | -             | 1.8343          |\\n| 0.0161 | 140   | -             | 1.8283          |\\n| 0.0172 | 150   | -             | 1.8223          |\\n| 0.0184 | 160   | -             | 1.8159          |\\n| 0.0195 | 170   | -             | 1.8091          |\\n| 0.0206 | 180   | -             | 1.8016          |\\n| 0.0218 | 190   | -             | 1.7938          |\\n| 0.0229 | 200   | 3.0303        | 1.7858          |\\n| 0.0241 | 210   | -             | 1.7775          |\\n| 0.0252 | 220   | -             | 1.7693          |\\n| 0.0264 | 230   | -             | 1.7605          |\\n| 0.0275 | 240   | -             | 1.7514          |\\n| 0.0287 | 250   | -             | 1.7417          |\\n| 0.0298 | 260   | -             | 1.7320          |\\n| 0.0310 | 270   | -             | 1.7227          |\\n| 0.0321 | 280   | -             | 1.7134          |\\n| 0.0333 | 290   | -             | 1.7040          |\\n| 0.0344 | 300   | 2.9459        | 1.6941          |\\n| 0.0356 | 310   | -             | 1.6833          |\\n| 0.0367 | 320   | -             | 1.6725          |\\n| 0.0379 | 330   | -             | 1.6614          |\\n| 0.0390 | 340   | -             | 1.6510          |\\n| 0.0402 | 350   | -             | 1.6402          |\\n| 0.0413 | 360   | -             | 1.6296          |\\n| 0.0424 | 370   | -             | 1.6187          |\\n| 0.0436 | 380   | -             | 1.6073          |\\n| 0.0447 | 390   | -             | 1.5962          |\\n| 0.0459 | 400   | 2.7813        | 1.5848          |\\n| 0.0470 | 410   | -             | 1.5735          |\\n| 0.0482 | 420   | -             | 1.5620          |\\n| 0.0493 | 430   | -             | 1.5495          |\\n| 0.0505 | 440   | -             | 1.5375          |\\n| 0.0516 | 450   | -             | 1.5256          |\\n| 0.0528 | 460   | -             | 1.5133          |\\n| 0.0539 | 470   | -             | 1.5012          |\\n| 0.0551 | 480   | -             | 1.4892          |\\n| 0.0562 | 490   | -             | 1.4769          |\\n| 0.0574 | 500   | 2.6308        | 1.4640          |\\n| 0.0585 | 510   | -             | 1.4513          |\\n| 0.0597 | 520   | -             | 1.4391          |\\n| 0.0608 | 530   | -             | 1.4262          |\\n| 0.0619 | 540   | -             | 1.4130          |\\n| 0.0631 | 550   | -             | 1.3998          |\\n| 0.0642 | 560   | -             | 1.3874          |\\n| 0.0654 | 570   | -             | 1.3752          |\\n| 0.0665 | 580   | -             | 1.3620          |\\n| 0.0677 | 590   | -             | 1.3485          |\\n| 0.0688 | 600   | 2.4452        | 1.3350          |\\n| 0.0700 | 610   | -             | 1.3213          |\\n| 0.0711 | 620   | -             | 1.3088          |\\n| 0.0723 | 630   | -             | 1.2965          |\\n| 0.0734 | 640   | -             | 1.2839          |\\n| 0.0746 | 650   | -             | 1.2713          |\\n| 0.0757 | 660   | -             | 1.2592          |\\n| 0.0769 | 670   | -             | 1.2466          |\\n| 0.0780 | 680   | -             | 1.2332          |\\n| 0.0792 | 690   | -             | 1.2203          |\\n| 0.0803 | 700   | 2.2626        | 1.2077          |\\n| 0.0815 | 710   | -             | 1.1959          |\\n| 0.0826 | 720   | -             | 1.1841          |\\n| 0.0837 | 730   | -             | 1.1725          |\\n| 0.0849 | 740   | -             | 1.1619          |\\n| 0.0860 | 750   | -             | 1.1516          |\\n| 0.0872 | 760   | -             | 1.1416          |\\n| 0.0883 | 770   | -             | 1.1320          |\\n| 0.0895 | 780   | -             | 1.1227          |\\n| 0.0906 | 790   | -             | 1.1138          |\\n| 0.0918 | 800   | 2.0044        | 1.1053          |\\n| 0.0929 | 810   | -             | 1.0965          |\\n| 0.0941 | 820   | -             | 1.0879          |\\n| 0.0952 | 830   | -             | 1.0796          |\\n| 0.0964 | 840   | -             | 1.0718          |\\n| 0.0975 | 850   | -             | 1.0644          |\\n| 0.0987 | 860   | -             | 1.0564          |\\n| 0.0998 | 870   | -             | 1.0490          |\\n| 0.1010 | 880   | -             | 1.0417          |\\n| 0.1021 | 890   | -             | 1.0354          |\\n| 0.1032 | 900   | 1.8763        | 1.0296          |\\n| 0.1044 | 910   | -             | 1.0239          |\\n| 0.1055 | 920   | -             | 1.0180          |\\n| 0.1067 | 930   | -             | 1.0123          |\\n| 0.1078 | 940   | -             | 1.0065          |\\n| 0.1090 | 950   | -             | 1.0008          |\\n| 0.1101 | 960   | -             | 0.9950          |\\n| 0.1113 | 970   | -             | 0.9894          |\\n| 0.1124 | 980   | -             | 0.9840          |\\n| 0.1136 | 990   | -             | 0.9793          |\\n| 0.1147 | 1000  | 1.7287        | 0.9752          |\\n| 0.1159 | 1010  | -             | 0.9706          |\\n| 0.1170 | 1020  | -             | 0.9659          |\\n| 0.1182 | 1030  | -             | 0.9615          |\\n| 0.1193 | 1040  | -             | 0.9572          |\\n| 0.1205 | 1050  | -             | 0.9531          |\\n| 0.1216 | 1060  | -             | 0.9494          |\\n| 0.1227 | 1070  | -             | 0.9456          |\\n| 0.1239 | 1080  | -             | 0.9415          |\\n| 0.1250 | 1090  | -             | 0.9377          |\\n| 0.1262 | 1100  | 1.6312        | 0.9339          |\\n| 0.1273 | 1110  | -             | 0.9303          |\\n| 0.1285 | 1120  | -             | 0.9267          |\\n| 0.1296 | 1130  | -             | 0.9232          |\\n| 0.1308 | 1140  | -             | 0.9197          |\\n| 0.1319 | 1150  | -             | 0.9162          |\\n| 0.1331 | 1160  | -             | 0.9128          |\\n| 0.1342 | 1170  | -             | 0.9097          |\\n| 0.1354 | 1180  | -             | 0.9069          |\\n| 0.1365 | 1190  | -             | 0.9040          |\\n| 0.1377 | 1200  | 1.5316        | 0.9010          |\\n| 0.1388 | 1210  | -             | 0.8979          |\\n| 0.1400 | 1220  | -             | 0.8947          |\\n| 0.1411 | 1230  | -             | 0.8915          |\\n| 0.1423 | 1240  | -             | 0.8888          |\\n| 0.1434 | 1250  | -             | 0.8861          |\\n| 0.1445 | 1260  | -             | 0.8833          |\\n| 0.1457 | 1270  | -             | 0.8806          |\\n| 0.1468 | 1280  | -             | 0.8779          |\\n| 0.1480 | 1290  | -             | 0.8748          |\\n| 0.1491 | 1300  | 1.4961        | 0.8718          |\\n| 0.1503 | 1310  | -             | 0.8690          |\\n| 0.1514 | 1320  | -             | 0.8664          |\\n| 0.1526 | 1330  | -             | 0.8635          |\\n| 0.1537 | 1340  | -             | 0.8603          |\\n| 0.1549 | 1350  | -             | 0.8574          |\\n| 0.1560 | 1360  | -             | 0.8545          |\\n| 0.1572 | 1370  | -             | 0.8521          |\\n| 0.1583 | 1380  | -             | 0.8497          |\\n| 0.1595 | 1390  | -             | 0.8474          |\\n| 0.1606 | 1400  | 1.451         | 0.8453          |\\n| 0.1618 | 1410  | -             | 0.8429          |\\n| 0.1629 | 1420  | -             | 0.8404          |\\n| 0.1640 | 1430  | -             | 0.8380          |\\n| 0.1652 | 1440  | -             | 0.8357          |\\n| 0.1663 | 1450  | -             | 0.8336          |\\n| 0.1675 | 1460  | -             | 0.8312          |\\n| 0.1686 | 1470  | -             | 0.8289          |\\n| 0.1698 | 1480  | -             | 0.8262          |\\n| 0.1709 | 1490  | -             | 0.8236          |\\n| 0.1721 | 1500  | 1.4177        | 0.8213          |\\n| 0.1732 | 1510  | -             | 0.8189          |\\n| 0.1744 | 1520  | -             | 0.8168          |\\n| 0.1755 | 1530  | -             | 0.8147          |\\n| 0.1767 | 1540  | -             | 0.8127          |\\n| 0.1778 | 1550  | -             | 0.8107          |\\n| 0.1790 | 1560  | -             | 0.8082          |\\n| 0.1801 | 1570  | -             | 0.8059          |\\n| 0.1813 | 1580  | -             | 0.8036          |\\n| 0.1824 | 1590  | -             | 0.8015          |\\n| 0.1835 | 1600  | 1.3734        | 0.7993          |\\n| 0.1847 | 1610  | -             | 0.7970          |\\n| 0.1858 | 1620  | -             | 0.7948          |\\n| 0.1870 | 1630  | -             | 0.7922          |\\n| 0.1881 | 1640  | -             | 0.7900          |\\n| 0.1893 | 1650  | -             | 0.7877          |\\n| 0.1904 | 1660  | -             | 0.7852          |\\n| 0.1916 | 1670  | -             | 0.7829          |\\n| 0.1927 | 1680  | -             | 0.7804          |\\n| 0.1939 | 1690  | -             | 0.7779          |\\n| 0.1950 | 1700  | 1.3327        | 0.7757          |\\n| 0.1962 | 1710  | -             | 0.7738          |\\n| 0.1973 | 1720  | -             | 0.7719          |\\n| 0.1985 | 1730  | -             | 0.7700          |\\n| 0.1996 | 1740  | -             | 0.7679          |\\n| 0.2008 | 1750  | -             | 0.7658          |\\n| 0.2019 | 1760  | -             | 0.7641          |\\n| 0.2031 | 1770  | -             | 0.7621          |\\n| 0.2042 | 1780  | -             | 0.7601          |\\n| 0.2053 | 1790  | -             | 0.7580          |\\n| 0.2065 | 1800  | 1.2804        | 0.7558          |\\n| 0.2076 | 1810  | -             | 0.7536          |\\n| 0.2088 | 1820  | -             | 0.7514          |\\n| 0.2099 | 1830  | -             | 0.7493          |\\n| 0.2111 | 1840  | -             | 0.7473          |\\n| 0.2122 | 1850  | -             | 0.7451          |\\n| 0.2134 | 1860  | -             | 0.7429          |\\n| 0.2145 | 1870  | -             | 0.7408          |\\n| 0.2157 | 1880  | -             | 0.7389          |\\n| 0.2168 | 1890  | -             | 0.7368          |\\n| 0.2180 | 1900  | 1.2255        | 0.7349          |\\n| 0.2191 | 1910  | -             | 0.7328          |\\n| 0.2203 | 1920  | -             | 0.7310          |\\n| 0.2214 | 1930  | -             | 0.7293          |\\n| 0.2226 | 1940  | -             | 0.7277          |\\n| 0.2237 | 1950  | -             | 0.7259          |\\n| 0.2248 | 1960  | -             | 0.7240          |\\n| 0.2260 | 1970  | -             | 0.7221          |\\n| 0.2271 | 1980  | -             | 0.7203          |\\n| 0.2283 | 1990  | -             | 0.7184          |\\n| 0.2294 | 2000  | 1.2635        | 0.7165          |\\n| 0.2306 | 2010  | -             | 0.7150          |\\n| 0.2317 | 2020  | -             | 0.7135          |\\n| 0.2329 | 2030  | -             | 0.7117          |\\n| 0.2340 | 2040  | -             | 0.7099          |\\n| 0.2352 | 2050  | -             | 0.7084          |\\n| 0.2363 | 2060  | -             | 0.7068          |\\n| 0.2375 | 2070  | -             | 0.7054          |\\n| 0.2386 | 2080  | -             | 0.7037          |\\n| 0.2398 | 2090  | -             | 0.7023          |\\n| 0.2409 | 2100  | 1.1912        | 0.7009          |\\n| 0.2421 | 2110  | -             | 0.6991          |\\n| 0.2432 | 2120  | -             | 0.6974          |\\n| 0.2444 | 2130  | -             | 0.6962          |\\n| 0.2455 | 2140  | -             | 0.6950          |\\n| 0.2466 | 2150  | -             | 0.6938          |\\n| 0.2478 | 2160  | -             | 0.6922          |\\n| 0.2489 | 2170  | -             | 0.6909          |\\n| 0.2501 | 2180  | -             | 0.6897          |\\n| 0.2512 | 2190  | -             | 0.6884          |\\n| 0.2524 | 2200  | 1.2144        | 0.6868          |\\n| 0.2535 | 2210  | -             | 0.6856          |\\n| 0.2547 | 2220  | -             | 0.6843          |\\n| 0.2558 | 2230  | -             | 0.6829          |\\n| 0.2570 | 2240  | -             | 0.6817          |\\n| 0.2581 | 2250  | -             | 0.6804          |\\n| 0.2593 | 2260  | -             | 0.6789          |\\n| 0.2604 | 2270  | -             | 0.6775          |\\n| 0.2616 | 2280  | -             | 0.6763          |\\n| 0.2627 | 2290  | -             | 0.6751          |\\n| 0.2639 | 2300  | 1.1498        | 0.6739          |\\n| 0.2650 | 2310  | -             | 0.6725          |\\n| 0.2661 | 2320  | -             | 0.6711          |\\n| 0.2673 | 2330  | -             | 0.6698          |\\n| 0.2684 | 2340  | -             | 0.6684          |\\n| 0.2696 | 2350  | -             | 0.6666          |\\n| 0.2707 | 2360  | -             | 0.6653          |\\n| 0.2719 | 2370  | -             | 0.6638          |\\n| 0.2730 | 2380  | -             | 0.6621          |\\n| 0.2742 | 2390  | -             | 0.6609          |\\n| 0.2753 | 2400  | 1.1446        | 0.6596          |\\n| 0.2765 | 2410  | -             | 0.6582          |\\n| 0.2776 | 2420  | -             | 0.6568          |\\n| 0.2788 | 2430  | -             | 0.6553          |\\n| 0.2799 | 2440  | -             | 0.6541          |\\n| 0.2811 | 2450  | -             | 0.6527          |\\n| 0.2822 | 2460  | -             | 0.6513          |\\n| 0.2834 | 2470  | -             | 0.6496          |\\n| 0.2845 | 2480  | -             | 0.6483          |\\n| 0.2856 | 2490  | -             | 0.6475          |\\n| 0.2868 | 2500  | 1.1309        | 0.6465          |\\n| 0.2879 | 2510  | -             | 0.6455          |\\n| 0.2891 | 2520  | -             | 0.6447          |\\n| 0.2902 | 2530  | -             | 0.6437          |\\n| 0.2914 | 2540  | -             | 0.6428          |\\n| 0.2925 | 2550  | -             | 0.6415          |\\n| 0.2937 | 2560  | -             | 0.6403          |\\n| 0.2948 | 2570  | -             | 0.6392          |\\n| 0.2960 | 2580  | -             | 0.6381          |\\n| 0.2971 | 2590  | -             | 0.6371          |\\n| 0.2983 | 2600  | 1.1006        | 0.6358          |\\n| 0.2994 | 2610  | -             | 0.6348          |\\n| 0.3006 | 2620  | -             | 0.6340          |\\n| 0.3017 | 2630  | -             | 0.6330          |\\n| 0.3029 | 2640  | -             | 0.6319          |\\n| 0.3040 | 2650  | -             | 0.6308          |\\n| 0.3052 | 2660  | -             | 0.6300          |\\n| 0.3063 | 2670  | -             | 0.6291          |\\n| 0.3074 | 2680  | -             | 0.6280          |\\n| 0.3086 | 2690  | -             | 0.6268          |\\n| 0.3097 | 2700  | 1.0772        | 0.6254          |\\n| 0.3109 | 2710  | -             | 0.6243          |\\n| 0.3120 | 2720  | -             | 0.6232          |\\n| 0.3132 | 2730  | -             | 0.6224          |\\n| 0.3143 | 2740  | -             | 0.6215          |\\n| 0.3155 | 2750  | -             | 0.6205          |\\n| 0.3166 | 2760  | -             | 0.6194          |\\n| 0.3178 | 2770  | -             | 0.6183          |\\n| 0.3189 | 2780  | -             | 0.6171          |\\n| 0.3201 | 2790  | -             | 0.6160          |\\n| 0.3212 | 2800  | 1.0648        | 0.6153          |\\n| 0.3224 | 2810  | -             | 0.6141          |\\n| 0.3235 | 2820  | -             | 0.6129          |\\n| 0.3247 | 2830  | -             | 0.6119          |\\n| 0.3258 | 2840  | -             | 0.6109          |\\n| 0.3269 | 2850  | -             | 0.6099          |\\n| 0.3281 | 2860  | -             | 0.6088          |\\n| 0.3292 | 2870  | -             | 0.6079          |\\n| 0.3304 | 2880  | -             | 0.6073          |\\n| 0.3315 | 2890  | -             | 0.6063          |\\n| 0.3327 | 2900  | 1.0398        | 0.6054          |\\n| 0.3338 | 2910  | -             | 0.6044          |\\n| 0.3350 | 2920  | -             | 0.6033          |\\n| 0.3361 | 2930  | -             | 0.6022          |\\n| 0.3373 | 2940  | -             | 0.6012          |\\n| 0.3384 | 2950  | -             | 0.6003          |\\n| 0.3396 | 2960  | -             | 0.5993          |\\n| 0.3407 | 2970  | -             | 0.5986          |\\n| 0.3419 | 2980  | -             | 0.5978          |\\n| 0.3430 | 2990  | -             | 0.5967          |\\n| 0.3442 | 3000  | 1.0256        | 0.5959          |\\n| 0.3453 | 3010  | -             | 0.5947          |\\n| 0.3464 | 3020  | -             | 0.5937          |\\n| 0.3476 | 3030  | -             | 0.5929          |\\n| 0.3487 | 3040  | -             | 0.5920          |\\n| 0.3499 | 3050  | -             | 0.5908          |\\n| 0.3510 | 3060  | -             | 0.5897          |\\n| 0.3522 | 3070  | -             | 0.5888          |\\n| 0.3533 | 3080  | -             | 0.5882          |\\n| 0.3545 | 3090  | -             | 0.5874          |\\n| 0.3556 | 3100  | 1.0489        | 0.5868          |\\n| 0.3568 | 3110  | -             | 0.5860          |\\n| 0.3579 | 3120  | -             | 0.5854          |\\n| 0.3591 | 3130  | -             | 0.5839          |\\n| 0.3602 | 3140  | -             | 0.5830          |\\n| 0.3614 | 3150  | -             | 0.5822          |\\n| 0.3625 | 3160  | -             | 0.5814          |\\n| 0.3637 | 3170  | -             | 0.5808          |\\n| 0.3648 | 3180  | -             | 0.5802          |\\n| 0.3660 | 3190  | -             | 0.5794          |\\n| 0.3671 | 3200  | 1.038         | 0.5788          |\\n| 0.3682 | 3210  | -             | 0.5778          |\\n| 0.3694 | 3220  | -             | 0.5770          |\\n| 0.3705 | 3230  | -             | 0.5763          |\\n| 0.3717 | 3240  | -             | 0.5752          |\\n| 0.3728 | 3250  | -             | 0.5745          |\\n| 0.3740 | 3260  | -             | 0.5737          |\\n| 0.3751 | 3270  | -             | 0.5728          |\\n| 0.3763 | 3280  | -             | 0.5720          |\\n| 0.3774 | 3290  | -             | 0.5713          |\\n| 0.3786 | 3300  | 1.0058        | 0.5707          |\\n| 0.3797 | 3310  | -             | 0.5700          |\\n| 0.3809 | 3320  | -             | 0.5690          |\\n| 0.3820 | 3330  | -             | 0.5681          |\\n| 0.3832 | 3340  | -             | 0.5673          |\\n| 0.3843 | 3350  | -             | 0.5669          |\\n| 0.3855 | 3360  | -             | 0.5667          |\\n| 0.3866 | 3370  | -             | 0.5665          |\\n| 0.3877 | 3380  | -             | 0.5659          |\\n| 0.3889 | 3390  | -             | 0.5650          |\\n| 0.3900 | 3400  | 1.0413        | 0.5645          |\\n| 0.3912 | 3410  | -             | 0.5641          |\\n| 0.3923 | 3420  | -             | 0.5635          |\\n| 0.3935 | 3430  | -             | 0.5629          |\\n| 0.3946 | 3440  | -             | 0.5622          |\\n| 0.3958 | 3450  | -             | 0.5617          |\\n| 0.3969 | 3460  | -             | 0.5614          |\\n| 0.3981 | 3470  | -             | 0.5607          |\\n| 0.3992 | 3480  | -             | 0.5603          |\\n| 0.4004 | 3490  | -             | 0.5598          |\\n| 0.4015 | 3500  | 0.938         | 0.5596          |\\n| 0.4027 | 3510  | -             | 0.5589          |\\n| 0.4038 | 3520  | -             | 0.5581          |\\n| 0.4050 | 3530  | -             | 0.5571          |\\n| 0.4061 | 3540  | -             | 0.5563          |\\n| 0.4073 | 3550  | -             | 0.5557          |\\n| 0.4084 | 3560  | -             | 0.5551          |\\n| 0.4095 | 3570  | -             | 0.5546          |\\n| 0.4107 | 3580  | -             | 0.5541          |\\n| 0.4118 | 3590  | -             | 0.5535          |\\n| 0.4130 | 3600  | 0.955         | 0.5528          |\\n| 0.4141 | 3610  | -             | 0.5522          |\\n| 0.4153 | 3620  | -             | 0.5516          |\\n| 0.4164 | 3630  | -             | 0.5509          |\\n| 0.4176 | 3640  | -             | 0.5503          |\\n| 0.4187 | 3650  | -             | 0.5495          |\\n| 0.4199 | 3660  | -             | 0.5490          |\\n| 0.4210 | 3670  | -             | 0.5481          |\\n| 0.4222 | 3680  | -             | 0.5475          |\\n| 0.4233 | 3690  | -             | 0.5467          |\\n| 0.4245 | 3700  | 0.9387        | 0.5463          |\\n| 0.4256 | 3710  | -             | 0.5459          |\\n| 0.4268 | 3720  | -             | 0.5452          |\\n| 0.4279 | 3730  | -             | 0.5448          |\\n| 0.4290 | 3740  | -             | 0.5443          |\\n| 0.4302 | 3750  | -             | 0.5440          |\\n| 0.4313 | 3760  | -             | 0.5435          |\\n| 0.4325 | 3770  | -             | 0.5430          |\\n| 0.4336 | 3780  | -             | 0.5423          |\\n| 0.4348 | 3790  | -             | 0.5418          |\\n| 0.4359 | 3800  | 0.9672        | 0.5415          |\\n| 0.4371 | 3810  | -             | 0.5413          |\\n| 0.4382 | 3820  | -             | 0.5410          |\\n| 0.4394 | 3830  | -             | 0.5406          |\\n| 0.4405 | 3840  | -             | 0.5403          |\\n| 0.4417 | 3850  | -             | 0.5397          |\\n| 0.4428 | 3860  | -             | 0.5394          |\\n| 0.4440 | 3870  | -             | 0.5386          |\\n| 0.4451 | 3880  | -             | 0.5378          |\\n| 0.4463 | 3890  | -             | 0.5370          |\\n| 0.4474 | 3900  | 0.926         | 0.5360          |\\n| 0.4485 | 3910  | -             | 0.5351          |\\n| 0.4497 | 3920  | -             | 0.5346          |\\n| 0.4508 | 3930  | -             | 0.5343          |\\n| 0.4520 | 3940  | -             | 0.5339          |\\n| 0.4531 | 3950  | -             | 0.5337          |\\n| 0.4543 | 3960  | -             | 0.5334          |\\n| 0.4554 | 3970  | -             | 0.5330          |\\n| 0.4566 | 3980  | -             | 0.5327          |\\n| 0.4577 | 3990  | -             | 0.5324          |\\n| 0.4589 | 4000  | 0.867         | 0.5319          |\\n| 0.4600 | 4010  | -             | 0.5313          |\\n| 0.4612 | 4020  | -             | 0.5308          |\\n| 0.4623 | 4030  | -             | 0.5300          |\\n| 0.4635 | 4040  | -             | 0.5293          |\\n| 0.4646 | 4050  | -             | 0.5287          |\\n| 0.4658 | 4060  | -             | 0.5284          |\\n| 0.4669 | 4070  | -             | 0.5281          |\\n| 0.4681 | 4080  | -             | 0.5277          |\\n| 0.4692 | 4090  | -             | 0.5272          |\\n| 0.4703 | 4100  | 0.916         | 0.5267          |\\n| 0.4715 | 4110  | -             | 0.5260          |\\n| 0.4726 | 4120  | -             | 0.5252          |\\n| 0.4738 | 4130  | -             | 0.5246          |\\n| 0.4749 | 4140  | -             | 0.5239          |\\n| 0.4761 | 4150  | -             | 0.5232          |\\n| 0.4772 | 4160  | -             | 0.5225          |\\n| 0.4784 | 4170  | -             | 0.5221          |\\n| 0.4795 | 4180  | -             | 0.5216          |\\n| 0.4807 | 4190  | -             | 0.5211          |\\n| 0.4818 | 4200  | 0.9667        | 0.5206          |\\n| 0.4830 | 4210  | -             | 0.5204          |\\n| 0.4841 | 4220  | -             | 0.5200          |\\n| 0.4853 | 4230  | -             | 0.5192          |\\n| 0.4864 | 4240  | -             | 0.5187          |\\n| 0.4876 | 4250  | -             | 0.5185          |\\n| 0.4887 | 4260  | -             | 0.5179          |\\n| 0.4898 | 4270  | -             | 0.5173          |\\n| 0.4910 | 4280  | -             | 0.5170          |\\n| 0.4921 | 4290  | -             | 0.5165          |\\n| 0.4933 | 4300  | 0.9276        | 0.5160          |\\n| 0.4944 | 4310  | -             | 0.5154          |\\n| 0.4956 | 4320  | -             | 0.5150          |\\n| 0.4967 | 4330  | -             | 0.5144          |\\n| 0.4979 | 4340  | -             | 0.5141          |\\n| 0.4990 | 4350  | -             | 0.5139          |\\n| 0.5002 | 4360  | -             | 0.5138          |\\n| 0.5013 | 4370  | -             | 0.5136          |\\n| 0.5025 | 4380  | -             | 0.5133          |\\n| 0.5036 | 4390  | -             | 0.5129          |\\n| 0.5048 | 4400  | 0.9331        | 0.5126          |\\n| 0.5059 | 4410  | -             | 0.5123          |\\n| 0.5071 | 4420  | -             | 0.5117          |\\n| 0.5082 | 4430  | -             | 0.5113          |\\n| 0.5093 | 4440  | -             | 0.5108          |\\n| 0.5105 | 4450  | -             | 0.5106          |\\n| 0.5116 | 4460  | -             | 0.5106          |\\n| 0.5128 | 4470  | -             | 0.5106          |\\n| 0.5139 | 4480  | -             | 0.5104          |\\n| 0.5151 | 4490  | -             | 0.5102          |\\n| 0.5162 | 4500  | 0.907         | 0.5097          |\\n| 0.5174 | 4510  | -             | 0.5092          |\\n| 0.5185 | 4520  | -             | 0.5086          |\\n| 0.5197 | 4530  | -             | 0.5082          |\\n| 0.5208 | 4540  | -             | 0.5079          |\\n| 0.5220 | 4550  | -             | 0.5075          |\\n| 0.5231 | 4560  | -             | 0.5071          |\\n| 0.5243 | 4570  | -             | 0.5067          |\\n| 0.5254 | 4580  | -             | 0.5066          |\\n| 0.5266 | 4590  | -             | 0.5062          |\\n| 0.5277 | 4600  | 0.913         | 0.5059          |\\n| 0.5289 | 4610  | -             | 0.5056          |\\n| 0.5300 | 4620  | -             | 0.5052          |\\n| 0.5311 | 4630  | -             | 0.5046          |\\n| 0.5323 | 4640  | -             | 0.5039          |\\n| 0.5334 | 4650  | -             | 0.5033          |\\n| 0.5346 | 4660  | -             | 0.5030          |\\n| 0.5357 | 4670  | -             | 0.5028          |\\n| 0.5369 | 4680  | -             | 0.5027          |\\n| 0.5380 | 4690  | -             | 0.5023          |\\n| 0.5392 | 4700  | 0.9047        | 0.5020          |\\n| 0.5403 | 4710  | -             | 0.5018          |\\n| 0.5415 | 4720  | -             | 0.5015          |\\n| 0.5426 | 4730  | -             | 0.5009          |\\n| 0.5438 | 4740  | -             | 0.5003          |\\n| 0.5449 | 4750  | -             | 0.4997          |\\n| 0.5461 | 4760  | -             | 0.4991          |\\n| 0.5472 | 4770  | -             | 0.4984          |\\n| 0.5484 | 4780  | -             | 0.4980          |\\n| 0.5495 | 4790  | -             | 0.4980          |\\n| 0.5506 | 4800  | 0.887         | 0.4979          |\\n| 0.5518 | 4810  | -             | 0.4975          |\\n| 0.5529 | 4820  | -             | 0.4973          |\\n| 0.5541 | 4830  | -             | 0.4969          |\\n| 0.5552 | 4840  | -             | 0.4966          |\\n| 0.5564 | 4850  | -             | 0.4964          |\\n| 0.5575 | 4860  | -             | 0.4964          |\\n| 0.5587 | 4870  | -             | 0.4960          |\\n| 0.5598 | 4880  | -             | 0.4957          |\\n| 0.5610 | 4890  | -             | 0.4955          |\\n| 0.5621 | 4900  | 0.8645        | 0.4952          |\\n| 0.5633 | 4910  | -             | 0.4950          |\\n| 0.5644 | 4920  | -             | 0.4952          |\\n| 0.5656 | 4930  | -             | 0.4949          |\\n| 0.5667 | 4940  | -             | 0.4943          |\\n| 0.5679 | 4950  | -             | 0.4938          |\\n| 0.5690 | 4960  | -             | 0.4936          |\\n| 0.5702 | 4970  | -             | 0.4933          |\\n| 0.5713 | 4980  | -             | 0.4931          |\\n| 0.5724 | 4990  | -             | 0.4929          |\\n| 0.5736 | 5000  | 0.8348        | 0.4924          |\\n| 0.5747 | 5010  | -             | 0.4921          |\\n| 0.5759 | 5020  | -             | 0.4915          |\\n| 0.5770 | 5030  | -             | 0.4911          |\\n| 0.5782 | 5040  | -             | 0.4909          |\\n| 0.5793 | 5050  | -             | 0.4905          |\\n| 0.5805 | 5060  | -             | 0.4900          |\\n| 0.5816 | 5070  | -             | 0.4892          |\\n| 0.5828 | 5080  | -             | 0.4886          |\\n| 0.5839 | 5090  | -             | 0.4883          |\\n| 0.5851 | 5100  | 0.871         | 0.4879          |\\n| 0.5862 | 5110  | -             | 0.4877          |\\n| 0.5874 | 5120  | -             | 0.4874          |\\n| 0.5885 | 5130  | -             | 0.4870          |\\n| 0.5897 | 5140  | -             | 0.4867          |\\n| 0.5908 | 5150  | -             | 0.4864          |\\n| 0.5919 | 5160  | -             | 0.4862          |\\n| 0.5931 | 5170  | -             | 0.4860          |\\n| 0.5942 | 5180  | -             | 0.4857          |\\n| 0.5954 | 5190  | -             | 0.4855          |\\n| 0.5965 | 5200  | 0.8522        | 0.4850          |\\n| 0.5977 | 5210  | -             | 0.4846          |\\n| 0.5988 | 5220  | -             | 0.4844          |\\n| 0.6000 | 5230  | -             | 0.4842          |\\n| 0.6011 | 5240  | -             | 0.4837          |\\n| 0.6023 | 5250  | -             | 0.4835          |\\n| 0.6034 | 5260  | -             | 0.4831          |\\n| 0.6046 | 5270  | -             | 0.4826          |\\n| 0.6057 | 5280  | -             | 0.4822          |\\n| 0.6069 | 5290  | -             | 0.4822          |\\n| 0.6080 | 5300  | 0.869         | 0.4820          |\\n| 0.6092 | 5310  | -             | 0.4818          |\\n| 0.6103 | 5320  | -             | 0.4819          |\\n| 0.6114 | 5330  | -             | 0.4819          |\\n| 0.6126 | 5340  | -             | 0.4815          |\\n| 0.6137 | 5350  | -             | 0.4813          |\\n| 0.6149 | 5360  | -             | 0.4812          |\\n| 0.6160 | 5370  | -             | 0.4810          |\\n| 0.6172 | 5380  | -             | 0.4809          |\\n| 0.6183 | 5390  | -             | 0.4806          |\\n| 0.6195 | 5400  | 0.8548        | 0.4805          |\\n| 0.6206 | 5410  | -             | 0.4800          |\\n| 0.6218 | 5420  | -             | 0.4798          |\\n| 0.6229 | 5430  | -             | 0.4795          |\\n| 0.6241 | 5440  | -             | 0.4792          |\\n| 0.6252 | 5450  | -             | 0.4790          |\\n| 0.6264 | 5460  | -             | 0.4790          |\\n| 0.6275 | 5470  | -             | 0.4791          |\\n| 0.6287 | 5480  | -             | 0.4794          |\\n| 0.6298 | 5490  | -             | 0.4792          |\\n| 0.6310 | 5500  | 0.8366        | 0.4790          |\\n| 0.6321 | 5510  | -             | 0.4786          |\\n| 0.6332 | 5520  | -             | 0.4780          |\\n| 0.6344 | 5530  | -             | 0.4773          |\\n| 0.6355 | 5540  | -             | 0.4768          |\\n| 0.6367 | 5550  | -             | 0.4767          |\\n| 0.6378 | 5560  | -             | 0.4765          |\\n| 0.6390 | 5570  | -             | 0.4765          |\\n| 0.6401 | 5580  | -             | 0.4763          |\\n| 0.6413 | 5590  | -             | 0.4760          |\\n| 0.6424 | 5600  | 0.8696        | 0.4757          |\\n| 0.6436 | 5610  | -             | 0.4754          |\\n| 0.6447 | 5620  | -             | 0.4752          |\\n| 0.6459 | 5630  | -             | 0.4751          |\\n| 0.6470 | 5640  | -             | 0.4747          |\\n| 0.6482 | 5650  | -             | 0.4747          |\\n| 0.6493 | 5660  | -             | 0.4742          |\\n| 0.6505 | 5670  | -             | 0.4740          |\\n| 0.6516 | 5680  | -             | 0.4736          |\\n| 0.6527 | 5690  | -             | 0.4730          |\\n| 0.6539 | 5700  | 0.8302        | 0.4725          |\\n| 0.6550 | 5710  | -             | 0.4723          |\\n| 0.6562 | 5720  | -             | 0.4720          |\\n| 0.6573 | 5730  | -             | 0.4718          |\\n| 0.6585 | 5740  | -             | 0.4715          |\\n| 0.6596 | 5750  | -             | 0.4714          |\\n| 0.6608 | 5760  | -             | 0.4711          |\\n| 0.6619 | 5770  | -             | 0.4707          |\\n| 0.6631 | 5780  | -             | 0.4707          |\\n| 0.6642 | 5790  | -             | 0.4703          |\\n| 0.6654 | 5800  | 0.8128        | 0.4703          |\\n| 0.6665 | 5810  | -             | 0.4701          |\\n| 0.6677 | 5820  | -             | 0.4699          |\\n| 0.6688 | 5830  | -             | 0.4697          |\\n| 0.6700 | 5840  | -             | 0.4698          |\\n| 0.6711 | 5850  | -             | 0.4695          |\\n| 0.6722 | 5860  | -             | 0.4691          |\\n| 0.6734 | 5870  | -             | 0.4689          |\\n| 0.6745 | 5880  | -             | 0.4689          |\\n| 0.6757 | 5890  | -             | 0.4688          |\\n| 0.6768 | 5900  | 0.8437        | 0.4683          |\\n| 0.6780 | 5910  | -             | 0.4683          |\\n| 0.6791 | 5920  | -             | 0.4681          |\\n| 0.6803 | 5930  | -             | 0.4678          |\\n| 0.6814 | 5940  | -             | 0.4677          |\\n| 0.6826 | 5950  | -             | 0.4676          |\\n| 0.6837 | 5960  | -             | 0.4673          |\\n| 0.6849 | 5970  | -             | 0.4668          |\\n| 0.6860 | 5980  | -             | 0.4667          |\\n| 0.6872 | 5990  | -             | 0.4661          |\\n| 0.6883 | 6000  | 0.7774        | 0.4657          |\\n| 0.6895 | 6010  | -             | 0.4654          |\\n| 0.6906 | 6020  | -             | 0.4650          |\\n| 0.6918 | 6030  | -             | 0.4648          |\\n| 0.6929 | 6040  | -             | 0.4646          |\\n| 0.6940 | 6050  | -             | 0.4644          |\\n| 0.6952 | 6060  | -             | 0.4643          |\\n| 0.6963 | 6070  | -             | 0.4641          |\\n| 0.6975 | 6080  | -             | 0.4640          |\\n| 0.6986 | 6090  | -             | 0.4638          |\\n| 0.6998 | 6100  | 0.834         | 0.4637          |\\n| 0.7009 | 6110  | -             | 0.4633          |\\n| 0.7021 | 6120  | -             | 0.4632          |\\n| 0.7032 | 6130  | -             | 0.4631          |\\n| 0.7044 | 6140  | -             | 0.4628          |\\n| 0.7055 | 6150  | -             | 0.4627          |\\n| 0.7067 | 6160  | -             | 0.4623          |\\n| 0.7078 | 6170  | -             | 0.4617          |\\n| 0.7090 | 6180  | -             | 0.4615          |\\n| 0.7101 | 6190  | -             | 0.4614          |\\n| 0.7113 | 6200  | 0.8118        | 0.4612          |\\n| 0.7124 | 6210  | -             | 0.4612          |\\n| 0.7135 | 6220  | -             | 0.4612          |\\n| 0.7147 | 6230  | -             | 0.4610          |\\n| 0.7158 | 6240  | -             | 0.4609          |\\n| 0.7170 | 6250  | -             | 0.4610          |\\n| 0.7181 | 6260  | -             | 0.4611          |\\n| 0.7193 | 6270  | -             | 0.4607          |\\n| 0.7204 | 6280  | -             | 0.4599          |\\n| 0.7216 | 6290  | -             | 0.4598          |\\n| 0.7227 | 6300  | 0.7884        | 0.4600          |\\n| 0.7239 | 6310  | -             | 0.4599          |\\n| 0.7250 | 6320  | -             | 0.4600          |\\n| 0.7262 | 6330  | -             | 0.4601          |\\n| 0.7273 | 6340  | -             | 0.4603          |\\n| 0.7285 | 6350  | -             | 0.4603          |\\n| 0.7296 | 6360  | -             | 0.4598          |\\n| 0.7308 | 6370  | -             | 0.4597          |\\n| 0.7319 | 6380  | -             | 0.4596          |\\n| 0.7331 | 6390  | -             | 0.4594          |\\n| 0.7342 | 6400  | 0.8092        | 0.4590          |\\n| 0.7353 | 6410  | -             | 0.4588          |\\n| 0.7365 | 6420  | -             | 0.4585          |\\n| 0.7376 | 6430  | -             | 0.4584          |\\n| 0.7388 | 6440  | -             | 0.4580          |\\n| 0.7399 | 6450  | -             | 0.4574          |\\n| 0.7411 | 6460  | -             | 0.4570          |\\n| 0.7422 | 6470  | -             | 0.4566          |\\n| 0.7434 | 6480  | -             | 0.4563          |\\n| 0.7445 | 6490  | -             | 0.4560          |\\n| 0.7457 | 6500  | 0.8195        | 0.4557          |\\n| 0.7468 | 6510  | -             | 0.4556          |\\n| 0.7480 | 6520  | -             | 0.4554          |\\n| 0.7491 | 6530  | -             | 0.4551          |\\n| 0.7503 | 6540  | -             | 0.4548          |\\n| 0.7514 | 6550  | -             | 0.4545          |\\n| 0.7526 | 6560  | -             | 0.4543          |\\n| 0.7537 | 6570  | -             | 0.4541          |\\n| 0.7548 | 6580  | -             | 0.4540          |\\n| 0.7560 | 6590  | -             | 0.4538          |\\n| 0.7571 | 6600  | 0.8163        | 0.4535          |\\n| 0.7583 | 6610  | -             | 0.4533          |\\n| 0.7594 | 6620  | -             | 0.4536          |\\n| 0.7606 | 6630  | -             | 0.4535          |\\n| 0.7617 | 6640  | -             | 0.4533          |\\n| 0.7629 | 6650  | -             | 0.4532          |\\n| 0.7640 | 6660  | -             | 0.4531          |\\n| 0.7652 | 6670  | -             | 0.4531          |\\n| 0.7663 | 6680  | -             | 0.4530          |\\n| 0.7675 | 6690  | -             | 0.4528          |\\n| 0.7686 | 6700  | 0.8091        | 0.4527          |\\n| 0.7698 | 6710  | -             | 0.4527          |\\n| 0.7709 | 6720  | -             | 0.4526          |\\n| 0.7721 | 6730  | -             | 0.4525          |\\n| 0.7732 | 6740  | -             | 0.4524          |\\n| 0.7743 | 6750  | -             | 0.4521          |\\n| 0.7755 | 6760  | -             | 0.4517          |\\n| 0.7766 | 6770  | -             | 0.4514          |\\n| 0.7778 | 6780  | -             | 0.4512          |\\n| 0.7789 | 6790  | -             | 0.4514          |\\n| 0.7801 | 6800  | 0.8098        | 0.4515          |\\n| 0.7812 | 6810  | -             | 0.4514          |\\n| 0.7824 | 6820  | -             | 0.4511          |\\n| 0.7835 | 6830  | -             | 0.4507          |\\n| 0.7847 | 6840  | -             | 0.4505          |\\n| 0.7858 | 6850  | -             | 0.4504          |\\n| 0.7870 | 6860  | -             | 0.4503          |\\n| 0.7881 | 6870  | -             | 0.4500          |\\n| 0.7893 | 6880  | -             | 0.4498          |\\n| 0.7904 | 6890  | -             | 0.4495          |\\n| 0.7916 | 6900  | 0.7857        | 0.4491          |\\n| 0.7927 | 6910  | -             | 0.4490          |\\n| 0.7939 | 6920  | -             | 0.4488          |\\n| 0.7950 | 6930  | -             | 0.4488          |\\n| 0.7961 | 6940  | -             | 0.4488          |\\n| 0.7973 | 6950  | -             | 0.4487          |\\n| 0.7984 | 6960  | -             | 0.4484          |\\n| 0.7996 | 6970  | -             | 0.4482          |\\n| 0.8007 | 6980  | -             | 0.4483          |\\n| 0.8019 | 6990  | -             | 0.4481          |\\n| 0.8030 | 7000  | 0.7817        | 0.4477          |\\n| 0.8042 | 7010  | -             | 0.4476          |\\n| 0.8053 | 7020  | -             | 0.4471          |\\n| 0.8065 | 7030  | -             | 0.4469          |\\n| 0.8076 | 7040  | -             | 0.4468          |\\n| 0.8088 | 7050  | -             | 0.4465          |\\n| 0.8099 | 7060  | -             | 0.4460          |\\n| 0.8111 | 7070  | -             | 0.4458          |\\n| 0.8122 | 7080  | -             | 0.4458          |\\n| 0.8134 | 7090  | -             | 0.4454          |\\n| 0.8145 | 7100  | 0.779         | 0.4452          |\\n| 0.8156 | 7110  | -             | 0.4449          |\\n| 0.8168 | 7120  | -             | 0.4448          |\\n| 0.8179 | 7130  | -             | 0.4446          |\\n| 0.8191 | 7140  | -             | 0.4442          |\\n| 0.8202 | 7150  | -             | 0.4442          |\\n| 0.8214 | 7160  | -             | 0.4441          |\\n| 0.8225 | 7170  | -             | 0.4440          |\\n| 0.8237 | 7180  | -             | 0.4437          |\\n| 0.8248 | 7190  | -             | 0.4434          |\\n| 0.8260 | 7200  | 0.7807        | 0.4434          |\\n| 0.8271 | 7210  | -             | 0.4435          |\\n| 0.8283 | 7220  | -             | 0.4433          |\\n| 0.8294 | 7230  | -             | 0.4431          |\\n| 0.8306 | 7240  | -             | 0.4430          |\\n| 0.8317 | 7250  | -             | 0.4428          |\\n| 0.8329 | 7260  | -             | 0.4426          |\\n| 0.8340 | 7270  | -             | 0.4424          |\\n| 0.8351 | 7280  | -             | 0.4428          |\\n| 0.8363 | 7290  | -             | 0.4426          |\\n| 0.8374 | 7300  | 0.7724        | 0.4423          |\\n| 0.8386 | 7310  | -             | 0.4419          |\\n| 0.8397 | 7320  | -             | 0.4418          |\\n| 0.8409 | 7330  | -             | 0.4417          |\\n| 0.8420 | 7340  | -             | 0.4415          |\\n| 0.8432 | 7350  | -             | 0.4413          |\\n| 0.8443 | 7360  | -             | 0.4409          |\\n| 0.8455 | 7370  | -             | 0.4406          |\\n| 0.8466 | 7380  | -             | 0.4405          |\\n| 0.8478 | 7390  | -             | 0.4400          |\\n| 0.8489 | 7400  | 0.7898        | 0.4393          |\\n| 0.8501 | 7410  | -             | 0.4389          |\\n| 0.8512 | 7420  | -             | 0.4384          |\\n| 0.8524 | 7430  | -             | 0.4381          |\\n| 0.8535 | 7440  | -             | 0.4380          |\\n| 0.8547 | 7450  | -             | 0.4380          |\\n| 0.8558 | 7460  | -             | 0.4379          |\\n| 0.8569 | 7470  | -             | 0.4377          |\\n| 0.8581 | 7480  | -             | 0.4377          |\\n| 0.8592 | 7490  | -             | 0.4376          |\\n| 0.8604 | 7500  | 0.8009        | 0.4375          |\\n| 0.8615 | 7510  | -             | 0.4371          |\\n| 0.8627 | 7520  | -             | 0.4369          |\\n| 0.8638 | 7530  | -             | 0.4365          |\\n| 0.8650 | 7540  | -             | 0.4362          |\\n| 0.8661 | 7550  | -             | 0.4359          |\\n| 0.8673 | 7560  | -             | 0.4357          |\\n| 0.8684 | 7570  | -             | 0.4355          |\\n| 0.8696 | 7580  | -             | 0.4351          |\\n| 0.8707 | 7590  | -             | 0.4347          |\\n| 0.8719 | 7600  | 0.7847        | 0.4346          |\\n| 0.8730 | 7610  | -             | 0.4346          |\\n| 0.8742 | 7620  | -             | 0.4344          |\\n| 0.8753 | 7630  | -             | 0.4343          |\\n| 0.8764 | 7640  | -             | 0.4338          |\\n| 0.8776 | 7650  | -             | 0.4336          |\\n| 0.8787 | 7660  | -             | 0.4332          |\\n| 0.8799 | 7670  | -             | 0.4331          |\\n| 0.8810 | 7680  | -             | 0.4329          |\\n| 0.8822 | 7690  | -             | 0.4326          |\\n| 0.8833 | 7700  | 0.7668        | 0.4324          |\\n| 0.8845 | 7710  | -             | 0.4325          |\\n| 0.8856 | 7720  | -             | 0.4327          |\\n| 0.8868 | 7730  | -             | 0.4329          |\\n| 0.8879 | 7740  | -             | 0.4328          |\\n| 0.8891 | 7750  | -             | 0.4325          |\\n| 0.8902 | 7760  | -             | 0.4325          |\\n| 0.8914 | 7770  | -             | 0.4326          |\\n| 0.8925 | 7780  | -             | 0.4324          |\\n| 0.8937 | 7790  | -             | 0.4322          |\\n| 0.8948 | 7800  | 0.7987        | 0.4320          |\\n| 0.8960 | 7810  | -             | 0.4319          |\\n| 0.8971 | 7820  | -             | 0.4318          |\\n| 0.8982 | 7830  | -             | 0.4315          |\\n| 0.8994 | 7840  | -             | 0.4312          |\\n| 0.9005 | 7850  | -             | 0.4308          |\\n| 0.9017 | 7860  | -             | 0.4308          |\\n| 0.9028 | 7870  | -             | 0.4309          |\\n| 0.9040 | 7880  | -             | 0.4306          |\\n| 0.9051 | 7890  | -             | 0.4305          |\\n| 0.9063 | 7900  | 0.7691        | 0.4305          |\\n| 0.9074 | 7910  | -             | 0.4305          |\\n| 0.9086 | 7920  | -             | 0.4308          |\\n| 0.9097 | 7930  | -             | 0.4309          |\\n| 0.9109 | 7940  | -             | 0.4309          |\\n| 0.9120 | 7950  | -             | 0.4305          |\\n| 0.9132 | 7960  | -             | 0.4297          |\\n| 0.9143 | 7970  | -             | 0.4294          |\\n| 0.9155 | 7980  | -             | 0.4292          |\\n| 0.9166 | 7990  | -             | 0.4292          |\\n| 0.9177 | 8000  | 0.7828        | 0.4289          |\\n| 0.9189 | 8010  | -             | 0.4288          |\\n| 0.9200 | 8020  | -             | 0.4289          |\\n| 0.9212 | 8030  | -             | 0.4285          |\\n| 0.9223 | 8040  | -             | 0.4286          |\\n| 0.9235 | 8050  | -             | 0.4289          |\\n| 0.9246 | 8060  | -             | 0.4288          |\\n| 0.9258 | 8070  | -             | 0.4290          |\\n| 0.9269 | 8080  | -             | 0.4289          |\\n| 0.9281 | 8090  | -             | 0.4287          |\\n| 0.9292 | 8100  | 0.7544        | 0.4288          |\\n| 0.9304 | 8110  | -             | 0.4284          |\\n| 0.9315 | 8120  | -             | 0.4287          |\\n| 0.9327 | 8130  | -             | 0.4289          |\\n| 0.9338 | 8140  | -             | 0.4293          |\\n| 0.9350 | 8150  | -             | 0.4292          |\\n| 0.9361 | 8160  | -             | 0.4289          |\\n| 0.9372 | 8170  | -             | 0.4286          |\\n| 0.9384 | 8180  | -             | 0.4280          |\\n| 0.9395 | 8190  | -             | 0.4281          |\\n| 0.9407 | 8200  | 0.7502        | 0.4281          |\\n| 0.9418 | 8210  | -             | 0.4278          |\\n| 0.9430 | 8220  | -             | 0.4276          |\\n| 0.9441 | 8230  | -             | 0.4274          |\\n| 0.9453 | 8240  | -             | 0.4270          |\\n| 0.9464 | 8250  | -             | 0.4267          |\\n| 0.9476 | 8260  | -             | 0.4263          |\\n| 0.9487 | 8270  | -             | 0.4261          |\\n| 0.9499 | 8280  | -             | 0.4257          |\\n| 0.9510 | 8290  | -             | 0.4254          |\\n| 0.9522 | 8300  | 0.7818        | 0.4255          |\\n| 0.9533 | 8310  | -             | 0.4255          |\\n| 0.9545 | 8320  | -             | 0.4254          |\\n| 0.9556 | 8330  | -             | 0.4252          |\\n| 0.9568 | 8340  | -             | 0.4249          |\\n| 0.9579 | 8350  | -             | 0.4249          |\\n| 0.9590 | 8360  | -             | 0.4248          |\\n| 0.9602 | 8370  | -             | 0.4249          |\\n| 0.9613 | 8380  | -             | 0.4248          |\\n| 0.9625 | 8390  | -             | 0.4246          |\\n| 0.9636 | 8400  | 0.7606        | 0.4243          |\\n| 0.9648 | 8410  | -             | 0.4242          |\\n| 0.9659 | 8420  | -             | 0.4240          |\\n| 0.9671 | 8430  | -             | 0.4239          |\\n| 0.9682 | 8440  | -             | 0.4238          |\\n| 0.9694 | 8450  | -             | 0.4238          |\\n| 0.9705 | 8460  | -             | 0.4237          |\\n| 0.9717 | 8470  | -             | 0.4236          |\\n| 0.9728 | 8480  | -             | 0.4232          |\\n| 0.9740 | 8490  | -             | 0.4229          |\\n| 0.9751 | 8500  | 0.7416        | 0.4227          |\\n| 0.9763 | 8510  | -             | 0.4226          |\\n| 0.9774 | 8520  | -             | 0.4220          |\\n| 0.9785 | 8530  | -             | 0.4218          |\\n| 0.9797 | 8540  | -             | 0.4217          |\\n| 0.9808 | 8550  | -             | 0.4217          |\\n| 0.9820 | 8560  | -             | 0.4215          |\\n| 0.9831 | 8570  | -             | 0.4216          |\\n| 0.9843 | 8580  | -             | 0.4217          |\\n| 0.9854 | 8590  | -             | 0.4216          |\\n| 0.9866 | 8600  | 0.748         | 0.4217          |\\n| 0.9877 | 8610  | -             | 0.4215          |\\n| 0.9889 | 8620  | -             | 0.4216          |\\n| 0.9900 | 8630  | -             | 0.4218          |\\n| 0.9912 | 8640  | -             | 0.4218          |\\n| 0.9923 | 8650  | -             | 0.4219          |\\n| 0.9935 | 8660  | -             | 0.4217          |\\n| 0.9946 | 8670  | -             | 0.4217          |\\n| 0.9958 | 8680  | -             | 0.4214          |\\n| 0.9969 | 8690  | -             | 0.4210          |\\n| 0.9980 | 8700  | 0.7553        | 0.4205          |\\n| 0.9992 | 8710  | -             | 0.4200          |\\n| 1.0003 | 8720  | -             | 0.4199          |\\n| 1.0015 | 8730  | -             | 0.4199          |\\n| 1.0026 | 8740  | -             | 0.4199          |\\n| 1.0038 | 8750  | -             | 0.4198          |\\n| 1.0049 | 8760  | -             | 0.4200          |\\n| 1.0061 | 8770  | -             | 0.4198          |\\n| 1.0072 | 8780  | -             | 0.4195          |\\n| 1.0084 | 8790  | -             | 0.4194          |\\n| 1.0095 | 8800  | 0.7202        | 0.4191          |\\n| 1.0107 | 8810  | -             | 0.4190          |\\n| 1.0118 | 8820  | -             | 0.4188          |\\n| 1.0130 | 8830  | -             | 0.4188          |\\n| 1.0141 | 8840  | -             | 0.4192          |\\n| 1.0153 | 8850  | -             | 0.4190          |\\n| 1.0164 | 8860  | -             | 0.4191          |\\n| 1.0176 | 8870  | -             | 0.4190          |\\n| 1.0187 | 8880  | -             | 0.4192          |\\n| 1.0198 | 8890  | -             | 0.4190          |\\n| 1.0210 | 8900  | 0.7567        | 0.4189          |\\n| 1.0221 | 8910  | -             | 0.4188          |\\n| 1.0233 | 8920  | -             | 0.4189          |\\n| 1.0244 | 8930  | -             | 0.4188          |\\n| 1.0256 | 8940  | -             | 0.4187          |\\n| 1.0267 | 8950  | -             | 0.4183          |\\n| 1.0279 | 8960  | -             | 0.4182          |\\n| 1.0290 | 8970  | -             | 0.4182          |\\n| 1.0302 | 8980  | -             | 0.4184          |\\n| 1.0313 | 8990  | -             | 0.4181          |\\n| 1.0325 | 9000  | 0.7345        | 0.4177          |\\n| 1.0336 | 9010  | -             | 0.4173          |\\n| 1.0348 | 9020  | -             | 0.4171          |\\n| 1.0359 | 9030  | -             | 0.4172          |\\n| 1.0371 | 9040  | -             | 0.4171          |\\n| 1.0382 | 9050  | -             | 0.4172          |\\n| 1.0393 | 9060  | -             | 0.4172          |\\n| 1.0405 | 9070  | -             | 0.4170          |\\n| 1.0416 | 9080  | -             | 0.4165          |\\n| 1.0428 | 9090  | -             | 0.4162          |\\n| 1.0439 | 9100  | 0.7344        | 0.4162          |\\n| 1.0451 | 9110  | -             | 0.4160          |\\n| 1.0462 | 9120  | -             | 0.4158          |\\n| 1.0474 | 9130  | -             | 0.4157          |\\n| 1.0485 | 9140  | -             | 0.4157          |\\n| 1.0497 | 9150  | -             | 0.4156          |\\n| 1.0508 | 9160  | -             | 0.4153          |\\n| 1.0520 | 9170  | -             | 0.4153          |\\n| 1.0531 | 9180  | -             | 0.4154          |\\n| 1.0543 | 9190  | -             | 0.4154          |\\n| 1.0554 | 9200  | 0.7233        | 0.4157          |\\n| 1.0566 | 9210  | -             | 0.4157          |\\n| 1.0577 | 9220  | -             | 0.4156          |\\n| 1.0589 | 9230  | -             | 0.4155          |\\n| 1.0600 | 9240  | -             | 0.4153          |\\n| 1.0611 | 9250  | -             | 0.4154          |\\n| 1.0623 | 9260  | -             | 0.4155          |\\n| 1.0634 | 9270  | -             | 0.4154          |\\n| 1.0646 | 9280  | -             | 0.4151          |\\n| 1.0657 | 9290  | -             | 0.4149          |\\n| 1.0669 | 9300  | 0.7442        | 0.4148          |\\n| 1.0680 | 9310  | -             | 0.4144          |\\n| 1.0692 | 9320  | -             | 0.4143          |\\n| 1.0703 | 9330  | -             | 0.4141          |\\n| 1.0715 | 9340  | -             | 0.4140          |\\n| 1.0726 | 9350  | -             | 0.4138          |\\n| 1.0738 | 9360  | -             | 0.4136          |\\n| 1.0749 | 9370  | -             | 0.4133          |\\n| 1.0761 | 9380  | -             | 0.4132          |\\n| 1.0772 | 9390  | -             | 0.4130          |\\n| 1.0784 | 9400  | 0.722         | 0.4129          |\\n| 1.0795 | 9410  | -             | 0.4131          |\\n| 1.0806 | 9420  | -             | 0.4132          |\\n| 1.0818 | 9430  | -             | 0.4133          |\\n| 1.0829 | 9440  | -             | 0.4134          |\\n| 1.0841 | 9450  | -             | 0.4134          |\\n| 1.0852 | 9460  | -             | 0.4133          |\\n| 1.0864 | 9470  | -             | 0.4132          |\\n| 1.0875 | 9480  | -             | 0.4132          |\\n| 1.0887 | 9490  | -             | 0.4134          |\\n| 1.0898 | 9500  | 0.7433        | 0.4133          |\\n| 1.0910 | 9510  | -             | 0.4133          |\\n| 1.0921 | 9520  | -             | 0.4133          |\\n| 1.0933 | 9530  | -             | 0.4132          |\\n| 1.0944 | 9540  | -             | 0.4131          |\\n| 1.0956 | 9550  | -             | 0.4130          |\\n| 1.0967 | 9560  | -             | 0.4130          |\\n| 1.0979 | 9570  | -             | 0.4126          |\\n| 1.0990 | 9580  | -             | 0.4125          |\\n| 1.1001 | 9590  | -             | 0.4121          |\\n| 1.1013 | 9600  | 0.746         | 0.4119          |\\n| 1.1024 | 9610  | -             | 0.4117          |\\n| 1.1036 | 9620  | -             | 0.4112          |\\n| 1.1047 | 9630  | -             | 0.4109          |\\n| 1.1059 | 9640  | -             | 0.4106          |\\n| 1.1070 | 9650  | -             | 0.4101          |\\n| 1.1082 | 9660  | -             | 0.4101          |\\n| 1.1093 | 9670  | -             | 0.4102          |\\n| 1.1105 | 9680  | -             | 0.4102          |\\n| 1.1116 | 9690  | -             | 0.4101          |\\n| 1.1128 | 9700  | 0.7447        | 0.4099          |\\n| 1.1139 | 9710  | -             | 0.4100          |\\n| 1.1151 | 9720  | -             | 0.4098          |\\n| 1.1162 | 9730  | -             | 0.4097          |\\n| 1.1174 | 9740  | -             | 0.4094          |\\n| 1.1185 | 9750  | -             | 0.4097          |\\n| 1.1197 | 9760  | -             | 0.4096          |\\n| 1.1208 | 9770  | -             | 0.4096          |\\n| 1.1219 | 9780  | -             | 0.4097          |\\n| 1.1231 | 9790  | -             | 0.4097          |\\n| 1.1242 | 9800  | 0.7234        | 0.4094          |\\n| 1.1254 | 9810  | -             | 0.4090          |\\n| 1.1265 | 9820  | -             | 0.4090          |\\n| 1.1277 | 9830  | -             | 0.4091          |\\n| 1.1288 | 9840  | -             | 0.4091          |\\n| 1.1300 | 9850  | -             | 0.4090          |\\n| 1.1311 | 9860  | -             | 0.4088          |\\n| 1.1323 | 9870  | -             | 0.4088          |\\n| 1.1334 | 9880  | -             | 0.4085          |\\n| 1.1346 | 9890  | -             | 0.4085          |\\n| 1.1357 | 9900  | 0.7054        | 0.4084          |\\n| 1.1369 | 9910  | -             | 0.4087          |\\n| 1.1380 | 9920  | -             | 0.4089          |\\n| 1.1392 | 9930  | -             | 0.4089          |\\n| 1.1403 | 9940  | -             | 0.4088          |\\n| 1.1414 | 9950  | -             | 0.4091          |\\n| 1.1426 | 9960  | -             | 0.4088          |\\n| 1.1437 | 9970  | -             | 0.4086          |\\n| 1.1449 | 9980  | -             | 0.4084          |\\n| 1.1460 | 9990  | -             | 0.4089          |\\n| 1.1472 | 10000 | 0.7071        | 0.4088          |\\n| 1.1483 | 10010 | -             | 0.4086          |\\n| 1.1495 | 10020 | -             | 0.4081          |\\n| 1.1506 | 10030 | -             | 0.4079          |\\n| 1.1518 | 10040 | -             | 0.4079          |\\n| 1.1529 | 10050 | -             | 0.4081          |\\n| 1.1541 | 10060 | -             | 0.4081          |\\n| 1.1552 | 10070 | -             | 0.4080          |\\n| 1.1564 | 10080 | -             | 0.4079          |\\n| 1.1575 | 10090 | -             | 0.4078          |\\n| 1.1587 | 10100 | 0.7289        | 0.4075          |\\n| 1.1598 | 10110 | -             | 0.4072          |\\n| 1.1609 | 10120 | -             | 0.4070          |\\n| 1.1621 | 10130 | -             | 0.4070          |\\n| 1.1632 | 10140 | -             | 0.4074          |\\n| 1.1644 | 10150 | -             | 0.4074          |\\n| 1.1655 | 10160 | -             | 0.4073          |\\n| 1.1667 | 10170 | -             | 0.4073          |\\n| 1.1678 | 10180 | -             | 0.4072          |\\n| 1.1690 | 10190 | -             | 0.4073          |\\n| 1.1701 | 10200 | 0.758         | 0.4071          |\\n| 1.1713 | 10210 | -             | 0.4071          |\\n| 1.1724 | 10220 | -             | 0.4071          |\\n| 1.1736 | 10230 | -             | 0.4068          |\\n| 1.1747 | 10240 | -             | 0.4063          |\\n| 1.1759 | 10250 | -             | 0.4062          |\\n| 1.1770 | 10260 | -             | 0.4064          |\\n| 1.1782 | 10270 | -             | 0.4065          |\\n| 1.1793 | 10280 | -             | 0.4063          |\\n| 1.1805 | 10290 | -             | 0.4065          |\\n| 1.1816 | 10300 | 0.7322        | 0.4066          |\\n| 1.1827 | 10310 | -             | 0.4065          |\\n| 1.1839 | 10320 | -             | 0.4065          |\\n| 1.1850 | 10330 | -             | 0.4061          |\\n| 1.1862 | 10340 | -             | 0.4060          |\\n| 1.1873 | 10350 | -             | 0.4057          |\\n| 1.1885 | 10360 | -             | 0.4056          |\\n| 1.1896 | 10370 | -             | 0.4056          |\\n| 1.1908 | 10380 | -             | 0.4059          |\\n| 1.1919 | 10390 | -             | 0.4061          |\\n| 1.1931 | 10400 | 0.6948        | 0.4059          |\\n| 1.1942 | 10410 | -             | 0.4059          |\\n| 1.1954 | 10420 | -             | 0.4060          |\\n| 1.1965 | 10430 | -             | 0.4058          |\\n| 1.1977 | 10440 | -             | 0.4057          |\\n| 1.1988 | 10450 | -             | 0.4056          |\\n| 1.2000 | 10460 | -             | 0.4056          |\\n| 1.2011 | 10470 | -             | 0.4056          |\\n| 1.2022 | 10480 | -             | 0.4057          |\\n| 1.2034 | 10490 | -             | 0.4056          |\\n| 1.2045 | 10500 | 0.7185        | 0.4055          |\\n| 1.2057 | 10510 | -             | 0.4056          |\\n| 1.2068 | 10520 | -             | 0.4054          |\\n| 1.2080 | 10530 | -             | 0.4053          |\\n| 1.2091 | 10540 | -             | 0.4051          |\\n| 1.2103 | 10550 | -             | 0.4050          |\\n| 1.2114 | 10560 | -             | 0.4051          |\\n| 1.2126 | 10570 | -             | 0.4052          |\\n| 1.2137 | 10580 | -             | 0.4053          |\\n| 1.2149 | 10590 | -             | 0.4053          |\\n| 1.2160 | 10600 | 0.7039        | 0.4053          |\\n| 1.2172 | 10610 | -             | 0.4054          |\\n| 1.2183 | 10620 | -             | 0.4051          |\\n| 1.2195 | 10630 | -             | 0.4050          |\\n| 1.2206 | 10640 | -             | 0.4048          |\\n| 1.2218 | 10650 | -             | 0.4044          |\\n| 1.2229 | 10660 | -             | 0.4046          |\\n| 1.2240 | 10670 | -             | 0.4044          |\\n| 1.2252 | 10680 | -             | 0.4041          |\\n| 1.2263 | 10690 | -             | 0.4039          |\\n| 1.2275 | 10700 | 0.6969        | 0.4037          |\\n| 1.2286 | 10710 | -             | 0.4037          |\\n| 1.2298 | 10720 | -             | 0.4035          |\\n| 1.2309 | 10730 | -             | 0.4036          |\\n| 1.2321 | 10740 | -             | 0.4035          |\\n| 1.2332 | 10750 | -             | 0.4038          |\\n| 1.2344 | 10760 | -             | 0.4038          |\\n| 1.2355 | 10770 | -             | 0.4037          |\\n| 1.2367 | 10780 | -             | 0.4037          |\\n| 1.2378 | 10790 | -             | 0.4037          |\\n| 1.2390 | 10800 | 0.6921        | 0.4038          |\\n| 1.2401 | 10810 | -             | 0.4039          |\\n| 1.2413 | 10820 | -             | 0.4038          |\\n| 1.2424 | 10830 | -             | 0.4037          |\\n| 1.2435 | 10840 | -             | 0.4040          |\\n| 1.2447 | 10850 | -             | 0.4042          |\\n| 1.2458 | 10860 | -             | 0.4044          |\\n| 1.2470 | 10870 | -             | 0.4043          |\\n| 1.2481 | 10880 | -             | 0.4043          |\\n| 1.2493 | 10890 | -             | 0.4044          |\\n| 1.2504 | 10900 | 0.728         | 0.4042          |\\n| 1.2516 | 10910 | -             | 0.4044          |\\n| 1.2527 | 10920 | -             | 0.4043          |\\n| 1.2539 | 10930 | -             | 0.4039          |\\n| 1.2550 | 10940 | -             | 0.4038          |\\n| 1.2562 | 10950 | -             | 0.4037          |\\n| 1.2573 | 10960 | -             | 0.4035          |\\n| 1.2585 | 10970 | -             | 0.4032          |\\n| 1.2596 | 10980 | -             | 0.4024          |\\n| 1.2608 | 10990 | -             | 0.4019          |\\n| 1.2619 | 11000 | 0.713         | 0.4018          |\\n| 1.2630 | 11010 | -             | 0.4015          |\\n| 1.2642 | 11020 | -             | 0.4015          |\\n| 1.2653 | 11030 | -             | 0.4014          |\\n| 1.2665 | 11040 | -             | 0.4015          |\\n| 1.2676 | 11050 | -             | 0.4014          |\\n| 1.2688 | 11060 | -             | 0.4013          |\\n| 1.2699 | 11070 | -             | 0.4015          |\\n| 1.2711 | 11080 | -             | 0.4016          |\\n| 1.2722 | 11090 | -             | 0.4017          |\\n| 1.2734 | 11100 | 0.668         | 0.4017          |\\n| 1.2745 | 11110 | -             | 0.4016          |\\n| 1.2757 | 11120 | -             | 0.4016          |\\n| 1.2768 | 11130 | -             | 0.4019          |\\n| 1.2780 | 11140 | -             | 0.4021          |\\n| 1.2791 | 11150 | -             | 0.4019          |\\n| 1.2803 | 11160 | -             | 0.4017          |\\n| 1.2814 | 11170 | -             | 0.4017          |\\n| 1.2826 | 11180 | -             | 0.4018          |\\n| 1.2837 | 11190 | -             | 0.4013          |\\n| 1.2848 | 11200 | 0.7101        | 0.4011          |\\n| 1.2860 | 11210 | -             | 0.4011          |\\n| 1.2871 | 11220 | -             | 0.4014          |\\n| 1.2883 | 11230 | -             | 0.4015          |\\n| 1.2894 | 11240 | -             | 0.4010          |\\n| 1.2906 | 11250 | -             | 0.4012          |\\n| 1.2917 | 11260 | -             | 0.4013          |\\n| 1.2929 | 11270 | -             | 0.4010          |\\n| 1.2940 | 11280 | -             | 0.4006          |\\n| 1.2952 | 11290 | -             | 0.4005          |\\n| 1.2963 | 11300 | 0.6963        | 0.4004          |\\n| 1.2975 | 11310 | -             | 0.4003          |\\n| 1.2986 | 11320 | -             | 0.4004          |\\n| 1.2998 | 11330 | -             | 0.4003          |\\n| 1.3009 | 11340 | -             | 0.3999          |\\n| 1.3021 | 11350 | -             | 0.3997          |\\n| 1.3032 | 11360 | -             | 0.3996          |\\n| 1.3043 | 11370 | -             | 0.3997          |\\n| 1.3055 | 11380 | -             | 0.3996          |\\n| 1.3066 | 11390 | -             | 0.3994          |\\n| 1.3078 | 11400 | 0.6706        | 0.3993          |\\n| 1.3089 | 11410 | -             | 0.3991          |\\n| 1.3101 | 11420 | -             | 0.3990          |\\n| 1.3112 | 11430 | -             | 0.3990          |\\n| 1.3124 | 11440 | -             | 0.3987          |\\n| 1.3135 | 11450 | -             | 0.3981          |\\n| 1.3147 | 11460 | -             | 0.3978          |\\n| 1.3158 | 11470 | -             | 0.3975          |\\n| 1.3170 | 11480 | -             | 0.3974          |\\n| 1.3181 | 11490 | -             | 0.3974          |\\n| 1.3193 | 11500 | 0.6962        | 0.3974          |\\n| 1.3204 | 11510 | -             | 0.3975          |\\n| 1.3216 | 11520 | -             | 0.3975          |\\n| 1.3227 | 11530 | -             | 0.3976          |\\n| 1.3238 | 11540 | -             | 0.3977          |\\n| 1.3250 | 11550 | -             | 0.3975          |\\n| 1.3261 | 11560 | -             | 0.3974          |\\n| 1.3273 | 11570 | -             | 0.3973          |\\n| 1.3284 | 11580 | -             | 0.3971          |\\n| 1.3296 | 11590 | -             | 0.3969          |\\n| 1.3307 | 11600 | 0.7083        | 0.3970          |\\n| 1.3319 | 11610 | -             | 0.3970          |\\n| 1.3330 | 11620 | -             | 0.3971          |\\n| 1.3342 | 11630 | -             | 0.3973          |\\n| 1.3353 | 11640 | -             | 0.3975          |\\n| 1.3365 | 11650 | -             | 0.3973          |\\n| 1.3376 | 11660 | -             | 0.3973          |\\n| 1.3388 | 11670 | -             | 0.3973          |\\n| 1.3399 | 11680 | -             | 0.3976          |\\n| 1.3411 | 11690 | -             | 0.3976          |\\n| 1.3422 | 11700 | 0.6757        | 0.3976          |\\n| 1.3434 | 11710 | -             | 0.3975          |\\n| 1.3445 | 11720 | -             | 0.3973          |\\n| 1.3456 | 11730 | -             | 0.3971          |\\n| 1.3468 | 11740 | -             | 0.3963          |\\n| 1.3479 | 11750 | -             | 0.3964          |\\n| 1.3491 | 11760 | -             | 0.3965          |\\n| 1.3502 | 11770 | -             | 0.3967          |\\n| 1.3514 | 11780 | -             | 0.3966          |\\n| 1.3525 | 11790 | -             | 0.3964          |\\n| 1.3537 | 11800 | 0.7091        | 0.3965          |\\n| 1.3548 | 11810 | -             | 0.3964          |\\n| 1.3560 | 11820 | -             | 0.3964          |\\n| 1.3571 | 11830 | -             | 0.3963          |\\n| 1.3583 | 11840 | -             | 0.3962          |\\n| 1.3594 | 11850 | -             | 0.3961          |\\n| 1.3606 | 11860 | -             | 0.3956          |\\n| 1.3617 | 11870 | -             | 0.3956          |\\n| 1.3629 | 11880 | -             | 0.3961          |\\n| 1.3640 | 11890 | -             | 0.3963          |\\n| 1.3651 | 11900 | 0.6977        | 0.3962          |\\n| 1.3663 | 11910 | -             | 0.3958          |\\n| 1.3674 | 11920 | -             | 0.3960          |\\n| 1.3686 | 11930 | -             | 0.3963          |\\n| 1.3697 | 11940 | -             | 0.3964          |\\n| 1.3709 | 11950 | -             | 0.3961          |\\n| 1.3720 | 11960 | -             | 0.3960          |\\n| 1.3732 | 11970 | -             | 0.3958          |\\n| 1.3743 | 11980 | -             | 0.3954          |\\n| 1.3755 | 11990 | -             | 0.3948          |\\n| 1.3766 | 12000 | 0.7003        | 0.3944          |\\n| 1.3778 | 12010 | -             | 0.3940          |\\n| 1.3789 | 12020 | -             | 0.3940          |\\n| 1.3801 | 12030 | -             | 0.3938          |\\n| 1.3812 | 12040 | -             | 0.3939          |\\n| 1.3824 | 12050 | -             | 0.3943          |\\n| 1.3835 | 12060 | -             | 0.3946          |\\n| 1.3847 | 12070 | -             | 0.3947          |\\n| 1.3858 | 12080 | -             | 0.3943          |\\n| 1.3869 | 12090 | -             | 0.3940          |\\n| 1.3881 | 12100 | 0.679         | 0.3943          |\\n| 1.3892 | 12110 | -             | 0.3945          |\\n| 1.3904 | 12120 | -             | 0.3946          |\\n| 1.3915 | 12130 | -             | 0.3944          |\\n| 1.3927 | 12140 | -             | 0.3941          |\\n| 1.3938 | 12150 | -             | 0.3941          |\\n| 1.3950 | 12160 | -             | 0.3941          |\\n| 1.3961 | 12170 | -             | 0.3939          |\\n| 1.3973 | 12180 | -             | 0.3939          |\\n| 1.3984 | 12190 | -             | 0.3939          |\\n| 1.3996 | 12200 | 0.692         | 0.3938          |\\n| 1.4007 | 12210 | -             | 0.3937          |\\n| 1.4019 | 12220 | -             | 0.3932          |\\n| 1.4030 | 12230 | -             | 0.3928          |\\n| 1.4042 | 12240 | -             | 0.3925          |\\n| 1.4053 | 12250 | -             | 0.3922          |\\n| 1.4064 | 12260 | -             | 0.3924          |\\n| 1.4076 | 12270 | -             | 0.3923          |\\n| 1.4087 | 12280 | -             | 0.3926          |\\n| 1.4099 | 12290 | -             | 0.3924          |\\n| 1.4110 | 12300 | 0.6677        | 0.3925          |\\n| 1.4122 | 12310 | -             | 0.3926          |\\n| 1.4133 | 12320 | -             | 0.3927          |\\n| 1.4145 | 12330 | -             | 0.3928          |\\n| 1.4156 | 12340 | -             | 0.3928          |\\n| 1.4168 | 12350 | -             | 0.3929          |\\n| 1.4179 | 12360 | -             | 0.3933          |\\n| 1.4191 | 12370 | -             | 0.3934          |\\n| 1.4202 | 12380 | -             | 0.3933          |\\n| 1.4214 | 12390 | -             | 0.3933          |\\n| 1.4225 | 12400 | 0.6892        | 0.3930          |\\n| 1.4237 | 12410 | -             | 0.3928          |\\n| 1.4248 | 12420 | -             | 0.3928          |\\n| 1.4259 | 12430 | -             | 0.3927          |\\n| 1.4271 | 12440 | -             | 0.3927          |\\n| 1.4282 | 12450 | -             | 0.3924          |\\n| 1.4294 | 12460 | -             | 0.3924          |\\n| 1.4305 | 12470 | -             | 0.3922          |\\n| 1.4317 | 12480 | -             | 0.3920          |\\n| 1.4328 | 12490 | -             | 0.3919          |\\n| 1.4340 | 12500 | 0.7016        | 0.3917          |\\n| 1.4351 | 12510 | -             | 0.3914          |\\n| 1.4363 | 12520 | -             | 0.3912          |\\n| 1.4374 | 12530 | -             | 0.3914          |\\n| 1.4386 | 12540 | -             | 0.3915          |\\n| 1.4397 | 12550 | -             | 0.3915          |\\n| 1.4409 | 12560 | -             | 0.3915          |\\n| 1.4420 | 12570 | -             | 0.3912          |\\n| 1.4432 | 12580 | -             | 0.3910          |\\n| 1.4443 | 12590 | -             | 0.3910          |\\n| 1.4455 | 12600 | 0.68          | 0.3907          |\\n| 1.4466 | 12610 | -             | 0.3906          |\\n| 1.4477 | 12620 | -             | 0.3904          |\\n| 1.4489 | 12630 | -             | 0.3903          |\\n| 1.4500 | 12640 | -             | 0.3902          |\\n| 1.4512 | 12650 | -             | 0.3899          |\\n| 1.4523 | 12660 | -             | 0.3898          |\\n| 1.4535 | 12670 | -             | 0.3898          |\\n| 1.4546 | 12680 | -             | 0.3897          |\\n| 1.4558 | 12690 | -             | 0.3896          |\\n| 1.4569 | 12700 | 0.681         | 0.3894          |\\n| 1.4581 | 12710 | -             | 0.3892          |\\n| 1.4592 | 12720 | -             | 0.3892          |\\n| 1.4604 | 12730 | -             | 0.3893          |\\n| 1.4615 | 12740 | -             | 0.3896          |\\n| 1.4627 | 12750 | -             | 0.3898          |\\n| 1.4638 | 12760 | -             | 0.3900          |\\n| 1.4650 | 12770 | -             | 0.3899          |\\n| 1.4661 | 12780 | -             | 0.3898          |\\n| 1.4672 | 12790 | -             | 0.3899          |\\n| 1.4684 | 12800 | 0.6816        | 0.3901          |\\n| 1.4695 | 12810 | -             | 0.3901          |\\n| 1.4707 | 12820 | -             | 0.3901          |\\n| 1.4718 | 12830 | -             | 0.3898          |\\n| 1.4730 | 12840 | -             | 0.3897          |\\n| 1.4741 | 12850 | -             | 0.3897          |\\n| 1.4753 | 12860 | -             | 0.3895          |\\n| 1.4764 | 12870 | -             | 0.3896          |\\n| 1.4776 | 12880 | -             | 0.3895          |\\n| 1.4787 | 12890 | -             | 0.3896          |\\n| 1.4799 | 12900 | 0.6635        | 0.3897          |\\n| 1.4810 | 12910 | -             | 0.3897          |\\n| 1.4822 | 12920 | -             | 0.3899          |\\n| 1.4833 | 12930 | -             | 0.3900          |\\n| 1.4845 | 12940 | -             | 0.3897          |\\n| 1.4856 | 12950 | -             | 0.3898          |\\n| 1.4868 | 12960 | -             | 0.3899          |\\n| 1.4879 | 12970 | -             | 0.3898          |\\n| 1.4890 | 12980 | -             | 0.3898          |\\n| 1.4902 | 12990 | -             | 0.3894          |\\n| 1.4913 | 13000 | 0.6698        | 0.3892          |\\n| 1.4925 | 13010 | -             | 0.3892          |\\n| 1.4936 | 13020 | -             | 0.3894          |\\n| 1.4948 | 13030 | -             | 0.3893          |\\n| 1.4959 | 13040 | -             | 0.3894          |\\n| 1.4971 | 13050 | -             | 0.3893          |\\n| 1.4982 | 13060 | -             | 0.3894          |\\n| 1.4994 | 13070 | -             | 0.3893          |\\n| 1.5005 | 13080 | -             | 0.3895          |\\n| 1.5017 | 13090 | -             | 0.3895          |\\n| 1.5028 | 13100 | 0.6757        | 0.3898          |\\n| 1.5040 | 13110 | -             | 0.3898          |\\n| 1.5051 | 13120 | -             | 0.3897          |\\n| 1.5063 | 13130 | -             | 0.3897          |\\n| 1.5074 | 13140 | -             | 0.3897          |\\n| 1.5085 | 13150 | -             | 0.3899          |\\n| 1.5097 | 13160 | -             | 0.3901          |\\n| 1.5108 | 13170 | -             | 0.3901          |\\n| 1.5120 | 13180 | -             | 0.3903          |\\n| 1.5131 | 13190 | -             | 0.3901          |\\n| 1.5143 | 13200 | 0.6483        | 0.3901          |\\n| 1.5154 | 13210 | -             | 0.3904          |\\n| 1.5166 | 13220 | -             | 0.3904          |\\n| 1.5177 | 13230 | -             | 0.3903          |\\n| 1.5189 | 13240 | -             | 0.3900          |\\n| 1.5200 | 13250 | -             | 0.3898          |\\n| 1.5212 | 13260 | -             | 0.3894          |\\n| 1.5223 | 13270 | -             | 0.3892          |\\n| 1.5235 | 13280 | -             | 0.3891          |\\n| 1.5246 | 13290 | -             | 0.3890          |\\n| 1.5258 | 13300 | 0.686         | 0.3892          |\\n| 1.5269 | 13310 | -             | 0.3892          |\\n| 1.5280 | 13320 | -             | 0.3892          |\\n| 1.5292 | 13330 | -             | 0.3891          |\\n| 1.5303 | 13340 | -             | 0.3890          |\\n| 1.5315 | 13350 | -             | 0.3894          |\\n| 1.5326 | 13360 | -             | 0.3895          |\\n| 1.5338 | 13370 | -             | 0.3895          |\\n| 1.5349 | 13380 | -             | 0.3894          |\\n| 1.5361 | 13390 | -             | 0.3895          |\\n| 1.5372 | 13400 | 0.6901        | 0.3896          |\\n| 1.5384 | 13410 | -             | 0.3895          |\\n| 1.5395 | 13420 | -             | 0.3891          |\\n| 1.5407 | 13430 | -             | 0.3891          |\\n| 1.5418 | 13440 | -             | 0.3890          |\\n| 1.5430 | 13450 | -             | 0.3889          |\\n| 1.5441 | 13460 | -             | 0.3887          |\\n| 1.5453 | 13470 | -             | 0.3885          |\\n| 1.5464 | 13480 | -             | 0.3885          |\\n| 1.5476 | 13490 | -             | 0.3886          |\\n| 1.5487 | 13500 | 0.6568        | 0.3887          |\\n| 1.5498 | 13510 | -             | 0.3884          |\\n| 1.5510 | 13520 | -             | 0.3879          |\\n| 1.5521 | 13530 | -             | 0.3874          |\\n| 1.5533 | 13540 | -             | 0.3870          |\\n| 1.5544 | 13550 | -             | 0.3868          |\\n| 1.5556 | 13560 | -             | 0.3869          |\\n| 1.5567 | 13570 | -             | 0.3872          |\\n| 1.5579 | 13580 | -             | 0.3873          |\\n| 1.5590 | 13590 | -             | 0.3874          |\\n| 1.5602 | 13600 | 0.6665        | 0.3875          |\\n| 1.5613 | 13610 | -             | 0.3876          |\\n| 1.5625 | 13620 | -             | 0.3875          |\\n| 1.5636 | 13630 | -             | 0.3872          |\\n| 1.5648 | 13640 | -             | 0.3873          |\\n| 1.5659 | 13650 | -             | 0.3872          |\\n| 1.5671 | 13660 | -             | 0.3869          |\\n| 1.5682 | 13670 | -             | 0.3867          |\\n| 1.5693 | 13680 | -             | 0.3864          |\\n| 1.5705 | 13690 | -             | 0.3861          |\\n| 1.5716 | 13700 | 0.6795        | 0.3860          |\\n| 1.5728 | 13710 | -             | 0.3858          |\\n| 1.5739 | 13720 | -             | 0.3858          |\\n| 1.5751 | 13730 | -             | 0.3854          |\\n| 1.5762 | 13740 | -             | 0.3851          |\\n| 1.5774 | 13750 | -             | 0.3850          |\\n| 1.5785 | 13760 | -             | 0.3849          |\\n| 1.5797 | 13770 | -             | 0.3849          |\\n| 1.5808 | 13780 | -             | 0.3849          |\\n| 1.5820 | 13790 | -             | 0.3848          |\\n| 1.5831 | 13800 | 0.6894        | 0.3848          |\\n| 1.5843 | 13810 | -             | 0.3846          |\\n| 1.5854 | 13820 | -             | 0.3845          |\\n| 1.5866 | 13830 | -             | 0.3847          |\\n| 1.5877 | 13840 | -             | 0.3848          |\\n| 1.5888 | 13850 | -             | 0.3849          |\\n| 1.5900 | 13860 | -             | 0.3848          |\\n| 1.5911 | 13870 | -             | 0.3846          |\\n| 1.5923 | 13880 | -             | 0.3845          |\\n| 1.5934 | 13890 | -             | 0.3844          |\\n| 1.5946 | 13900 | 0.6483        | 0.3845          |\\n| 1.5957 | 13910 | -             | 0.3841          |\\n| 1.5969 | 13920 | -             | 0.3841          |\\n| 1.5980 | 13930 | -             | 0.3842          |\\n| 1.5992 | 13940 | -             | 0.3842          |\\n| 1.6003 | 13950 | -             | 0.3843          |\\n| 1.6015 | 13960 | -             | 0.3843          |\\n| 1.6026 | 13970 | -             | 0.3843          |\\n| 1.6038 | 13980 | -             | 0.3844          |\\n| 1.6049 | 13990 | -             | 0.3845          |\\n| 1.6061 | 14000 | 0.6856        | 0.3846          |\\n| 1.6072 | 14010 | -             | 0.3845          |\\n| 1.6084 | 14020 | -             | 0.3846          |\\n| 1.6095 | 14030 | -             | 0.3845          |\\n| 1.6106 | 14040 | -             | 0.3843          |\\n| 1.6118 | 14050 | -             | 0.3842          |\\n| 1.6129 | 14060 | -             | 0.3841          |\\n| 1.6141 | 14070 | -             | 0.3842          |\\n| 1.6152 | 14080 | -             | 0.3843          |\\n| 1.6164 | 14090 | -             | 0.3845          |\\n| 1.6175 | 14100 | 0.6797        | 0.3845          |\\n| 1.6187 | 14110 | -             | 0.3845          |\\n| 1.6198 | 14120 | -             | 0.3844          |\\n| 1.6210 | 14130 | -             | 0.3842          |\\n| 1.6221 | 14140 | -             | 0.3841          |\\n| 1.6233 | 14150 | -             | 0.3838          |\\n| 1.6244 | 14160 | -             | 0.3836          |\\n| 1.6256 | 14170 | -             | 0.3835          |\\n| 1.6267 | 14180 | -             | 0.3834          |\\n| 1.6279 | 14190 | -             | 0.3831          |\\n| 1.6290 | 14200 | 0.7057        | 0.3828          |\\n| 1.6301 | 14210 | -             | 0.3825          |\\n| 1.6313 | 14220 | -             | 0.3822          |\\n| 1.6324 | 14230 | -             | 0.3821          |\\n| 1.6336 | 14240 | -             | 0.3820          |\\n| 1.6347 | 14250 | -             | 0.3822          |\\n| 1.6359 | 14260 | -             | 0.3822          |\\n| 1.6370 | 14270 | -             | 0.3822          |\\n| 1.6382 | 14280 | -             | 0.3821          |\\n| 1.6393 | 14290 | -             | 0.3822          |\\n| 1.6405 | 14300 | 0.6699        | 0.3827          |\\n| 1.6416 | 14310 | -             | 0.3828          |\\n| 1.6428 | 14320 | -             | 0.3827          |\\n| 1.6439 | 14330 | -             | 0.3823          |\\n| 1.6451 | 14340 | -             | 0.3822          |\\n| 1.6462 | 14350 | -             | 0.3824          |\\n| 1.6474 | 14360 | -             | 0.3826          |\\n| 1.6485 | 14370 | -             | 0.3826          |\\n| 1.6497 | 14380 | -             | 0.3827          |\\n| 1.6508 | 14390 | -             | 0.3827          |\\n| 1.6519 | 14400 | 0.6615        | 0.3827          |\\n| 1.6531 | 14410 | -             | 0.3828          |\\n| 1.6542 | 14420 | -             | 0.3826          |\\n| 1.6554 | 14430 | -             | 0.3825          |\\n| 1.6565 | 14440 | -             | 0.3826          |\\n| 1.6577 | 14450 | -             | 0.3830          |\\n| 1.6588 | 14460 | -             | 0.3830          |\\n| 1.6600 | 14470 | -             | 0.3830          |\\n| 1.6611 | 14480 | -             | 0.3830          |\\n| 1.6623 | 14490 | -             | 0.3830          |\\n| 1.6634 | 14500 | 0.6628        | 0.3829          |\\n| 1.6646 | 14510 | -             | 0.3829          |\\n| 1.6657 | 14520 | -             | 0.3829          |\\n| 1.6669 | 14530 | -             | 0.3829          |\\n| 1.6680 | 14540 | -             | 0.3829          |\\n| 1.6692 | 14550 | -             | 0.3829          |\\n| 1.6703 | 14560 | -             | 0.3830          |\\n| 1.6714 | 14570 | -             | 0.3828          |\\n| 1.6726 | 14580 | -             | 0.3825          |\\n| 1.6737 | 14590 | -             | 0.3822          |\\n| 1.6749 | 14600 | 0.6728        | 0.3819          |\\n| 1.6760 | 14610 | -             | 0.3817          |\\n| 1.6772 | 14620 | -             | 0.3817          |\\n| 1.6783 | 14630 | -             | 0.3815          |\\n| 1.6795 | 14640 | -             | 0.3813          |\\n| 1.6806 | 14650 | -             | 0.3815          |\\n| 1.6818 | 14660 | -             | 0.3814          |\\n| 1.6829 | 14670 | -             | 0.3814          |\\n| 1.6841 | 14680 | -             | 0.3812          |\\n| 1.6852 | 14690 | -             | 0.3809          |\\n| 1.6864 | 14700 | 0.6852        | 0.3808          |\\n| 1.6875 | 14710 | -             | 0.3807          |\\n| 1.6887 | 14720 | -             | 0.3804          |\\n| 1.6898 | 14730 | -             | 0.3802          |\\n| 1.6909 | 14740 | -             | 0.3799          |\\n| 1.6921 | 14750 | -             | 0.3798          |\\n| 1.6932 | 14760 | -             | 0.3797          |\\n| 1.6944 | 14770 | -             | 0.3795          |\\n| 1.6955 | 14780 | -             | 0.3797          |\\n| 1.6967 | 14790 | -             | 0.3797          |\\n| 1.6978 | 14800 | 0.6585        | 0.3797          |\\n| 1.6990 | 14810 | -             | 0.3797          |\\n| 1.7001 | 14820 | -             | 0.3798          |\\n| 1.7013 | 14830 | -             | 0.3800          |\\n| 1.7024 | 14840 | -             | 0.3800          |\\n| 1.7036 | 14850 | -             | 0.3798          |\\n| 1.7047 | 14860 | -             | 0.3799          |\\n| 1.7059 | 14870 | -             | 0.3799          |\\n| 1.7070 | 14880 | -             | 0.3798          |\\n| 1.7082 | 14890 | -             | 0.3801          |\\n| 1.7093 | 14900 | 0.6691        | 0.3801          |\\n| 1.7105 | 14910 | -             | 0.3800          |\\n| 1.7116 | 14920 | -             | 0.3798          |\\n| 1.7127 | 14930 | -             | 0.3795          |\\n| 1.7139 | 14940 | -             | 0.3792          |\\n| 1.7150 | 14950 | -             | 0.3791          |\\n| 1.7162 | 14960 | -             | 0.3790          |\\n| 1.7173 | 14970 | -             | 0.3790          |\\n| 1.7185 | 14980 | -             | 0.3793          |\\n| 1.7196 | 14990 | -             | 0.3794          |\\n| 1.7208 | 15000 | 0.6676        | 0.3794          |\\n| 1.7219 | 15010 | -             | 0.3794          |\\n| 1.7231 | 15020 | -             | 0.3794          |\\n| 1.7242 | 15030 | -             | 0.3793          |\\n| 1.7254 | 15040 | -             | 0.3791          |\\n| 1.7265 | 15050 | -             | 0.3790          |\\n| 1.7277 | 15060 | -             | 0.3788          |\\n| 1.7288 | 15070 | -             | 0.3787          |\\n| 1.7300 | 15080 | -             | 0.3787          |\\n| 1.7311 | 15090 | -             | 0.3787          |\\n| 1.7322 | 15100 | 0.6945        | 0.3785          |\\n| 1.7334 | 15110 | -             | 0.3782          |\\n| 1.7345 | 15120 | -             | 0.3781          |\\n| 1.7357 | 15130 | -             | 0.3780          |\\n| 1.7368 | 15140 | -             | 0.3782          |\\n| 1.7380 | 15150 | -             | 0.3782          |\\n| 1.7391 | 15160 | -             | 0.3781          |\\n| 1.7403 | 15170 | -             | 0.3781          |\\n| 1.7414 | 15180 | -             | 0.3781          |\\n| 1.7426 | 15190 | -             | 0.3784          |\\n| 1.7437 | 15200 | 0.6697        | 0.3787          |\\n| 1.7449 | 15210 | -             | 0.3790          |\\n| 1.7460 | 15220 | -             | 0.3792          |\\n| 1.7472 | 15230 | -             | 0.3792          |\\n| 1.7483 | 15240 | -             | 0.3791          |\\n| 1.7495 | 15250 | -             | 0.3791          |\\n| 1.7506 | 15260 | -             | 0.3788          |\\n| 1.7517 | 15270 | -             | 0.3789          |\\n| 1.7529 | 15280 | -             | 0.3788          |\\n| 1.7540 | 15290 | -             | 0.3788          |\\n| 1.7552 | 15300 | 0.6557        | 0.3784          |\\n| 1.7563 | 15310 | -             | 0.3784          |\\n| 1.7575 | 15320 | -             | 0.3784          |\\n| 1.7586 | 15330 | -             | 0.3785          |\\n| 1.7598 | 15340 | -             | 0.3789          |\\n| 1.7609 | 15350 | -             | 0.3791          |\\n| 1.7621 | 15360 | -             | 0.3791          |\\n| 1.7632 | 15370 | -             | 0.3791          |\\n| 1.7644 | 15380 | -             | 0.3789          |\\n| 1.7655 | 15390 | -             | 0.3788          |\\n| 1.7667 | 15400 | 0.6837        | 0.3788          |\\n| 1.7678 | 15410 | -             | 0.3788          |\\n| 1.7690 | 15420 | -             | 0.3788          |\\n| 1.7701 | 15430 | -             | 0.3787          |\\n| 1.7713 | 15440 | -             | 0.3786          |\\n| 1.7724 | 15450 | -             | 0.3785          |\\n| 1.7735 | 15460 | -             | 0.3784          |\\n| 1.7747 | 15470 | -             | 0.3780          |\\n| 1.7758 | 15480 | -             | 0.3778          |\\n| 1.7770 | 15490 | -             | 0.3778          |\\n| 1.7781 | 15500 | 0.6685        | 0.3779          |\\n| 1.7793 | 15510 | -             | 0.3781          |\\n| 1.7804 | 15520 | -             | 0.3783          |\\n| 1.7816 | 15530 | -             | 0.3784          |\\n| 1.7827 | 15540 | -             | 0.3782          |\\n| 1.7839 | 15550 | -             | 0.3779          |\\n| 1.7850 | 15560 | -             | 0.3779          |\\n| 1.7862 | 15570 | -             | 0.3782          |\\n| 1.7873 | 15580 | -             | 0.3786          |\\n| 1.7885 | 15590 | -             | 0.3785          |\\n| 1.7896 | 15600 | 0.6521        | 0.3783          |\\n| 1.7908 | 15610 | -             | 0.3785          |\\n| 1.7919 | 15620 | -             | 0.3783          |\\n| 1.7930 | 15630 | -             | 0.3783          |\\n| 1.7942 | 15640 | -             | 0.3784          |\\n| 1.7953 | 15650 | -             | 0.3783          |\\n| 1.7965 | 15660 | -             | 0.3782          |\\n| 1.7976 | 15670 | -             | 0.3780          |\\n| 1.7988 | 15680 | -             | 0.3779          |\\n| 1.7999 | 15690 | -             | 0.3779          |\\n| 1.8011 | 15700 | 0.649         | 0.3779          |\\n| 1.8022 | 15710 | -             | 0.3781          |\\n| 1.8034 | 15720 | -             | 0.3781          |\\n| 1.8045 | 15730 | -             | 0.3782          |\\n| 1.8057 | 15740 | -             | 0.3780          |\\n| 1.8068 | 15750 | -             | 0.3780          |\\n| 1.8080 | 15760 | -             | 0.3780          |\\n| 1.8091 | 15770 | -             | 0.3780          |\\n| 1.8103 | 15780 | -             | 0.3780          |\\n| 1.8114 | 15790 | -             | 0.3781          |\\n| 1.8126 | 15800 | 0.6673        | 0.3783          |\\n| 1.8137 | 15810 | -             | 0.3781          |\\n| 1.8148 | 15820 | -             | 0.3781          |\\n| 1.8160 | 15830 | -             | 0.3785          |\\n| 1.8171 | 15840 | -             | 0.3788          |\\n| 1.8183 | 15850 | -             | 0.3789          |\\n| 1.8194 | 15860 | -             | 0.3788          |\\n| 1.8206 | 15870 | -             | 0.3786          |\\n| 1.8217 | 15880 | -             | 0.3783          |\\n| 1.8229 | 15890 | -             | 0.3782          |\\n| 1.8240 | 15900 | 0.6902        | 0.3783          |\\n| 1.8252 | 15910 | -             | 0.3781          |\\n| 1.8263 | 15920 | -             | 0.3779          |\\n| 1.8275 | 15930 | -             | 0.3777          |\\n| 1.8286 | 15940 | -             | 0.3778          |\\n| 1.8298 | 15950 | -             | 0.3780          |\\n| 1.8309 | 15960 | -             | 0.3780          |\\n| 1.8321 | 15970 | -             | 0.3781          |\\n| 1.8332 | 15980 | -             | 0.3780          |\\n| 1.8343 | 15990 | -             | 0.3779          |\\n| 1.8355 | 16000 | 0.6568        | 0.3778          |\\n\\n</details>\\n\\n### Framework Versions\\n- Python: 3.12.8\\n- Sentence Transformers: 3.4.1\\n- Transformers: 4.49.0\\n- PyTorch: 2.2.0+cu121\\n- Accelerate: 1.4.0\\n- Datasets: 3.3.2\\n- Tokenizers: 0.21.0\\n\\n## Citation\\n\\n### BibTeX\\n\\n#### Sentence Transformers\\n```bibtex\\n@inproceedings{reimers-2019-sentence-bert,\\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\\n    author = \"Reimers, Nils and Gurevych, Iryna\",\\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\\n    month = \"11\",\\n    year = \"2019\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://arxiv.org/abs/1908.10084\",\\n}\\n```\\n\\n#### MultipleNegativesRankingLoss\\n```bibtex\\n@misc{henderson2017efficient,\\n    title={Efficient Natural Language Response Suggestion for Smart Reply},\\n    author={Matthew Henderson and Rami Al-Rfou and Brian Strope and Yun-hsuan Sung and Laszlo Lukacs and Ruiqi Guo and Sanjiv Kumar and Balint Miklos and Ray Kurzweil},\\n    year={2017},\\n    eprint={1705.00652},\\n    archivePrefix={arXiv},\\n    primaryClass={cs.CL}\\n}\\n```\\n\\n<!--\\n## Glossary\\n\\n*Clearly define terms in order to be accessible across audiences.*\\n-->\\n\\n<!--\\n## Model Card Authors\\n\\n*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*\\n-->\\n\\n<!--\\n## Model Card Contact\\n\\n*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*\\n-->',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c273fcad08a0d12ccaaf',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.997000',\n",
       "  'date_modified': '2025-02-27T19:18:13.997000'},\n",
       " {'model_identifier': 'jssky/53b68aff-86bf-4e20-b0ba-8c675ee04f16',\n",
       "  'version': 11,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T15:59:12',\n",
       "  'last_modified': '2025-02-27T17:10:43',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'qwen2', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c272fcad08a0d12ccaae',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.996000',\n",
       "  'date_modified': '2025-02-27T19:18:13.996000'},\n",
       " {'model_identifier': 'PrunaAI/hfl-chinese-alpaca-2-7b-HQQ-4bit-smashed',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:05:06',\n",
       "  'last_modified': '2025-02-27T17:11:02',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['llama', 'pruna-ai', 'hqq', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\\nbase_model: ORIGINAL_REPO_NAME\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\n---\\n<!-- header start -->\\n<!-- 200823 -->\\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\\n    </a>\\n</div>\\n<!-- header end -->\\n\\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\\n\\n# Simply make AI models cheaper, smaller, faster, and greener!\\n\\n- Give a thumbs up if you like this model!\\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\\n\\n## Results\\n\\n![image info](./plots.png)\\n\\n**Frequently Asked Questions**\\n- ***How does the compression work?*** The model is compressed with hqq.\\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\\n- ***How is the model efficiency evaluated?*** These results were obtained with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\\n- ***What is the model format?*** We use safetensors.\\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\\n\\n## Setup\\n\\nYou can run the smashed model with these steps:\\n\\n0. Check requirements from the original repo ORIGINAL_REPO_NAME installed. In particular, check python, cuda, and transformers versions.\\n1. Make sure that you have installed quantization related packages.\\n    ```bash\\n    pip install hqq\\n    ```\\n2. Load & run the model.\\n    ```python \\n   from transformers import AutoModelForCausalLM, AutoTokenizer\\n    from hqq.engine.hf import HQQModelForCausalLM\\n from hqq.models.hf.base import AutoHQQHFModel\\n\\n   try:\\n     model = HQQModelForCausalLM.from_quantized(\"PrunaAI/hfl-chinese-alpaca-2-7b-HQQ-4bit-smashed\", device_map=\\'auto\\')\\n    except: \\n     model = AutoHQQHFModel.from_quantized(\"PrunaAI/hfl-chinese-alpaca-2-7b-HQQ-4bit-smashed\")\\n   tokenizer = AutoTokenizer.from_pretrained(\"ORIGINAL_REPO_NAME\")\\n    \\n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors=\\'pt\\').to(model.device)[\"input_ids\"]\\n    \\n   outputs = model.generate(input_ids, max_new_tokens=216)\\n   tokenizer.decode(outputs[0])\\n    ```\\n\\n## Configurations\\n\\nThe configuration info are in `smash_config.json`.\\n\\n## Credits & License\\n\\nThe license of the smashed model follows the license of the original model. Please check the license of the original model ORIGINAL_REPO_NAME before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\\n\\n## Want to compress other models?\\n\\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c26ffcad08a0d12ccaad',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.995000',\n",
       "  'date_modified': '2025-02-27T19:18:13.995000'},\n",
       " {'model_identifier': 'mradermacher/Code-Mistral-7B-GGUF',\n",
       "  'version': 20,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2024-11-13T06:22:53',\n",
       "  'last_modified': '2025-02-27T17:11:05',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': True,\n",
       "  'tags': ['transformers',\n",
       "   'gguf',\n",
       "   'code',\n",
       "   'mathematics',\n",
       "   'en',\n",
       "   'dataset:ajibawa-2023/Code-290k-ShareGPT',\n",
       "   'dataset:m-a-p/Code-Feedback',\n",
       "   'dataset:microsoft/orca-math-word-problems-200k',\n",
       "   'dataset:teknium/openhermes',\n",
       "   'base_model:ajibawa-2023/Code-Mistral-7B',\n",
       "   'base_model:quantized:ajibawa-2023/Code-Mistral-7B',\n",
       "   'license:apache-2.0',\n",
       "   'endpoints_compatible',\n",
       "   'region:us',\n",
       "   'conversational'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 6.744388580322266},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': \"---\\nbase_model: ajibawa-2023/Code-Mistral-7B\\ndatasets:\\n- ajibawa-2023/Code-290k-ShareGPT\\n- m-a-p/Code-Feedback\\n- microsoft/orca-math-word-problems-200k\\n- teknium/openhermes\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\nquantized_by: mradermacher\\ntags:\\n- code\\n- mathematics\\n---\\n## About\\n\\n<!-- ### quantize_version: 2 -->\\n<!-- ### output_tensor_quantised: 1 -->\\n<!-- ### convert_type: hf -->\\n<!-- ### vocab_type:  -->\\n<!-- ### tags:  -->\\nstatic quants of https://huggingface.co/ajibawa-2023/Code-Mistral-7B\\n\\n<!-- provided-files -->\\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/Code-Mistral-7B-i1-GGUF\\n## Usage\\n\\nIf you are unsure how to use GGUF files, refer to one of [TheBloke's\\nREADMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for\\nmore details, including on how to concatenate multi-part files.\\n\\n## Provided Quants\\n\\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\\n\\n| Link | Type | Size/GB | Notes |\\n|:-----|:-----|--------:|:------|\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q2_K.gguf) | Q2_K | 2.8 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q3_K_S.gguf) | Q3_K_S | 3.3 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q3_K_M.gguf) | Q3_K_M | 3.6 | lower quality |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q3_K_L.gguf) | Q3_K_L | 3.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.IQ4_XS.gguf) | IQ4_XS | 4.0 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q4_0_4_4.gguf) | Q4_0_4_4 | 4.2 | fast on arm, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q4_K_S.gguf) | Q4_K_S | 4.2 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q4_K_M.gguf) | Q4_K_M | 4.5 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q5_K_S.gguf) | Q5_K_S | 5.1 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q5_K_M.gguf) | Q5_K_M | 5.2 |  |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q6_K.gguf) | Q6_K | 6.0 | very good quality |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.Q8_0.gguf) | Q8_0 | 7.8 | fast, best quality |\\n| [GGUF](https://huggingface.co/mradermacher/Code-Mistral-7B-GGUF/resolve/main/Code-Mistral-7B.f16.gguf) | f16 | 14.6 | 16 bpw, overkill |\\n\\nHere is a handy graph by ikawrakow comparing some lower-quality quant\\ntypes (lower is better):\\n\\n![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)\\n\\nAnd here are Artefact2's thoughts on the matter:\\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\\n\\n## FAQ / Model Request\\n\\nSee https://huggingface.co/mradermacher/model_requests for some answers to\\nquestions you might have and/or if you want some other model quantized.\\n\\n## Thanks\\n\\nI thank my company, [nethype GmbH](https://www.nethype.de/), for letting\\nme use its servers and providing upgrades to my workstation to enable\\nthis work in my free time.\\n\\n<!-- end -->\\n\",\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 82.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c26efcad08a0d12ccaac',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.887000',\n",
       "  'date_modified': '2025-02-27T19:18:13.887000'},\n",
       " {'model_identifier': 'Supichi/BBAI_525_Tsu_gZ_Xia0',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:06:55',\n",
       "  'last_modified': '2025-02-27T17:11:37',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'mergekit',\n",
       "   'merge',\n",
       "   'conversational',\n",
       "   'base_model:Supichi/BBAI_250_Xia0_gZ',\n",
       "   'base_model:merge:Supichi/BBAI_250_Xia0_gZ',\n",
       "   'base_model:Supichi/BBAI_275_Tsunami_gZ',\n",
       "   'base_model:merge:Supichi/BBAI_275_Tsunami_gZ',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 14.18519115447998},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nbase_model:\\n- Supichi/BBAI_275_Tsunami_gZ\\n- Supichi/BBAI_250_Xia0_gZ\\nlibrary_name: transformers\\ntags:\\n- mergekit\\n- merge\\n\\n---\\n# merge\\n\\nThis is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).\\n\\n## Merge Details\\n### Merge Method\\n\\nThis model was merged using the [SLERP](https://en.wikipedia.org/wiki/Slerp) merge method.\\n\\n### Models Merged\\n\\nThe following models were included in the merge:\\n* [Supichi/BBAI_275_Tsunami_gZ](https://huggingface.co/Supichi/BBAI_275_Tsunami_gZ)\\n* [Supichi/BBAI_250_Xia0_gZ](https://huggingface.co/Supichi/BBAI_250_Xia0_gZ)\\n\\n### Configuration\\n\\nThe following YAML configuration was used to produce this model:\\n\\n```yaml\\nslices:\\n- sources:\\n  - model: Supichi/BBAI_275_Tsunami_gZ\\n    layer_range:\\n    - 0\\n    - 28\\n  - model: Supichi/BBAI_250_Xia0_gZ\\n    layer_range:\\n    - 0\\n    - 28\\nmerge_method: slerp\\nbase_model: Supichi/BBAI_275_Tsunami_gZ\\nparameters:\\n  t:\\n  - filter: self_attn\\n    value:\\n    - 0\\n    - 0.5\\n    - 0.3\\n    - 0.7\\n    - 1\\n  - filter: mlp\\n    value:\\n    - 1\\n    - 0.5\\n    - 0.7\\n    - 0.3\\n    - 0\\n  - value: 0.5\\ndtype: bfloat16\\n```\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c26dfcad08a0d12ccaab',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.887000',\n",
       "  'date_modified': '2025-02-27T19:18:13.887000'},\n",
       " {'model_identifier': 'leixa/82ca2c9f-699c-4c56-a0c2-b864fda13e6a',\n",
       "  'version': 18,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T16:40:26',\n",
       "  'last_modified': '2025-02-27T17:12:13',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['peft',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'axolotl',\n",
       "   'generated_from_trainer',\n",
       "   'base_model:Qwen/Qwen2.5-1.5B-Instruct',\n",
       "   'base_model:adapter:Qwen/Qwen2.5-1.5B-Instruct',\n",
       "   'license:apache-2.0',\n",
       "   'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlibrary_name: peft\\nlicense: apache-2.0\\nbase_model: Qwen/Qwen2.5-1.5B-Instruct\\ntags:\\n- axolotl\\n- generated_from_trainer\\nmodel-index:\\n- name: 82ca2c9f-699c-4c56-a0c2-b864fda13e6a\\n  results: []\\n---\\n\\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n[<img src=\"https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/axolotl-ai-cloud/axolotl)\\n<details><summary>See axolotl config</summary>\\n\\naxolotl version: `0.4.1`\\n```yaml\\nadapter: lora\\nbase_model: Qwen/Qwen2.5-1.5B-Instruct\\nbf16: auto\\nchat_template: llama3\\ndataset_prepared_path: null\\ndatasets:\\n- data_files:\\n  - 70885662a4657c52_train_data.json\\n  ds_type: json\\n  format: custom\\n  path: /workspace/input_data/70885662a4657c52_train_data.json\\n  type:\\n    field_input: instruction\\n    field_instruction: input\\n    field_output: output\\n    format: \\'{instruction} {input}\\'\\n    no_input_format: \\'{instruction}\\'\\n    system_format: \\'{system}\\'\\n    system_prompt: \\'\\'\\nddp_timeout: 1800\\ndebug: null\\ndeepspeed: null\\ndevice_map: auto\\ndo_eval: true\\nearly_stopping_patience: 3\\neval_batch_size: 4\\neval_max_new_tokens: 128\\neval_steps: 200\\neval_table_size: null\\nevals_per_epoch: null\\nflash_attention: true\\nfp16: false\\nfsdp: null\\nfsdp_config: null\\ngradient_accumulation_steps: 4\\ngradient_checkpointing: true\\ngradient_checkpointing_kwargs:\\n  use_reentrant: true\\ngroup_by_length: true\\nhub_model_id: leixa/82ca2c9f-699c-4c56-a0c2-b864fda13e6a\\nhub_repo: null\\nhub_strategy: checkpoint\\nhub_token: null\\nlearning_rate: 0.0002\\nload_in_4bit: false\\nload_in_8bit: false\\nlocal_rank: 0\\nlogging_steps: 10\\nlora_alpha: 32\\nlora_dropout: 0.06\\nlora_fan_in_fan_out: null\\nlora_model_dir: null\\nlora_r: 16\\nlora_target_linear: true\\nlr_scheduler: constant\\nmax_grad_norm: 1.0\\nmax_memory:\\n  0: 75GB\\nmax_steps: 1600\\nmicro_batch_size: 4\\nmlflow_experiment_name: /tmp/70885662a4657c52_train_data.json\\nmodel_type: AutoModelForCausalLM\\nnum_epochs: 10\\noptim_args:\\n  adam_beta1: 0.9\\n  adam_beta2: 0.999\\n  adam_epsilon: 1e-08\\noptimizer: adamw_bnb_8bit\\noutput_dir: miner_id_24\\npad_to_sequence_len: true\\nrelora_prune_ratio: 0.9\\nresume_from_checkpoint: null\\ns2_attention: null\\nsample_packing: false\\nsave_steps: 200\\nsaves_per_epoch: null\\nsequence_len: 512\\nstrict: false\\ntf32: true\\ntokenizer_type: AutoTokenizer\\ntrain_on_inputs: false\\ntrust_remote_code: true\\nval_set_size: 0.05\\nwandb_entity: acopia-grant\\nwandb_mode: online\\nwandb_name: 9c9c1696-fddf-412a-a23d-9af232ad64b9\\nwandb_project: Gradients-On-112\\nwandb_run: your_name\\nwandb_runid: 9c9c1696-fddf-412a-a23d-9af232ad64b9\\nwarmup_steps: 50\\nweight_decay: 0.0\\nxformers_attention: null\\n\\n```\\n\\n</details><br>\\n\\n# 82ca2c9f-699c-4c56-a0c2-b864fda13e6a\\n\\nThis model is a fine-tuned version of [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) on the None dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: 1.4005\\n\\n## Model description\\n\\nMore information needed\\n\\n## Intended uses & limitations\\n\\nMore information needed\\n\\n## Training and evaluation data\\n\\nMore information needed\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 0.0002\\n- train_batch_size: 4\\n- eval_batch_size: 4\\n- seed: 42\\n- gradient_accumulation_steps: 4\\n- total_train_batch_size: 16\\n- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08\\n- lr_scheduler_type: constant\\n- lr_scheduler_warmup_steps: 50\\n- training_steps: 1600\\n\\n### Training results\\n\\n| Training Loss | Epoch  | Step | Validation Loss |\\n|:-------------:|:------:|:----:|:---------------:|\\n| No log        | 0.0006 | 1    | 2.1382          |\\n| 1.6343        | 0.1293 | 200  | 1.5492          |\\n| 1.6866        | 0.2585 | 400  | 1.4457          |\\n| 1.4536        | 0.3878 | 600  | 1.4105          |\\n| 1.638         | 0.5170 | 800  | 1.4037          |\\n| 1.3777        | 0.6463 | 1000 | 1.3969          |\\n| 1.4472        | 0.7756 | 1200 | 1.3773          |\\n| 1.3957        | 0.9048 | 1400 | 1.3688          |\\n| 1.2097        | 1.0341 | 1600 | 1.4005          |\\n\\n\\n### Framework versions\\n\\n- PEFT 0.13.2\\n- Transformers 4.46.0\\n- Pytorch 2.5.0+cu124\\n- Datasets 3.0.1\\n- Tokenizers 0.20.1',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c26cfcad08a0d12ccaaa',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.766000',\n",
       "  'date_modified': '2025-02-27T19:18:13.766000'},\n",
       " {'model_identifier': 'AltEinstein/emc9',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:02:19',\n",
       "  'last_modified': '2025-02-27T17:12:59',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'llama', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 14.957527160644531},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c26bfcad08a0d12ccaa9',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.766000',\n",
       "  'date_modified': '2025-02-27T19:18:13.766000'},\n",
       " {'model_identifier': 'strangerzonehf/Real-Claymation',\n",
       "  'version': 7,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T16:23:23',\n",
       "  'last_modified': '2025-02-27T17:13:08',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['diffusers',\n",
       "   'text-to-image',\n",
       "   'lora',\n",
       "   'template:diffusion-lora',\n",
       "   'base_model:black-forest-labs/FLUX.1-dev',\n",
       "   'base_model:adapter:black-forest-labs/FLUX.1-dev',\n",
       "   'license:apache-2.0',\n",
       "   'region:us'],\n",
       "  'task': ['text-to-image'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'description': '---\\ntags:\\n- text-to-image\\n- lora\\n- diffusers\\n- template:diffusion-lora\\nwidget:\\n- text: \\'Real Claymation, Captured at eye-level on a low-angle perspective, a blue figurine with a red hat on its head and a red scarf around its neck. The figurines head is positioned in the center of the frame, with two large white eyes and a black nose on its face. Its mouth is slightly open, revealing a black smile. The background is a stark white, adding a pop of color to the scene.\\'\\n  output:\\n    url: images/1.png\\n- text: \\'Real Claymation, a green toy frog is standing on a light brown wooden table. The frogs mouth is open, revealing a red heart in its mouth. Its mouth is adorned with red beads, adding a pop of color to the scene. Above the frogs head, a white keyboard can be seen.\\'\\n  output:\\n    url: images/22.png\\n- text: \\'Real Claymation, Captured at eye-level on a low-angle shot, two green plush toys are perched on a piece of wood. The toy on the left is facing the viewer, while the one on the right is facing away from the viewer. Both toys are made of clay, a vibrant green color, and have white teeth and a red button in the mouth. The backdrop is a lush green field, with a body of water in the distance. The sky is filled with white, fluffy clouds, adding a touch of contrast to the scene.\\'\\n  output:\\n    url: images/33.png\\n- text: \\'Real Claymation, Captured at eye-level on a low-angle, close-up shot of a green frog and a black and white penguin. The frog and penguin are facing each other, their arms wrapped around each other. The penguins head is encircled by a red hot dog, its tongue sticking out, adding a pop of color to the scene. The background is a stark white, allowing the frog to be the focal point of the image.\\'\\n  output:\\n    url: images/44.png\\n- text: \\'Real Claymation, Captured at eye-level on a low-angle, close-up shot of a yellow smiley face with open hands. The smiley face has two large black eyes and a wide, friendly grin. Its hands are extended forward as if offering a warm hug. The background is a stark white, making the bright yellow and expressive features stand out, adding a pop of color to the scene.\\'\\n  output:\\n    url: images/55.png\\nbase_model: black-forest-labs/FLUX.1-dev\\ninstance_prompt: Real Claymation\\nlicense: apache-2.0\\n---\\n![szdfcvsd.png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/zE3YsMK-NZHqXABBpp8Eu.png)\\n\\n<Gallery />\\n\\n# Model description for Real-Claymation\\nImage Processing Parameters \\n\\n| Parameter                 | Value  | Parameter                 | Value  |\\n|---------------------------|--------|---------------------------|--------|\\n| LR Scheduler              | constant | Noise Offset              | 0.03   |\\n| Optimizer                 | AdamW  | Multires Noise Discount   | 0.1    |\\n| Network Dim               | 64     | Multires Noise Iterations | 10     |\\n| Network Alpha             | 32     | Repeat & Steps           | 25 & 3100 |\\n| Epoch                     | 21   | Save Every N Epochs       | 1    |\\n\\n    Labeling: florence2-en(natural language & English)\\n    \\n    Total Images Used for Training : 28\\n\\n## Best Dimensions & Inference\\n\\n| **Dimensions** | **Aspect Ratio** | **Recommendation**       |\\n|-----------------|------------------|---------------------------|\\n| 1280 x 832      | 3:2              | Best                     |\\n| 1024 x 1024     | 1:1              | Default                  |\\n\\n### Inference Range\\n\\n- **Recommended Inference Steps:** 30–35\\n\\n## Setting Up\\n```python\\nimport torch\\nfrom pipelines import DiffusionPipeline\\n\\nbase_model = \"black-forest-labs/FLUX.1-dev\"\\npipe = DiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.bfloat16)\\n\\nlora_repo = \"strangerzonehf/Real-Claymation\"\\ntrigger_word = \"333 Pro Sketch\"  \\npipe.load_lora_weights(lora_repo)\\n\\ndevice = torch.device(\"cuda\")\\npipe.to(device)\\n```\\n## Trigger words\\n\\nYou should use `Real Claymation` to trigger the image generation.\\n\\n## Download model\\n\\nWeights for this model are available in Safetensors format.\\n\\n[Download](/strangerzonehf/Real-Claymation/tree/main) them in the Files & versions tab.\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 10.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c26afcad08a0d12ccaa8',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.765000',\n",
       "  'date_modified': '2025-02-27T19:18:13.765000'},\n",
       " {'model_identifier': 'lkoenig/BBAI_323_bunnycore',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:11:43',\n",
       "  'last_modified': '2025-02-27T17:13:30',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'mergekit',\n",
       "   'merge',\n",
       "   'conversational',\n",
       "   'base_model:bunnycore/QwQen-3B-LCoT',\n",
       "   'base_model:merge:bunnycore/QwQen-3B-LCoT',\n",
       "   'base_model:bunnycore/Qwen2.5-3B-Model-Stock-v3.1',\n",
       "   'base_model:merge:bunnycore/Qwen2.5-3B-Model-Stock-v3.1',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 4.602725982666016},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nbase_model:\\n- bunnycore/Qwen2.5-3B-Model-Stock-v3.1\\n- bunnycore/QwQen-3B-LCoT\\nlibrary_name: transformers\\ntags:\\n- mergekit\\n- merge\\n\\n---\\n# merge\\n\\nThis is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).\\n\\n## Merge Details\\n### Merge Method\\n\\nThis model was merged using the [SLERP](https://en.wikipedia.org/wiki/Slerp) merge method.\\n\\n### Models Merged\\n\\nThe following models were included in the merge:\\n* [bunnycore/Qwen2.5-3B-Model-Stock-v3.1](https://huggingface.co/bunnycore/Qwen2.5-3B-Model-Stock-v3.1)\\n* [bunnycore/QwQen-3B-LCoT](https://huggingface.co/bunnycore/QwQen-3B-LCoT)\\n\\n### Configuration\\n\\nThe following YAML configuration was used to produce this model:\\n\\n```yaml\\nslices:\\n- sources:\\n  - model: bunnycore/QwQen-3B-LCoT\\n    layer_range:\\n    - 0\\n    - 24\\n  - model: bunnycore/Qwen2.5-3B-Model-Stock-v3.1\\n    layer_range:\\n    - 0\\n    - 24\\nmerge_method: slerp\\nbase_model: bunnycore/QwQen-3B-LCoT\\nparameters:\\n  t:\\n  - filter: self_attn\\n    value: 0.5\\n  - filter: mlp\\n    value: 0.5\\n  - value: 0.5\\ndtype: bfloat16\\nallow_different_architectures: true\\nskip_module_check: true\\n```\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c269fcad08a0d12ccaa7',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.765000',\n",
       "  'date_modified': '2025-02-27T19:18:13.765000'},\n",
       " {'model_identifier': 'kyoungmiin/style_9',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:07:18',\n",
       "  'last_modified': '2025-02-27T17:13:52',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['diffusers',\n",
       "   'text-to-image',\n",
       "   'diffusers-training',\n",
       "   'lora',\n",
       "   'template:sd-lora',\n",
       "   'stable-diffusion-xl',\n",
       "   'stable-diffusion-xl-diffusers',\n",
       "   'base_model:stabilityai/stable-diffusion-xl-base-1.0',\n",
       "   'base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0',\n",
       "   'license:openrail++',\n",
       "   'region:us'],\n",
       "  'task': ['text-to-image'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'description': '---\\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\\nlibrary_name: diffusers\\nlicense: openrail++\\ninstance_prompt: sks\\nwidget: []\\ntags:\\n- text-to-image\\n- text-to-image\\n- diffusers-training\\n- diffusers\\n- lora\\n- template:sd-lora\\n- stable-diffusion-xl\\n- stable-diffusion-xl-diffusers\\n---\\n\\n<!-- This model card has been generated automatically according to the information the training script had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n\\n# SDXL LoRA DreamBooth - kyoungmiin/style_9\\n\\n<Gallery />\\n\\n## Model description\\n\\nThese are kyoungmiin/style_9 LoRA adaption weights for stabilityai/stable-diffusion-xl-base-1.0.\\n\\nThe weights were trained  using [DreamBooth](https://dreambooth.github.io/).\\n\\nLoRA for the text encoder was enabled: False.\\n\\nSpecial VAE used for training: madebyollin/sdxl-vae-fp16-fix.\\n\\n## Trigger words\\n\\nYou should use sks to trigger the image generation.\\n\\n## Download model\\n\\nWeights for this model are available in Safetensors format.\\n\\n[Download](kyoungmiin/style_9/tree/main) them in the Files & versions tab.\\n\\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\n```python\\n# TODO: add an example code snippet for running this diffusion pipeline\\n```\\n\\n#### Limitations and bias\\n\\n[TODO: provide examples of latent issues and potential remediations]\\n\\n## Training details\\n\\n[TODO: describe the data used to train the model]',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c268fcad08a0d12ccaa6',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.765000',\n",
       "  'date_modified': '2025-02-27T19:18:13.765000'},\n",
       " {'model_identifier': '4rduino/leo_gguf_v1',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:14:14',\n",
       "  'last_modified': '2025-02-27T17:14:14',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c267fcad08a0d12ccaa5',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.765000',\n",
       "  'date_modified': '2025-02-27T19:18:13.765000'},\n",
       " {'model_identifier': 'strangerzonehf/Flux-Ultimate-LoRA-Collection',\n",
       "  'version': 98,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2024-11-18T06:47:02',\n",
       "  'last_modified': '2025-02-27T17:14:25',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['diffusers',\n",
       "   'Flux.1-Dev',\n",
       "   'lora',\n",
       "   'Flux.1-Dev-Adapter',\n",
       "   'Collections',\n",
       "   'SOTA',\n",
       "   'Realism',\n",
       "   'text-to-image',\n",
       "   'base_model:black-forest-labs/FLUX.1-dev',\n",
       "   'base_model:adapter:black-forest-labs/FLUX.1-dev',\n",
       "   'license:creativeml-openrail-m',\n",
       "   'region:us'],\n",
       "  'task': ['text-to-image'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'description': '---\\nlicense: creativeml-openrail-m\\nwidget:\\n- text: Stranger Zones Ultimate LoRA Collection\\n  output:\\n    url: images/fluxxmas.png\\nbase_model:\\n- black-forest-labs/FLUX.1-dev\\npipeline_tag: text-to-image\\nlibrary_name: diffusers\\ntags:\\n- Flux.1-Dev\\n- lora\\n- Flux.1-Dev-Adapter\\n- Collections\\n- SOTA\\n- Realism\\n---\\n![fffffffffffffffff.png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/_no2IDuYBS_EZHuJlp1KJ.png)\\n## Flux.1dev Adapter Resources\\n\\n| File Name                                      | Size   | LFS  | File Type       |\\n|------------------------------------------------|--------|------|-----------------|\\n| 3DXL-Mannequin.safetensors                     | 613 MB | LFS  | .safetensors    |\\n| 3DXLC1.safetensors                             | 613 MB | LFS  | .safetensors    |\\n| 3DXLP1.safetensors                             | 613 MB | LFS  | .safetensors    |\\n| 3DXLP2.safetensors                             | 613 MB | LFS  | .safetensors    |\\n| 3DXLP3.safetensors                             | 613 MB | LFS  | .safetensors    |\\n| 3DXLP4.safetensors                             | 613 MB | LFS  | .safetensors    |\\n| 3DXLP5.safetensors                             | 613 MB | LFS  | .safetensors    |\\n| 3DXLP6.safetensors                             | 613 MB | LFS  | .safetensors    |\\n| Abstract-Cartoon.safetensors                   | 613 MB | LFS  | .safetensors    |\\n| Amxtoon.safetensors                            | 613 MB | LFS  | .safetensors    |\\n| Animeo.safetensors                             | 613 MB | LFS  | .safetensors    |\\n| Animex.safetensors                             | 613 MB | LFS  | .safetensors    |\\n| Aura-9999.safetensors                          | 613 MB | LFS  | .safetensors    |\\n| Bold-Shadows.safetensors                       | 613 MB | LFS  | .safetensors    |\\n| C33.safetensors                                | 613 MB | LFS  | .safetensors    |\\n| CAM00.safetensors                              | 613 MB | LFS  | .safetensors    |\\n| Canopus-Anime-Character-Art-FluxDev-LoRA.safetensors | 613 MB | LFS  | .safetensors    |\\n| Canopus-Car-Flux-Dev-LoRA.safetensors          | 613 MB | LFS  | .safetensors    |\\n| Canopus-Clothing-Flux-Dev-Florence2-LoRA.safetensors | 613 MB | LFS  | .safetensors    |\\n| Canopus-Cute-Kawaii-Flux-LoRA.safetensors      | 613 MB | LFS  | .safetensors    |\\n| Castor-3D-Portrait-Flux-LoRA.safetensors       | 306 MB | LFS  | .safetensors    |\\n| Castor-3D-Sketchfab-Flux-LoRA.safetensors      | 613 MB | LFS  | .safetensors    |\\n| Castor-Character-Polygon-LoRA.safetensors      | 613 MB | LFS  | .safetensors    |\\n| Castor-Collage-Dim-Flux-LoRA.safetensors       | 613 MB | LFS  | .safetensors    |\\n| Castor-Happy-Halloween-Flux-LoRA.safetensors   | 613 MB | LFS  | .safetensors    |\\n| Castor-Red-Dead-Redemption-2-Flux-LoRA.safetensors | 613 MB | LFS  | .safetensors    |\\n| Claymation.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| Clothing-Flux-Dev-Florence2-LoRA-Pruned.safetensors | 613 MB | LFS  | .safetensors    |\\n| Clouds Illusion.safetensors                    | 613 MB | LFS  | .safetensors    |\\n| Creative-Stocks.safetensors                    | 613 MB | LFS  | .safetensors    |\\n| Cute-3d-Kawaii.safetensors                     | 613 MB | LFS  | .safetensors    |\\n| Dark_Creature.safetensors                      | 613 MB | LFS  | .safetensors    |\\n| Digital-Chaos.safetensors                      | 613 MB | LFS  | .safetensors    |\\n| Digital-Yellow.safetensors                     | 613 MB | LFS  | .safetensors    |\\n| Dramatic-Neon-Flux-LoRA.safetensors            | 613 MB | LFS  | .safetensors    |\\n| EBook-Cover.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| Electric-Blue.safetensors                      | 613 MB | LFS  | .safetensors    |\\n| Fashion-Modeling.safetensors                   | 613 MB | LFS  | .safetensors    |\\n| Flux-Dev-Real-Anime-LoRA.safetensors           | 613 MB | LFS  | .safetensors    |\\n| Flux-Realism-FineDetailed.safetensors          | 613 MB | LFS  | .safetensors    |\\n| GArt.safetensors                               | 613 MB | LFS  | .safetensors    |\\n| Ghibli-Art.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| Glowing-Body.safetensors                       | 613 MB | LFS  | .safetensors    |\\n| Golden-Coin.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| Green-Cartoon.safetensors                      | 613 MB | LFS  | .safetensors    |\\n| Gta6-Concept-Charecter.safetensors             | 613 MB | LFS  | .safetensors    |\\n| Gta6.safetensors                               | 613 MB | LFS  | .safetensors    |\\n| HDR-Digital-Chaos.safetensors                  | 613 MB | LFS  | .safetensors    |\\n| HDR.safetensors                                | 613 MB | LFS  | .safetensors    |\\n| Icon-Kit.safetensors                           | 613 MB | LFS  | .safetensors    |\\n| Intense-Red.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| Isometric-3D-Cinematography.safetensors        | 613 MB | LFS  | .safetensors    |\\n| Isometric-3D.safetensors                       | 613 MB | LFS  | .safetensors    |\\n| Kepler-452b-LoRA-Flux-Dev-3D-Bubbly.safetensors | 613 MB | LFS  | .safetensors    |\\n| Knitted- Character.safetensors                 | 613 MB | LFS  | .safetensors    |\\n| Lego.safetensors                               | 613 MB | LFS  | .safetensors    |\\n| Lime-Green.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| Logo-design.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| Long-Toon.safetensors                          | 613 MB | LFS  | .safetensors    |\\n| Minimal-Futuristic.safetensors                 | 613 MB | LFS  | .safetensors    |\\n| Mockup-Texture.safetensors                     | 613 MB | LFS  | .safetensors    |\\n| Multi-Frame-Shot(MFS).safetensors              | 613 MB | LFS  | .safetensors    |\\n| NFTv4.safetensors                              | 613 MB | LFS  | .safetensors    |\\n| Orange-Chroma.safetensors                      | 613 MB | LFS  | .safetensors    |\\n| Past-Present-Deep-Mix-Flux-LoRA.safetensors    | 613 MB | LFS  | .safetensors    |\\n| Pastel-BG.safetensors                          | 613 MB | LFS  | .safetensors    |\\n| Prod-Ad.safetensors                            | 613 MB | LFS  | .safetensors    |\\n| Purple-Dreamy.safetensors                      | 613 MB | LFS  | .safetensors    |\\n| Purple_Grid.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| Red-Undersea.safetensors                       | 613 MB | LFS  | .safetensors    |\\n| Retro-Pixel.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| Seamless-Pattern-Design.safetensors            | 613 MB | LFS  | .safetensors    |\\n| Shadow-Projection.safetensors                  | 613 MB | LFS  | .safetensors    |\\n| Simple_ Doodle.safetensors                     | 270 MB | LFS  | .safetensors    |\\n| Smiley-C4C.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| Snoopy-Charlie-Brown-Flux-LoRA.safetensors     | 613 MB | LFS  | .safetensors    |\\n| Street_Bokeh.safetensors                       | 613 MB | LFS  | .safetensors    |\\n| Super-Blend.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| Super-Detail.safetensors                       | 613 MB | LFS  | .safetensors    |\\n| Super-Portrait.safetensors                     | 613 MB | LFS  | .safetensors    |\\n| Tarot-card.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| Teen-Outfit.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| Typography.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| Uncoloured-3D-Polygon.safetensors              | 613 MB | LFS  | .safetensors    |\\n| Yellow-Laser.safetensors                       | 613 MB | LFS  | .safetensors    |\\n| Yellow_Pop.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| capybara-hf.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| chill-guy.safetensors                          | 613 MB | LFS  | .safetensors    |\\n| coloring-book.safetensors                      | 613 MB | LFS  | .safetensors    |\\n| ctoon.safetensors                              | 613 MB | LFS  | .safetensors    |\\n| dalle-mix.safetensors                          | 613 MB | LFS  | .safetensors    |\\n| frosted-gc.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| handstick69.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| indo-realism.safetensors                       | 613 MB | LFS  | .safetensors    |\\n| look-in-2.safetensors                          | 613 MB | LFS  | .safetensors    |\\n| meme.safetensors                               | 613 MB | LFS  | .safetensors    |\\n| midjourney-mix.safetensors                     | 613 MB | LFS  | .safetensors    |\\n| mjV6.safetensors                               | 613 MB | LFS  | .safetensors    |\\n| movieboard.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| nm99.safetensors                               | 613 MB | LFS  | .safetensors    |\\n| only-stickers.safetensors                      | 613 MB | LFS  | .safetensors    |\\n| polaroid-plus.safetensors                      | 613 MB | LFS  | .safetensors    |\\n| poster-foss.safetensors                        | 613 MB | LFS  | .safetensors    |\\n| quoter.safetensors                             | 613 MB | LFS  | .safetensors    |\\n| sketchcard.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| stam9.safetensors                              | 613 MB | LFS  | .safetensors    |\\n| super-realism.safetensors                      | 613 MB | LFS  | .safetensors    |\\n| toon-mix.safetensors                           | 613 MB | LFS  | .safetensors    |\\n| toonic2.5D.safetensors                         | 613 MB | LFS  | .safetensors    |\\n| ywl-realism.safetensors                        | 613 MB | LFS  | .safetensors    |\\n\\n<Gallery />\\n\\n| **Repository**              | **Description**                                             | **Link**                                           |\\n|-----------------------------|-------------------------------------------------------------|---------------------------------------------------|\\n| PrithivMLMods               | Repository featuring various adapters and ML models.        | [Visit Repository](https://huggingface.co/prithivMLmods) |\\n| StrangerZoneHF              | Repository containing specialized Hugging Face models.      | [Visit Repository](https://huggingface.co/strangerzonehf) |\\n\\n------------------------------------------------------------------------------------------------------------------------------------------',\n",
       "  'popularity': {'huggingface': {'spaces_count': 6.0,\n",
       "    'downloads': 21660.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 69.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c266fcad08a0d12ccaa4',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.765000',\n",
       "  'date_modified': '2025-02-27T19:18:13.765000'},\n",
       " {'model_identifier': 'PrunaAI/nvidia-Nemotron-Mini-4B-Instruct-HQQ-4bit-smashed',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:07:16',\n",
       "  'last_modified': '2025-02-27T17:14:32',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['nemotron', 'pruna-ai', 'hqq', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'NemotronForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\\nbase_model: ORIGINAL_REPO_NAME\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\n---\\n<!-- header start -->\\n<!-- 200823 -->\\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\\n    </a>\\n</div>\\n<!-- header end -->\\n\\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\\n\\n# Simply make AI models cheaper, smaller, faster, and greener!\\n\\n- Give a thumbs up if you like this model!\\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\\n\\n## Results\\n\\n![image info](./plots.png)\\n\\n**Frequently Asked Questions**\\n- ***How does the compression work?*** The model is compressed with hqq.\\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\\n- ***How is the model efficiency evaluated?*** These results were obtained with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\\n- ***What is the model format?*** We use safetensors.\\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\\n\\n## Setup\\n\\nYou can run the smashed model with these steps:\\n\\n0. Check requirements from the original repo ORIGINAL_REPO_NAME installed. In particular, check python, cuda, and transformers versions.\\n1. Make sure that you have installed quantization related packages.\\n    ```bash\\n    pip install hqq\\n    ```\\n2. Load & run the model.\\n    ```python \\n   from transformers import AutoModelForCausalLM, AutoTokenizer\\n    from hqq.engine.hf import HQQModelForCausalLM\\n from hqq.models.hf.base import AutoHQQHFModel\\n\\n   try:\\n     model = HQQModelForCausalLM.from_quantized(\"PrunaAI/nvidia-Nemotron-Mini-4B-Instruct-HQQ-4bit-smashed\", device_map=\\'auto\\')\\n    except: \\n     model = AutoHQQHFModel.from_quantized(\"PrunaAI/nvidia-Nemotron-Mini-4B-Instruct-HQQ-4bit-smashed\")\\n   tokenizer = AutoTokenizer.from_pretrained(\"ORIGINAL_REPO_NAME\")\\n    \\n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors=\\'pt\\').to(model.device)[\"input_ids\"]\\n    \\n   outputs = model.generate(input_ids, max_new_tokens=216)\\n   tokenizer.decode(outputs[0])\\n    ```\\n\\n## Configurations\\n\\nThe configuration info are in `smash_config.json`.\\n\\n## Credits & License\\n\\nThe license of the smashed model follows the license of the original model. Please check the license of the original model ORIGINAL_REPO_NAME before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\\n\\n## Want to compress other models?\\n\\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c265fcad08a0d12ccaa3',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.764000',\n",
       "  'date_modified': '2025-02-27T19:18:13.764000'},\n",
       " {'model_identifier': 'timmmyo/1487d83f-4c34-4e14-a34f-cc2167434a29',\n",
       "  'version': 124,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T15:02:12',\n",
       "  'last_modified': '2025-02-27T17:14:38',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c264fcad08a0d12ccaa2',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.763000',\n",
       "  'date_modified': '2025-02-27T19:18:13.763000'},\n",
       " {'model_identifier': 'ucfc2024/karenpaez179',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T16:24:37',\n",
       "  'last_modified': '2025-02-27T17:15:39',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['license:other', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: other\\nlicense_name: flux-1-dev-non-commercial-license\\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md\\n---',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c261fcad08a0d12ccaa0',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.762000',\n",
       "  'date_modified': '2025-02-27T19:18:13.762000'},\n",
       " {'model_identifier': 'Davidx180/brainex.ai',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:16:11',\n",
       "  'last_modified': '2025-02-27T17:16:11',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['license:artistic-2.0', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'artistic-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: artistic-2.0\\n---\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c25ffcad08a0d12cca9e',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.555000',\n",
       "  'date_modified': '2025-02-27T19:18:13.555000'},\n",
       " {'model_identifier': 'AltEinstein/emc12',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:02:28',\n",
       "  'last_modified': '2025-02-27T17:16:41',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'llama', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 14.957527160644531},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c25efcad08a0d12cca9d',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.555000',\n",
       "  'date_modified': '2025-02-27T19:18:13.555000'},\n",
       " {'model_identifier': 'Hachipo/llama3-_Llama-3-8B_MIFT_ja_1000_trans_v5_cot',\n",
       "  'version': 3,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:07:41',\n",
       "  'last_modified': '2025-02-27T17:16:46',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'llama',\n",
       "   'text-generation',\n",
       "   'trl',\n",
       "   'sft',\n",
       "   'arxiv:1910.09700',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 14.957588195800781},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nlibrary_name: transformers\\ntags:\\n- trl\\n- sft\\n---\\n\\n# Model Card for Model ID\\n\\n<!-- Provide a quick summary of what the model is/does. -->\\n\\n\\n\\n## Model Details\\n\\n### Model Description\\n\\n<!-- Provide a longer summary of what this model is. -->\\n\\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\\n\\n- **Developed by:** [More Information Needed]\\n- **Funded by [optional]:** [More Information Needed]\\n- **Shared by [optional]:** [More Information Needed]\\n- **Model type:** [More Information Needed]\\n- **Language(s) (NLP):** [More Information Needed]\\n- **License:** [More Information Needed]\\n- **Finetuned from model [optional]:** [More Information Needed]\\n\\n### Model Sources [optional]\\n\\n<!-- Provide the basic links for the model. -->\\n\\n- **Repository:** [More Information Needed]\\n- **Paper [optional]:** [More Information Needed]\\n- **Demo [optional]:** [More Information Needed]\\n\\n## Uses\\n\\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\\n\\n### Direct Use\\n\\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\\n\\n[More Information Needed]\\n\\n### Downstream Use [optional]\\n\\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\\n\\n[More Information Needed]\\n\\n### Out-of-Scope Use\\n\\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\\n\\n[More Information Needed]\\n\\n## Bias, Risks, and Limitations\\n\\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\\n\\n[More Information Needed]\\n\\n### Recommendations\\n\\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\\n\\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n## How to Get Started with the Model\\n\\nUse the code below to get started with the model.\\n\\n[More Information Needed]\\n\\n## Training Details\\n\\n### Training Data\\n\\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\\n\\n[More Information Needed]\\n\\n### Training Procedure\\n\\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\\n\\n#### Preprocessing [optional]\\n\\n[More Information Needed]\\n\\n\\n#### Training Hyperparameters\\n\\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\\n\\n#### Speeds, Sizes, Times [optional]\\n\\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\\n\\n[More Information Needed]\\n\\n## Evaluation\\n\\n<!-- This section describes the evaluation protocols and provides the results. -->\\n\\n### Testing Data, Factors & Metrics\\n\\n#### Testing Data\\n\\n<!-- This should link to a Dataset Card if possible. -->\\n\\n[More Information Needed]\\n\\n#### Factors\\n\\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\\n\\n[More Information Needed]\\n\\n#### Metrics\\n\\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\\n\\n[More Information Needed]\\n\\n### Results\\n\\n[More Information Needed]\\n\\n#### Summary\\n\\n\\n\\n## Model Examination [optional]\\n\\n<!-- Relevant interpretability work for the model goes here -->\\n\\n[More Information Needed]\\n\\n## Environmental Impact\\n\\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\\n\\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n\\n- **Hardware Type:** [More Information Needed]\\n- **Hours used:** [More Information Needed]\\n- **Cloud Provider:** [More Information Needed]\\n- **Compute Region:** [More Information Needed]\\n- **Carbon Emitted:** [More Information Needed]\\n\\n## Technical Specifications [optional]\\n\\n### Model Architecture and Objective\\n\\n[More Information Needed]\\n\\n### Compute Infrastructure\\n\\n[More Information Needed]\\n\\n#### Hardware\\n\\n[More Information Needed]\\n\\n#### Software\\n\\n[More Information Needed]\\n\\n## Citation [optional]\\n\\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\\n\\n**BibTeX:**\\n\\n[More Information Needed]\\n\\n**APA:**\\n\\n[More Information Needed]\\n\\n## Glossary [optional]\\n\\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\\n\\n[More Information Needed]\\n\\n## More Information [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Authors [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Contact\\n\\n[More Information Needed]',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c25dfcad08a0d12cca9c',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.555000',\n",
       "  'date_modified': '2025-02-27T19:18:13.555000'},\n",
       " {'model_identifier': 'ianthereal-z/DeepSeek-R1-Qwen-14B-StackVM',\n",
       "  'version': 7,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T10:56:03',\n",
       "  'last_modified': '2025-02-27T17:16:57',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'text-generation-inference',\n",
       "   'unsloth',\n",
       "   'trl',\n",
       "   'sft',\n",
       "   'conversational',\n",
       "   'en',\n",
       "   'base_model:unsloth/DeepSeek-R1-Distill-Qwen-14B',\n",
       "   'base_model:finetune:unsloth/DeepSeek-R1-Distill-Qwen-14B',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A workstation GPU with 40GB VRAM (e.g., NVIDIA A100 40GB).',\n",
       "   'memory_in_gb': 27.51133155822754},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nbase_model: unsloth/DeepSeek-R1-Distill-Qwen-14B\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- qwen2\\n- trl\\n- sft\\nlicense: apache-2.0\\nlanguage:\\n- en\\n---\\n\\n# Uploaded  model\\n\\n- **Developed by:** ianthereal-z\\n- **License:** apache-2.0\\n- **Finetuned from model :** unsloth/DeepSeek-R1-Distill-Qwen-14B\\n\\nThis qwen2 model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface\\'s TRL library.\\n\\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c25cfcad08a0d12cca9b',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.555000',\n",
       "  'date_modified': '2025-02-27T19:18:13.555000'},\n",
       " {'model_identifier': 'mradermacher/MS3-megamerge-i1-GGUF',\n",
       "  'version': 26,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T14:06:47',\n",
       "  'last_modified': '2025-02-27T17:16:59',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': True,\n",
       "  'tags': ['transformers',\n",
       "   'gguf',\n",
       "   'en',\n",
       "   'base_model:d-rang-d/MS3-megamerge',\n",
       "   'base_model:quantized:d-rang-d/MS3-megamerge',\n",
       "   'endpoints_compatible',\n",
       "   'region:us',\n",
       "   'imatrix',\n",
       "   'conversational'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A high-end GPU with 24GB VRAM (e.g., NVIDIA RTX 3090, A5000).',\n",
       "   'memory_in_gb': 21.953510284423828},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': \"---\\nbase_model: d-rang-d/MS3-megamerge\\nlanguage:\\n- en\\nlibrary_name: transformers\\nquantized_by: mradermacher\\n---\\n## About\\n\\n<!-- ### quantize_version: 2 -->\\n<!-- ### output_tensor_quantised: 1 -->\\n<!-- ### convert_type: hf -->\\n<!-- ### vocab_type:  -->\\n<!-- ### tags: nicoboss -->\\nweighted/imatrix quants of https://huggingface.co/d-rang-d/MS3-megamerge\\n\\n<!-- provided-files -->\\nstatic quants are available at https://huggingface.co/mradermacher/MS3-megamerge-GGUF\\n## Usage\\n\\nIf you are unsure how to use GGUF files, refer to one of [TheBloke's\\nREADMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for\\nmore details, including on how to concatenate multi-part files.\\n\\n## Provided Quants\\n\\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\\n\\n| Link | Type | Size/GB | Notes |\\n|:-----|:-----|--------:|:------|\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ1_S.gguf) | i1-IQ1_S | 5.4 | for the desperate |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ1_M.gguf) | i1-IQ1_M | 5.9 | mostly desperate |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ2_XXS.gguf) | i1-IQ2_XXS | 6.6 |  |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ2_XS.gguf) | i1-IQ2_XS | 7.3 |  |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ2_S.gguf) | i1-IQ2_S | 7.6 |  |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ2_M.gguf) | i1-IQ2_M | 8.2 |  |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q2_K_S.gguf) | i1-Q2_K_S | 8.4 | very low quality |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q2_K.gguf) | i1-Q2_K | 9.0 | IQ3_XXS probably better |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ3_XXS.gguf) | i1-IQ3_XXS | 9.4 | lower quality |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ3_XS.gguf) | i1-IQ3_XS | 10.0 |  |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q3_K_S.gguf) | i1-Q3_K_S | 10.5 | IQ3_XS probably better |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ3_S.gguf) | i1-IQ3_S | 10.5 | beats Q3_K* |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ3_M.gguf) | i1-IQ3_M | 10.8 |  |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q3_K_M.gguf) | i1-Q3_K_M | 11.6 | IQ3_S probably better |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q3_K_L.gguf) | i1-Q3_K_L | 12.5 | IQ3_M probably better |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-IQ4_XS.gguf) | i1-IQ4_XS | 12.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q4_0.gguf) | i1-Q4_0 | 13.6 | fast, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q4_K_S.gguf) | i1-Q4_K_S | 13.6 | optimal size/speed/quality |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q4_K_M.gguf) | i1-Q4_K_M | 14.4 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q4_1.gguf) | i1-Q4_1 | 15.0 |  |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q5_K_S.gguf) | i1-Q5_K_S | 16.4 |  |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q5_K_M.gguf) | i1-Q5_K_M | 16.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/MS3-megamerge-i1-GGUF/resolve/main/MS3-megamerge.i1-Q6_K.gguf) | i1-Q6_K | 19.4 | practically like static Q6_K |\\n\\nHere is a handy graph by ikawrakow comparing some lower-quality quant\\ntypes (lower is better):\\n\\n![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)\\n\\nAnd here are Artefact2's thoughts on the matter:\\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\\n\\n## FAQ / Model Request\\n\\nSee https://huggingface.co/mradermacher/model_requests for some answers to\\nquestions you might have and/or if you want some other model quantized.\\n\\n## Thanks\\n\\nI thank my company, [nethype GmbH](https://www.nethype.de/), for letting\\nme use its servers and providing upgrades to my workstation to enable\\nthis work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.\\n\\n<!-- end -->\\n\",\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c25bfcad08a0d12cca9a',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.444000',\n",
       "  'date_modified': '2025-02-27T19:18:13.444000'},\n",
       " {'model_identifier': 'whiteapple8222/a18584fb-76f5-451b-9d6d-33f5ea9a5245',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:17:27',\n",
       "  'last_modified': '2025-02-27T17:17:27',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c257fcad08a0d12cca96',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.338000',\n",
       "  'date_modified': '2025-02-27T19:18:13.338000'},\n",
       " {'model_identifier': 'whiteapple8222/92b45eff-d7ea-4726-94de-d31c04e78866',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:17:27',\n",
       "  'last_modified': '2025-02-27T17:17:27',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c256fcad08a0d12cca95',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.338000',\n",
       "  'date_modified': '2025-02-27T19:18:13.338000'},\n",
       " {'model_identifier': 'mradermacher/Megatron-Opus-14B-2.1-i1-GGUF',\n",
       "  'version': 5,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T15:23:52',\n",
       "  'last_modified': '2025-02-27T17:17:29',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': True,\n",
       "  'tags': ['gguf',\n",
       "   'endpoints_compatible',\n",
       "   'region:us',\n",
       "   'imatrix',\n",
       "   'conversational'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 13.652729988098145},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '<!-- ### quantize_version: 2 -->\\n<!-- ### output_tensor_quantised: 1 -->\\n<!-- ### convert_type: hf -->\\n<!-- ### vocab_type:  -->\\n<!-- ### tags: nicoboss -->\\nweighted/imatrix quants of https://huggingface.co/prithivMLmods/Megatron-Opus-14B-2.1\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c254fcad08a0d12cca94',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:13.338000',\n",
       "  'date_modified': '2025-02-27T19:18:13.338000'},\n",
       " {'model_identifier': 'tomaarsen/reranker-msmarco-v1.1-MiniLM-L12-H384-uncased-listnet-sigmoid',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:22:46',\n",
       "  'last_modified': '2025-02-27T17:22:49',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['sentence-transformers',\n",
       "   'safetensors',\n",
       "   'bert',\n",
       "   'cross-encoder',\n",
       "   'text-classification',\n",
       "   'generated_from_trainer',\n",
       "   'dataset_size:78704',\n",
       "   'loss:ListNetLoss',\n",
       "   'en',\n",
       "   'dataset:microsoft/ms_marco',\n",
       "   'arxiv:1908.10084',\n",
       "   'base_model:microsoft/MiniLM-L12-H384-uncased',\n",
       "   'base_model:finetune:microsoft/MiniLM-L12-H384-uncased',\n",
       "   'co2_eq_emissions',\n",
       "   'region:us'],\n",
       "  'task': ['text-classification'],\n",
       "  'architecture': 'BertForSequenceClassification',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.12427711486816406},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'description': '---\\nlanguage:\\n- en\\ntags:\\n- sentence-transformers\\n- cross-encoder\\n- text-classification\\n- generated_from_trainer\\n- dataset_size:78704\\n- loss:ListNetLoss\\nbase_model: microsoft/MiniLM-L12-H384-uncased\\ndatasets:\\n- microsoft/ms_marco\\npipeline_tag: text-classification\\nlibrary_name: sentence-transformers\\nmetrics:\\n- map\\n- mrr@10\\n- ndcg@10\\nco2_eq_emissions:\\n  emissions: 206.93024560685208\\n  energy_consumed: 0.532362183901426\\n  source: codecarbon\\n  training_type: fine-tuning\\n  on_cloud: false\\n  cpu_model: 13th Gen Intel(R) Core(TM) i7-13700K\\n  ram_total_size: 31.777088165283203\\n  hours_used: 1.708\\n  hardware_used: 1 x NVIDIA GeForce RTX 3090\\nmodel-index:\\n- name: CrossEncoder based on microsoft/MiniLM-L12-H384-uncased\\n  results: []\\n---\\n\\n# CrossEncoder based on microsoft/MiniLM-L12-H384-uncased\\n\\nThis is a [Cross Encoder](https://www.sbert.net/docs/cross_encoder/usage/usage.html) model finetuned from [microsoft/MiniLM-L12-H384-uncased](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased) on the [ms_marco](https://huggingface.co/datasets/microsoft/ms_marco) dataset using the [sentence-transformers](https://www.SBERT.net) library. It computes scores for pairs of texts, which can be used for semantic textual similarity, semantic search, paraphrase mining, text classification, clustering, and more.\\n\\n## Model Details\\n\\n### Model Description\\n- **Model Type:** Cross Encoder\\n- **Base model:** [microsoft/MiniLM-L12-H384-uncased](https://huggingface.co/microsoft/MiniLM-L12-H384-uncased) <!-- at revision 44acabbec0ef496f6dbc93adadea57f376b7c0ec -->\\n- **Maximum Sequence Length:** 512 tokens\\n- **Number of Output Labels:** 1 label\\n- **Training Dataset:**\\n    - [ms_marco](https://huggingface.co/datasets/microsoft/ms_marco)\\n- **Language:** en\\n<!-- - **License:** Unknown -->\\n\\n### Model Sources\\n\\n- **Documentation:** [Sentence Transformers Documentation](https://sbert.net)\\n- **Documentation:** [Cross Encoder Documentation](https://www.sbert.net/docs/cross_encoder/usage/usage.html)\\n- **Repository:** [Sentence Transformers on GitHub](https://github.com/UKPLab/sentence-transformers)\\n- **Hugging Face:** [Cross Encoders on Hugging Face](https://huggingface.co/models?library=sentence-transformers&other=cross-encoder)\\n\\n## Usage\\n\\n### Direct Usage (Sentence Transformers)\\n\\nFirst install the Sentence Transformers library:\\n\\n```bash\\npip install -U sentence-transformers\\n```\\n\\nThen you can load this model and run inference.\\n```python\\nfrom sentence_transformers import CrossEncoder\\n\\n# Download from the 🤗 Hub\\nmodel = CrossEncoder(\"tomaarsen/reranker-msmarco-v1.1-MiniLM-L12-H384-uncased-listnet-sigmoid\")\\n# Get scores for pairs of texts\\npairs = [\\n    [\\'How many calories in an egg\\', \\'There are on average between 55 and 80 calories in an egg depending on its size.\\'],\\n    [\\'How many calories in an egg\\', \\'Egg whites are very low in calories, have no fat, no cholesterol, and are loaded with protein.\\'],\\n    [\\'How many calories in an egg\\', \\'Most of the calories in an egg come from the yellow yolk in the center.\\'],\\n]\\nscores = model.predict(pairs)\\nprint(scores.shape)\\n# (3,)\\n\\n# Or rank different texts based on similarity to a single text\\nranks = model.rank(\\n    \\'How many calories in an egg\\',\\n    [\\n        \\'There are on average between 55 and 80 calories in an egg depending on its size.\\',\\n        \\'Egg whites are very low in calories, have no fat, no cholesterol, and are loaded with protein.\\',\\n        \\'Most of the calories in an egg come from the yellow yolk in the center.\\',\\n    ]\\n)\\n# [{\\'corpus_id\\': ..., \\'score\\': ...}, {\\'corpus_id\\': ..., \\'score\\': ...}, ...]\\n```\\n\\n<!--\\n### Direct Usage (Transformers)\\n\\n<details><summary>Click to see the direct usage in Transformers</summary>\\n\\n</details>\\n-->\\n\\n<!--\\n### Downstream Usage (Sentence Transformers)\\n\\nYou can finetune this model on your own dataset.\\n\\n<details><summary>Click to expand</summary>\\n\\n</details>\\n-->\\n\\n<!--\\n### Out-of-Scope Use\\n\\n*List how the model may foreseeably be misused and address what users ought not to do with the model.*\\n-->\\n\\n## Evaluation\\n\\n### Metrics\\n\\n#### Cross Encoder Reranking\\n\\n* Datasets: `NanoMSMARCO`, `NanoNFCorpus` and `NanoNQ`\\n* Evaluated with [<code>CrossEncoderRerankingEvaluator</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.CrossEncoderRerankingEvaluator)\\n\\n| Metric      | NanoMSMARCO          | NanoNFCorpus         | NanoNQ               |\\n|:------------|:---------------------|:---------------------|:---------------------|\\n| map         | 0.5282 (+0.0387)     | 0.3272 (+0.0662)     | 0.5598 (+0.1402)     |\\n| mrr@10      | 0.5156 (+0.0381)     | 0.5520 (+0.0521)     | 0.5627 (+0.1360)     |\\n| **ndcg@10** | **0.5787 (+0.0383)** | **0.3588 (+0.0337)** | **0.6221 (+0.1215)** |\\n\\n#### Cross Encoder Nano BEIR\\n\\n* Dataset: `NanoBEIR_R100_mean`\\n* Evaluated with [<code>CrossEncoderNanoBEIREvaluator</code>](https://sbert.net/docs/package_reference/cross_encoder/evaluation.html#sentence_transformers.cross_encoder.evaluation.CrossEncoderNanoBEIREvaluator)\\n\\n| Metric      | Value                |\\n|:------------|:---------------------|\\n| map         | 0.4717 (+0.0817)     |\\n| mrr@10      | 0.5434 (+0.0754)     |\\n| **ndcg@10** | **0.5199 (+0.0645)** |\\n\\n<!--\\n## Bias, Risks and Limitations\\n\\n*What are the known or foreseeable issues stemming from this model? You could also flag here known failure cases or weaknesses of the model.*\\n-->\\n\\n<!--\\n### Recommendations\\n\\n*What are recommendations with respect to the foreseeable issues? For example, filtering explicit content.*\\n-->\\n\\n## Training Details\\n\\n### Training Dataset\\n\\n#### ms_marco\\n\\n* Dataset: [ms_marco](https://huggingface.co/datasets/microsoft/ms_marco) at [a47ee7a](https://huggingface.co/datasets/microsoft/ms_marco/tree/a47ee7aae8d7d466ba15f9f0bfac3b3681087b3a)\\n* Size: 78,704 training samples\\n* Columns: <code>query</code>, <code>docs</code>, and <code>labels</code>\\n* Approximate statistics based on the first 1000 samples:\\n  |         | query                                                                                          | docs                                | labels                              |\\n  |:--------|:-----------------------------------------------------------------------------------------------|:------------------------------------|:------------------------------------|\\n  | type    | string                                                                                         | list                                | list                                |\\n  | details | <ul><li>min: 11 characters</li><li>mean: 33.13 characters</li><li>max: 90 characters</li></ul> | <ul><li>size: 10 elements</li></ul> | <ul><li>size: 10 elements</li></ul> |\\n* Samples:\\n  | query                                                | docs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | labels                            |\\n  |:-----------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------|\\n  | <code>how much does the average funeral cost</code>  | <code>[\\'Average Funeral Costs. According to the Federal Trade Commission, the average funeral costs in the United States can be well over $10,000 by the time you add floral arrangements, prayer cards and family transportation. Traditionally, when people think of funeral expenses they think of things like a casket and flowers. Headstones often start around $500 and run upwards of $4000. The materials used for construction contribute to a wide price range. An average granite headstone in 2009 cost about $1500. If your loved one did not already have a cemetery plot, you will need to purchase one. Prices of cemetery plots depend on location. They start as low as a few hundred dollars and can be upward of a few thousand\\', \\'National average funeral cost: $5,000-$15,000 CAD. In Canada, the price of a funeral varies greatly by area and method. A basic cremation service in Toronto can cost $1,470 CAD to cut costs. Many families are opting to choose at-home funerals. National average funeral cost: $6,...</code> | <code>[1, 1, 0, 0, 0, ...]</code> |\\n  | <code>what is the seven sisters constellation</code> | <code>[\"The Seven Sisters is a small grouping of stars better known as the Pleiades, in the constellation Taurus. It is a group of six to seven stars (with the naked eye) and about 36 … stars (with binoculars) easily viewable on any clear night in the wintertime or very early spring. The Seven Sisters is what is known as an open cluster of stars. It\\'s also known as The Pleiades. (PLEE-uh-DEES). 3 people found this useful. Edit. Share to: 1  The Periodic Table of Elements Life is sustained by a number of chemical elements.\", \\'For other uses of Pleiades or Pleiades, pléiades See (pleiades) . Disambiguation in, astronomy The (/pleiades.ˈplaɪ/ ədiːz /or.ˈpliː/), ədiːz Or Seven (Sisters messier 45 Or), m45 is an open star cluster containing-middle aged Hot-b type stars located in the constellation Of. taurus The nine brightest stars of the Pleiades are named for the Seven Sisters of Greek mythology: Sterope, Merope, Electra, Maia, Taygeta, Celaeno, and Alcyone, along with their parents Atlas and ...</code> | <code>[1, 0, 0, 0, 0, ...]</code> |\\n  | <code>the name nicole means</code>                   | <code>[\"Nicole is a feminine given name and a surname. The given name Nicole is of Greek origin and means victorious people. It\\'s evolved into a French feminine derivative of the masculine given name Nicolas. There are many variants. The surname Nicole originates in Netherlands where it was notable for its various branches, and associated status or influenc\", \"Nicole is a feminine given name and a surname.The given name Nicole is of Greek origin and means victorious people. It\\'s evolved into a French feminine derivative of the masculine given name Nicholas.The given name Nicole is of Greek origin and means v. A feminine form of Nicolas, which is from the Greek Nikolaos, a compound name composed of the elements nikē (victory) and laos (the people): hence, victory of the people. Cole, Ercole, Micole, Niocole, Nycole. Nicole is a feminine given name and a surname.The given name Nicole is of Greek origin and means victorious people. It\\'s evolved into a French feminine derivative of the masculine...</code> | <code>[1, 0, 0, 0, 0, ...]</code> |\\n* Loss: [<code>ListNetLoss</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#listnetloss) with these parameters:\\n  ```json\\n  {\\n      \"pad_value\": -1,\\n      \"activation_fct\": \"torch.nn.modules.activation.Sigmoid\"\\n  }\\n  ```\\n\\n### Evaluation Dataset\\n\\n#### ms_marco\\n\\n* Dataset: [ms_marco](https://huggingface.co/datasets/microsoft/ms_marco) at [a47ee7a](https://huggingface.co/datasets/microsoft/ms_marco/tree/a47ee7aae8d7d466ba15f9f0bfac3b3681087b3a)\\n* Size: 1,000 evaluation samples\\n* Columns: <code>query</code>, <code>docs</code>, and <code>labels</code>\\n* Approximate statistics based on the first 1000 samples:\\n  |         | query                                                                                           | docs                                | labels                              |\\n  |:--------|:------------------------------------------------------------------------------------------------|:------------------------------------|:------------------------------------|\\n  | type    | string                                                                                          | list                                | list                                |\\n  | details | <ul><li>min: 10 characters</li><li>mean: 33.86 characters</li><li>max: 100 characters</li></ul> | <ul><li>size: 10 elements</li></ul> | <ul><li>size: 10 elements</li></ul> |\\n* Samples:\\n  | query                                                   | docs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | labels                            |\\n  |:--------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------|\\n  | <code>how much does a adjunct professor get paid</code> | <code>[\\'The average per-course pay reported for adjuncts at Ohio State University is $4,853, compared with an average of $6,500 reported at the University of Michigan at Ann Arbor. Harvard pays adjuncts $11,037, on average, according to the data that adjuncts have submitted so far. Many adjuncts have also indicated that they are essentially shut out of participating in most forms of governance. The overall average pay reported by adjuncts is $2,987 per three-credit course. Adjuncts at 16 colleges reported earning less than $1,000. The highest pay reported is $12,575, in the anthropology department at Harvard University\\', \"Not surprisingly, at community colleges, adjuncts said they are paid much less. At Houston Community College, adjuncts reported earning between $1,200 and $2,200 for a three-credit English course. In some departments, adjuncts said anecdotally that pay depends on the degree held. One adjunct professor in history, for example, reported that where he or she works, instructors...</code> | <code>[1, 1, 0, 0, 0, ...]</code> |\\n  | <code>what a normal heart beat per minute</code>        | <code>[\"1 A normal adult resting heart beat is between 60-100 heartbeats per minute. 2  Some experienced athletes may see their resting heartrate fall below 60 beats per minute. 3  Tachycardia refers to the heart beating too fast at rest-over 100 beats per minute. 1 Your heart rate is the number of times per minute that the heart beats. 2  Heart rate rises significantly in response to adrenaline if a person is frightened or surprised. 3  Taking a person\\'s pulse is a direct measure of heart rate.\", \\'Most adults have a resting heart rate of 60-100 beats per minute (bpm). The fitter you are, the lower your resting heart rate is likely to be. For example, athletes may have a resting heart rate of 40-60 bpm or lower. \\', \\'Even if you’re not an athlete, knowledge about your heart rate can help you monitor your fitness level — and it might even help you spot developing health problems. Your heart rate, or pulse, is the number of times your heart beats per minute. Normal heart rate varies from person...</code> | <code>[1, 0, 0, 0, 0, ...]</code> |\\n  | <code>what is sauterne wine</code>                      | <code>[\\'Sauternes is a French sweet wine from the Sauternais region of the Graves section in Bordeaux. Barsac lies within Sauternes, and is entitled to use either name. Somewhat similar but less expensive and typically less-distinguished wines are produced in the neighboring regions of Monbazillac, Cerons, Cérons loupiac And. cadillac\\', \\'Sauternes is made from Semillon, Sémillon sauvignon, blanc And muscadelle grapes that have been affected By botrytis, cinerea also known as noble. rot Barsac lies within Sauternes, and is entitled to use either name. Somewhat similar but less expensive and typically less-distinguished wines are produced in the neighboring regions of Monbazillac, Cerons, Cérons loupiac And. cadillac\\', \\'Sauternes, 40 miles (65km) south of Bordeaux city, is a village famous for its high-quality sweet wines. Although some wineries here produce dry wines, they sell them under appellations other than the sweet-specific Sauternes appellation. A half-bottle of top-quality, aged Saut...</code> | <code>[1, 0, 0, 0, 0, ...]</code> |\\n* Loss: [<code>ListNetLoss</code>](https://sbert.net/docs/package_reference/cross_encoder/losses.html#listnetloss) with these parameters:\\n  ```json\\n  {\\n      \"pad_value\": -1,\\n      \"activation_fct\": \"torch.nn.modules.activation.Sigmoid\"\\n  }\\n  ```\\n\\n### Training Hyperparameters\\n#### Non-Default Hyperparameters\\n\\n- `eval_strategy`: steps\\n- `per_device_train_batch_size`: 6\\n- `per_device_eval_batch_size`: 16\\n- `learning_rate`: 2e-05\\n- `warmup_ratio`: 0.1\\n- `seed`: 12\\n- `bf16`: True\\n- `load_best_model_at_end`: True\\n\\n#### All Hyperparameters\\n<details><summary>Click to expand</summary>\\n\\n- `overwrite_output_dir`: False\\n- `do_predict`: False\\n- `eval_strategy`: steps\\n- `prediction_loss_only`: True\\n- `per_device_train_batch_size`: 6\\n- `per_device_eval_batch_size`: 16\\n- `per_gpu_train_batch_size`: None\\n- `per_gpu_eval_batch_size`: None\\n- `gradient_accumulation_steps`: 1\\n- `eval_accumulation_steps`: None\\n- `torch_empty_cache_steps`: None\\n- `learning_rate`: 2e-05\\n- `weight_decay`: 0.0\\n- `adam_beta1`: 0.9\\n- `adam_beta2`: 0.999\\n- `adam_epsilon`: 1e-08\\n- `max_grad_norm`: 1.0\\n- `num_train_epochs`: 3\\n- `max_steps`: -1\\n- `lr_scheduler_type`: linear\\n- `lr_scheduler_kwargs`: {}\\n- `warmup_ratio`: 0.1\\n- `warmup_steps`: 0\\n- `log_level`: passive\\n- `log_level_replica`: warning\\n- `log_on_each_node`: True\\n- `logging_nan_inf_filter`: True\\n- `save_safetensors`: True\\n- `save_on_each_node`: False\\n- `save_only_model`: False\\n- `restore_callback_states_from_checkpoint`: False\\n- `no_cuda`: False\\n- `use_cpu`: False\\n- `use_mps_device`: False\\n- `seed`: 12\\n- `data_seed`: None\\n- `jit_mode_eval`: False\\n- `use_ipex`: False\\n- `bf16`: True\\n- `fp16`: False\\n- `fp16_opt_level`: O1\\n- `half_precision_backend`: auto\\n- `bf16_full_eval`: False\\n- `fp16_full_eval`: False\\n- `tf32`: None\\n- `local_rank`: 0\\n- `ddp_backend`: None\\n- `tpu_num_cores`: None\\n- `tpu_metrics_debug`: False\\n- `debug`: []\\n- `dataloader_drop_last`: False\\n- `dataloader_num_workers`: 0\\n- `dataloader_prefetch_factor`: None\\n- `past_index`: -1\\n- `disable_tqdm`: False\\n- `remove_unused_columns`: True\\n- `label_names`: None\\n- `load_best_model_at_end`: True\\n- `ignore_data_skip`: False\\n- `fsdp`: []\\n- `fsdp_min_num_params`: 0\\n- `fsdp_config`: {\\'min_num_params\\': 0, \\'xla\\': False, \\'xla_fsdp_v2\\': False, \\'xla_fsdp_grad_ckpt\\': False}\\n- `fsdp_transformer_layer_cls_to_wrap`: None\\n- `accelerator_config`: {\\'split_batches\\': False, \\'dispatch_batches\\': None, \\'even_batches\\': True, \\'use_seedable_sampler\\': True, \\'non_blocking\\': False, \\'gradient_accumulation_kwargs\\': None}\\n- `deepspeed`: None\\n- `label_smoothing_factor`: 0.0\\n- `optim`: adamw_torch\\n- `optim_args`: None\\n- `adafactor`: False\\n- `group_by_length`: False\\n- `length_column_name`: length\\n- `ddp_find_unused_parameters`: None\\n- `ddp_bucket_cap_mb`: None\\n- `ddp_broadcast_buffers`: False\\n- `dataloader_pin_memory`: True\\n- `dataloader_persistent_workers`: False\\n- `skip_memory_metrics`: True\\n- `use_legacy_prediction_loop`: False\\n- `push_to_hub`: False\\n- `resume_from_checkpoint`: None\\n- `hub_model_id`: None\\n- `hub_strategy`: every_save\\n- `hub_private_repo`: None\\n- `hub_always_push`: False\\n- `gradient_checkpointing`: False\\n- `gradient_checkpointing_kwargs`: None\\n- `include_inputs_for_metrics`: False\\n- `include_for_metrics`: []\\n- `eval_do_concat_batches`: True\\n- `fp16_backend`: auto\\n- `push_to_hub_model_id`: None\\n- `push_to_hub_organization`: None\\n- `mp_parameters`: \\n- `auto_find_batch_size`: False\\n- `full_determinism`: False\\n- `torchdynamo`: None\\n- `ray_scope`: last\\n- `ddp_timeout`: 1800\\n- `torch_compile`: False\\n- `torch_compile_backend`: None\\n- `torch_compile_mode`: None\\n- `dispatch_batches`: None\\n- `split_batches`: None\\n- `include_tokens_per_second`: False\\n- `include_num_input_tokens_seen`: False\\n- `neftune_noise_alpha`: None\\n- `optim_target_modules`: None\\n- `batch_eval_metrics`: False\\n- `eval_on_start`: False\\n- `use_liger_kernel`: False\\n- `eval_use_gather_object`: False\\n- `average_tokens_across_devices`: False\\n- `prompts`: None\\n- `batch_sampler`: batch_sampler\\n- `multi_dataset_batch_sampler`: proportional\\n\\n</details>\\n\\n### Training Logs\\n| Epoch      | Step      | Training Loss | Validation Loss | NanoMSMARCO_ndcg@10  | NanoNFCorpus_ndcg@10 | NanoNQ_ndcg@10       | NanoBEIR_R100_mean_ndcg@10 |\\n|:----------:|:---------:|:-------------:|:---------------:|:--------------------:|:--------------------:|:--------------------:|:--------------------------:|\\n| -1         | -1        | -             | -               | 0.0644 (-0.4760)     | 0.2696 (-0.0554)     | 0.0762 (-0.4245)     | 0.1367 (-0.3186)           |\\n| 0.0001     | 1         | 2.0486        | -               | -                    | -                    | -                    | -                          |\\n| 0.0762     | 1000      | 2.0857        | -               | -                    | -                    | -                    | -                          |\\n| 0.1525     | 2000      | 2.082         | -               | -                    | -                    | -                    | -                          |\\n| 0.2287     | 3000      | 2.0758        | -               | -                    | -                    | -                    | -                          |\\n| 0.3049     | 4000      | 2.0736        | 2.0711          | 0.5528 (+0.0124)     | 0.3638 (+0.0388)     | 0.6038 (+0.1032)     | 0.5068 (+0.0514)           |\\n| 0.3812     | 5000      | 2.0735        | -               | -                    | -                    | -                    | -                          |\\n| 0.4574     | 6000      | 2.0685        | -               | -                    | -                    | -                    | -                          |\\n| 0.5336     | 7000      | 2.0725        | -               | -                    | -                    | -                    | -                          |\\n| 0.6098     | 8000      | 2.0746        | 2.0698          | 0.5251 (-0.0153)     | 0.3446 (+0.0195)     | 0.5542 (+0.0535)     | 0.4746 (+0.0193)           |\\n| 0.6861     | 9000      | 2.0759        | -               | -                    | -                    | -                    | -                          |\\n| 0.7623     | 10000     | 2.0723        | -               | -                    | -                    | -                    | -                          |\\n| 0.8385     | 11000     | 2.0787        | -               | -                    | -                    | -                    | -                          |\\n| 0.9148     | 12000     | 2.0742        | 2.0697          | 0.5562 (+0.0157)     | 0.3700 (+0.0450)     | 0.6062 (+0.1056)     | 0.5108 (+0.0554)           |\\n| 0.9910     | 13000     | 2.0736        | -               | -                    | -                    | -                    | -                          |\\n| 1.0672     | 14000     | 2.0715        | -               | -                    | -                    | -                    | -                          |\\n| 1.1435     | 15000     | 2.073         | -               | -                    | -                    | -                    | -                          |\\n| 1.2197     | 16000     | 2.0699        | 2.0696          | 0.5532 (+0.0128)     | 0.3634 (+0.0384)     | 0.6355 (+0.1349)     | 0.5174 (+0.0620)           |\\n| 1.2959     | 17000     | 2.0671        | -               | -                    | -                    | -                    | -                          |\\n| 1.3722     | 18000     | 2.0682        | -               | -                    | -                    | -                    | -                          |\\n| 1.4484     | 19000     | 2.0702        | -               | -                    | -                    | -                    | -                          |\\n| **1.5246** | **20000** | **2.0699**    | **2.0695**      | **0.5787 (+0.0383)** | **0.3588 (+0.0337)** | **0.6221 (+0.1215)** | **0.5199 (+0.0645)**       |\\n| 1.6009     | 21000     | 2.0689        | -               | -                    | -                    | -                    | -                          |\\n| 1.6771     | 22000     | 2.0699        | -               | -                    | -                    | -                    | -                          |\\n| 1.7533     | 23000     | 2.0667        | -               | -                    | -                    | -                    | -                          |\\n| 1.8295     | 24000     | 2.0694        | 2.0694          | 0.5411 (+0.0006)     | 0.3817 (+0.0567)     | 0.6167 (+0.1161)     | 0.5132 (+0.0578)           |\\n| 1.9058     | 25000     | 2.0632        | -               | -                    | -                    | -                    | -                          |\\n| 1.9820     | 26000     | 2.0721        | -               | -                    | -                    | -                    | -                          |\\n| 2.0582     | 27000     | 2.0647        | -               | -                    | -                    | -                    | -                          |\\n| 2.1345     | 28000     | 2.0688        | 2.0702          | 0.5746 (+0.0342)     | 0.3845 (+0.0594)     | 0.5908 (+0.0901)     | 0.5166 (+0.0613)           |\\n| 2.2107     | 29000     | 2.0635        | -               | -                    | -                    | -                    | -                          |\\n| 2.2869     | 30000     | 2.0665        | -               | -                    | -                    | -                    | -                          |\\n| 2.3632     | 31000     | 2.0643        | -               | -                    | -                    | -                    | -                          |\\n| 2.4394     | 32000     | 2.0681        | 2.0699          | 0.5529 (+0.0125)     | 0.3690 (+0.0440)     | 0.5725 (+0.0718)     | 0.4981 (+0.0428)           |\\n| 2.5156     | 33000     | 2.0642        | -               | -                    | -                    | -                    | -                          |\\n| 2.5919     | 34000     | 2.0613        | -               | -                    | -                    | -                    | -                          |\\n| 2.6681     | 35000     | 2.0673        | -               | -                    | -                    | -                    | -                          |\\n| 2.7443     | 36000     | 2.0641        | 2.0696          | 0.5534 (+0.0130)     | 0.3701 (+0.0451)     | 0.5639 (+0.0632)     | 0.4958 (+0.0404)           |\\n| 2.8206     | 37000     | 2.0645        | -               | -                    | -                    | -                    | -                          |\\n| 2.8968     | 38000     | 2.0673        | -               | -                    | -                    | -                    | -                          |\\n| 2.9730     | 39000     | 2.0656        | -               | -                    | -                    | -                    | -                          |\\n| -1         | -1        | -             | -               | 0.5787 (+0.0383)     | 0.3588 (+0.0337)     | 0.6221 (+0.1215)     | 0.5199 (+0.0645)           |\\n\\n* The bold row denotes the saved checkpoint.\\n\\n### Environmental Impact\\nCarbon emissions were measured using [CodeCarbon](https://github.com/mlco2/codecarbon).\\n- **Energy Consumed**: 0.532 kWh\\n- **Carbon Emitted**: 0.207 kg of CO2\\n- **Hours Used**: 1.708 hours\\n\\n### Training Hardware\\n- **On Cloud**: No\\n- **GPU Model**: 1 x NVIDIA GeForce RTX 3090\\n- **CPU Model**: 13th Gen Intel(R) Core(TM) i7-13700K\\n- **RAM Size**: 31.78 GB\\n\\n### Framework Versions\\n- Python: 3.11.6\\n- Sentence Transformers: 3.5.0.dev0\\n- Transformers: 4.48.3\\n- PyTorch: 2.5.0+cu121\\n- Accelerate: 1.4.0\\n- Datasets: 3.3.2\\n- Tokenizers: 0.21.0\\n\\n## Citation\\n\\n### BibTeX\\n\\n#### Sentence Transformers\\n```bibtex\\n@inproceedings{reimers-2019-sentence-bert,\\n    title = \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\",\\n    author = \"Reimers, Nils and Gurevych, Iryna\",\\n    booktitle = \"Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\",\\n    month = \"11\",\\n    year = \"2019\",\\n    publisher = \"Association for Computational Linguistics\",\\n    url = \"https://arxiv.org/abs/1908.10084\",\\n}\\n```\\n\\n#### ListNetLoss\\n```bibtex\\n@inproceedings{cao2007learning,\\n    title={Learning to rank: from pairwise approach to listwise approach},\\n    author={Cao, Zhe and Qin, Tao and Liu, Tie-Yan and Tsai, Ming-Feng and Li, Hang},\\n    booktitle={Proceedings of the 24th international conference on Machine learning},\\n    pages={129--136},\\n    year={2007}\\n}\\n```\\n\\n<!--\\n## Glossary\\n\\n*Clearly define terms in order to be accessible across audiences.*\\n-->\\n\\n<!--\\n## Model Card Authors\\n\\n*Lists the people who create the model card, providing recognition and accountability for the detailed work that goes into its construction.*\\n-->\\n\\n<!--\\n## Model Card Contact\\n\\n*Provides a way for people who have updates to the Model Card, suggestions, or questions, to contact the Model Card authors.*\\n-->',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c241fcad08a0d12cca92',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:31.114000',\n",
       "  'date_modified': '2025-02-27T19:18:31.114000'},\n",
       " {'model_identifier': 'ksaksp/123',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:23:22',\n",
       "  'last_modified': '2025-02-27T17:23:22',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['license:apache-2.0', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: apache-2.0\\n---\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c240fcad08a0d12cca91',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:31.008000',\n",
       "  'date_modified': '2025-02-27T19:18:31.008000'},\n",
       " {'model_identifier': 'mradermacher/MistralThinker-v1.1-i1-GGUF',\n",
       "  'version': 3,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:06:33',\n",
       "  'last_modified': '2025-02-27T17:23:37',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': True,\n",
       "  'tags': ['gguf',\n",
       "   'endpoints_compatible',\n",
       "   'region:us',\n",
       "   'imatrix',\n",
       "   'conversational'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A high-end GPU with 24GB VRAM (e.g., NVIDIA RTX 3090, A5000).',\n",
       "   'memory_in_gb': 21.953510284423828},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '<!-- ### quantize_version: 2 -->\\n<!-- ### output_tensor_quantised: 1 -->\\n<!-- ### convert_type: hf -->\\n<!-- ### vocab_type:  -->\\n<!-- ### tags: nicoboss -->\\nweighted/imatrix quants of https://huggingface.co/Undi95/MistralThinker-v1.1\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c23ffcad08a0d12cca90',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:31.008000',\n",
       "  'date_modified': '2025-02-27T19:18:31.008000'},\n",
       " {'model_identifier': 'NeuralTofu/zombies-n-gorillas-v3',\n",
       "  'version': 4,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:22:31',\n",
       "  'last_modified': '2025-02-27T17:23:49',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'text-generation-inference',\n",
       "   'unsloth',\n",
       "   'trl',\n",
       "   'conversational',\n",
       "   'en',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 14.18519115447998},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nbase_model: unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit\\ntags:\\n- text-generation-inference\\n- transformers\\n- unsloth\\n- qwen2\\n- trl\\nlicense: apache-2.0\\nlanguage:\\n- en\\n---\\n\\n# Uploaded  model\\n\\n- **Developed by:** NeuralTofu\\n- **License:** apache-2.0\\n- **Finetuned from model :** unsloth/qwen2.5-7b-instruct-unsloth-bnb-4bit\\n\\nThis qwen2 model was trained 2x faster with [Unsloth](https://github.com/unslothai/unsloth) and Huggingface\\'s TRL library.\\n\\n[<img src=\"https://raw.githubusercontent.com/unslothai/unsloth/main/images/unsloth%20made%20with%20love.png\" width=\"200\"/>](https://github.com/unslothai/unsloth)\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c23efcad08a0d12cca8f',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:31.008000',\n",
       "  'date_modified': '2025-02-27T19:18:31.008000'},\n",
       " {'model_identifier': 'whiteapple8222/c4c86c5b-4023-4cd7-8deb-cfb30651e1de',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:24:10',\n",
       "  'last_modified': '2025-02-27T17:24:10',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c23bfcad08a0d12cca8d',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:31.008000',\n",
       "  'date_modified': '2025-02-27T19:18:31.008000'},\n",
       " {'model_identifier': 'TobiGeth/tg_user_489475181_lora_1740674892',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:24:27',\n",
       "  'last_modified': '2025-02-27T17:24:29',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['diffusers',\n",
       "   'flux',\n",
       "   'lora',\n",
       "   'replicate',\n",
       "   'text-to-image',\n",
       "   'en',\n",
       "   'base_model:black-forest-labs/FLUX.1-dev',\n",
       "   'base_model:adapter:black-forest-labs/FLUX.1-dev',\n",
       "   'license:other',\n",
       "   'region:us'],\n",
       "  'task': ['text-to-image'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'description': '---\\nlicense: other\\nlicense_name: flux-1-dev-non-commercial-license\\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md\\nlanguage:\\n- en\\ntags:\\n- flux\\n- diffusers\\n- lora\\n- replicate\\nbase_model: \"black-forest-labs/FLUX.1-dev\"\\npipeline_tag: text-to-image\\n# widget:\\n#   - text: >-\\n#       prompt\\n#     output:\\n#       url: https://...\\ninstance_prompt: USER_489475181_1740674892\\n---\\n\\n# Tg_User_489475181_Lora_1740674892\\n\\n<Gallery />\\n\\nTrained on Replicate using:\\n\\nhttps://replicate.com/ostris/flux-dev-lora-trainer/train\\n\\n\\n## Trigger words\\nYou should use `USER_489475181_1740674892` to trigger the image generation.\\n\\n\\n## Use it with the [🧨 diffusers library](https://github.com/huggingface/diffusers)\\n\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2Image.from_pretrained(\\'black-forest-labs/FLUX.1-dev\\', torch_dtype=torch.float16).to(\\'cuda\\')\\npipeline.load_lora_weights(\\'TobiGeth/tg_user_489475181_lora_1740674892\\', weight_name=\\'lora.safetensors\\')\\nimage = pipeline(\\'your prompt\\').images[0]\\n```\\n\\nFor more details, including weighting, merging and fusing LoRAs, check the [documentation on loading LoRAs in diffusers](https://huggingface.co/docs/diffusers/main/en/using-diffusers/loading_adapters)\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c23afcad08a0d12cca8c',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:31.008000',\n",
       "  'date_modified': '2025-02-27T19:18:31.008000'},\n",
       " {'model_identifier': 'mama-to/puppy1',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:03:41',\n",
       "  'last_modified': '2025-02-27T17:24:52',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'llama', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 14.957527160644531},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c239fcad08a0d12cca8b',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:31.008000',\n",
       "  'date_modified': '2025-02-27T19:18:31.008000'},\n",
       " {'model_identifier': 'dabrown/9121d07e-722a-45e9-a97a-cba66b1b47dd',\n",
       "  'version': 10,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T16:40:21',\n",
       "  'last_modified': '2025-02-27T17:25:07',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['peft',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'axolotl',\n",
       "   'generated_from_trainer',\n",
       "   'base_model:Qwen/Qwen2.5-1.5B-Instruct',\n",
       "   'base_model:adapter:Qwen/Qwen2.5-1.5B-Instruct',\n",
       "   'license:apache-2.0',\n",
       "   'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlibrary_name: peft\\nlicense: apache-2.0\\nbase_model: Qwen/Qwen2.5-1.5B-Instruct\\ntags:\\n- axolotl\\n- generated_from_trainer\\nmodel-index:\\n- name: 9121d07e-722a-45e9-a97a-cba66b1b47dd\\n  results: []\\n---\\n\\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n[<img src=\"https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/axolotl-ai-cloud/axolotl)\\n<details><summary>See axolotl config</summary>\\n\\naxolotl version: `0.5.2`\\n```yaml\\nadapter: lora\\nbase_model: Qwen/Qwen2.5-1.5B-Instruct\\nbf16: true\\nchat_template: llama3\\ndataset_prepared_path: null\\ndatasets:\\n- data_files:\\n  - 70885662a4657c52_train_data.json\\n  ds_type: json\\n  format: custom\\n  path: /workspace/input_data/70885662a4657c52_train_data.json\\n  type:\\n    field_input: instruction\\n    field_instruction: input\\n    field_output: output\\n    format: \\'{instruction} {input}\\'\\n    no_input_format: \\'{instruction}\\'\\n    system_format: \\'{system}\\'\\n    system_prompt: \\'\\'\\ndebug: null\\ndeepspeed: null\\nearly_stopping_patience: null\\neval_max_new_tokens: 128\\neval_table_size: null\\nevals_per_epoch: 4\\nflash_attention: false\\nfp16: false\\nfsdp: null\\nfsdp_config: null\\ngradient_accumulation_steps: 16\\ngradient_checkpointing: false\\ngroup_by_length: true\\nhub_model_id: dabrown/9121d07e-722a-45e9-a97a-cba66b1b47dd\\nhub_repo: null\\nhub_strategy: checkpoint\\nhub_token: null\\nlearning_rate: 0.0002\\nload_in_4bit: false\\nload_in_8bit: false\\nlocal_rank: null\\nlogging_steps: 1\\nlora_alpha: 16\\nlora_dropout: 0.05\\nlora_fan_in_fan_out: false\\nlora_inference_mode: true\\nlora_model_dir: null\\nlora_r: 8\\nlora_target_linear: true\\nlr_scheduler: cosine\\nmax_grad_norm: 1.0\\nmax_steps: 1500\\nmicro_batch_size: 2\\nmlflow_experiment_name: /tmp/70885662a4657c52_train_data.json\\nmodel_type: AutoModelForCausalLM\\nmodules_to_save: lm_head\\nnum_epochs: 1\\noptimizer: adamw_bnb_8bit\\noutput_dir: miner_id_24\\npad_to_sequence_len: true\\npeft_use_rslora: true\\nresume_from_checkpoint: null\\ns2_attention: null\\nsample_packing: false\\nsaves_per_epoch: 4\\nsequence_len: 1024\\nstrict: false\\ntf32: true\\ntokenizer_type: AutoTokenizer\\ntrain_on_inputs: false\\ntrust_remote_code: true\\nval_set_size: 0.05\\nwandb_entity: null\\nwandb_mode: offline\\nwandb_name: 9c9c1696-fddf-412a-a23d-9af232ad64b9\\nwandb_project: Gradients-On-Demand\\nwandb_run: your_name\\nwandb_runid: 9c9c1696-fddf-412a-a23d-9af232ad64b9\\nwarmup_steps: 10\\nweight_decay: 0.0\\nxformers_attention: null\\n\\n```\\n\\n</details><br>\\n\\n# 9121d07e-722a-45e9-a97a-cba66b1b47dd\\n\\nThis model is a fine-tuned version of [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) on the None dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: 1.3688\\n\\n## Model description\\n\\nMore information needed\\n\\n## Intended uses & limitations\\n\\nMore information needed\\n\\n## Training and evaluation data\\n\\nMore information needed\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 0.0002\\n- train_batch_size: 2\\n- eval_batch_size: 2\\n- seed: 42\\n- gradient_accumulation_steps: 16\\n- total_train_batch_size: 32\\n- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\\n- lr_scheduler_type: cosine\\n- lr_scheduler_warmup_steps: 10\\n- training_steps: 774\\n\\n### Training results\\n\\n| Training Loss | Epoch  | Step | Validation Loss |\\n|:-------------:|:------:|:----:|:---------------:|\\n| 1.6608        | 0.0013 | 1    | 2.2057          |\\n| 2.0662        | 0.2508 | 194  | 1.4467          |\\n| 1.4647        | 0.5016 | 388  | 1.3877          |\\n| 1.4299        | 0.7524 | 582  | 1.3688          |\\n\\n\\n### Framework versions\\n\\n- PEFT 0.13.2\\n- Transformers 4.46.3\\n- Pytorch 2.3.1+cu121\\n- Datasets 3.1.0\\n- Tokenizers 0.20.3',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c238fcad08a0d12cca8a',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.894000',\n",
       "  'date_modified': '2025-02-27T19:18:30.894000'},\n",
       " {'model_identifier': 'mohammedrrrrr/iki',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:25:28',\n",
       "  'last_modified': '2025-02-27T17:25:28',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['license:apache-2.0', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: apache-2.0\\n---\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c237fcad08a0d12cca89',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.788000',\n",
       "  'date_modified': '2025-02-27T19:18:30.788000'},\n",
       " {'model_identifier': 'Tipu8606/tipu',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:25:32',\n",
       "  'last_modified': '2025-02-27T17:25:32',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['license:mit', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'mit'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: mit\\n---\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c236fcad08a0d12cca88',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.687000',\n",
       "  'date_modified': '2025-02-27T19:18:30.687000'},\n",
       " {'model_identifier': 'ustchhy/Qwen2.5-1.5B-Open-R1-Distill',\n",
       "  'version': 10,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T01:54:43',\n",
       "  'last_modified': '2025-02-27T17:25:54',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'generated_from_trainer',\n",
       "   'open-r1',\n",
       "   'trl',\n",
       "   'sft',\n",
       "   'conversational',\n",
       "   'dataset:open-r1/OpenR1-Math-220k',\n",
       "   'base_model:Qwen/Qwen2.5-1.5B-Instruct',\n",
       "   'base_model:finetune:Qwen/Qwen2.5-1.5B-Instruct',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 2.875391960144043},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nbase_model: Qwen/Qwen2.5-1.5B-Instruct\\ndatasets: open-r1/OpenR1-Math-220k\\nlibrary_name: transformers\\nmodel_name: Qwen2.5-1.5B-Open-R1-Distill\\ntags:\\n- generated_from_trainer\\n- open-r1\\n- trl\\n- sft\\nlicence: license\\n---\\n\\n# Model Card for Qwen2.5-1.5B-Open-R1-Distill\\n\\nThis model is a fine-tuned version of [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) on the [open-r1/OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) dataset.\\nIt has been trained using [TRL](https://github.com/huggingface/trl).\\n\\n## Quick start\\n\\n```python\\nfrom transformers import pipeline\\n\\nquestion = \"If you had a time machine, but could only go to the past or the future once and never return, which would you choose and why?\"\\ngenerator = pipeline(\"text-generation\", model=\"ustchhy/Qwen2.5-1.5B-Open-R1-Distill\", device=\"cuda\")\\noutput = generator([{\"role\": \"user\", \"content\": question}], max_new_tokens=128, return_full_text=False)[0]\\nprint(output[\"generated_text\"])\\n```\\n\\n## Training procedure\\n\\n[<img src=\"https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-28.svg\" alt=\"Visualize in Weights & Biases\" width=\"150\" height=\"24\"/>](https://wandb.ai/dse_graph/huggingface/runs/1s4gczw8) \\n\\n\\nThis model was trained with SFT.\\n\\n### Framework versions\\n\\n- TRL: 0.16.0.dev0\\n- Transformers: 4.49.0\\n- Pytorch: 2.5.1\\n- Datasets: 3.3.2\\n- Tokenizers: 0.21.0\\n\\n## Citations\\n\\n\\n\\nCite TRL as:\\n    \\n```bibtex\\n@misc{vonwerra2022trl,\\n\\ttitle        = {{TRL: Transformer Reinforcement Learning}},\\n\\tauthor       = {Leandro von Werra and Younes Belkada and Lewis Tunstall and Edward Beeching and Tristan Thrush and Nathan Lambert and Shengyi Huang and Kashif Rasul and Quentin Gallouédec},\\n\\tyear         = 2020,\\n\\tjournal      = {GitHub repository},\\n\\tpublisher    = {GitHub},\\n\\thowpublished = {\\\\url{https://github.com/huggingface/trl}}\\n}\\n```',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c235fcad08a0d12cca87',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.687000',\n",
       "  'date_modified': '2025-02-27T19:18:30.687000'},\n",
       " {'model_identifier': 'seastar105/Phi-4-mm-inst-zeroth-kor',\n",
       "  'version': 13,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T12:27:19',\n",
       "  'last_modified': '2025-02-27T17:26:44',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors',\n",
       "   'phi4mm',\n",
       "   'custom_code',\n",
       "   'ko',\n",
       "   'dataset:kresnik/zeroth_korean',\n",
       "   'base_model:microsoft/Phi-4-multimodal-instruct',\n",
       "   'base_model:finetune:microsoft/Phi-4-multimodal-instruct',\n",
       "   'model-index',\n",
       "   'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 10.383241653442383},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\ndatasets:\\n- kresnik/zeroth_korean\\nmetrics:\\n- bleu\\n- cer\\nbase_model:\\n- microsoft/Phi-4-multimodal-instruct\\nmodel-index:\\n- name: Phi-4-mm-inst-zeroth-kor\\n  results:\\n  - task:\\n      type: speech-to-text-translation\\n    dataset:\\n      type: seastar105/fleurs_ko_en_test\\n      name: fleurs (ko-en test intersection)\\n    metrics:\\n    - type: bleu\\n      name: ko2en\\n      value: To-be-filled\\n    - type: bleu\\n      name: ko2en-cot\\n      value: To-be-filled\\n    - type: bleu\\n      name: en2ko (ko-mecab)\\n      value: To-be-filled\\n    - type: bleu\\n      name: en2ko-cot (ko-mecab)\\n      value: To-be-filled\\n  - task:\\n      type: automatic-speech-recognition\\n    dataset:\\n      type: kresnik/zeroth_korean\\n      name: zeroth_korean test\\n    metrics:\\n    - type: cer\\n      name: test CER\\n      value: To-be-filled\\nlanguage:\\n- ko\\n---\\nThis model is fine-tuned from [microsoft/Phi-4-multimodal-instruct](https://huggingface.co/microsoft/Phi-4-multimodal-instruct) on [kresnik/zeroth_korean](https://huggingface.co/datasets/kresnik/zeroth_korean) dataset only 1 epoch.\\n\\nscript for fine-tuning is [here](https://gist.github.com/seastar105/d1d8983b27611370528e3b194dcc5577#file-main-py), adapted from phi-4 repository example\\n\\nmodel is trained only 174 steps on zeroth train set, and main purpose is to check if only korean ASR training can expand to other speech tasks(e.g. speech-to-text-translation)\\n\\n## Evaluation\\n\\nASR on zeroth-test set and fleurs ko <-> en speech translation result will be filled.',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c232fcad08a0d12cca85',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.680000',\n",
       "  'date_modified': '2025-02-27T19:18:30.680000'},\n",
       " {'model_identifier': 'Paladiso/bb1f33a6-70a4-4d3f-95ca-3096bd3f4697',\n",
       "  'version': 7,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:15:39',\n",
       "  'last_modified': '2025-02-27T17:27:38',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['peft',\n",
       "   'safetensors',\n",
       "   'llama',\n",
       "   'axolotl',\n",
       "   'generated_from_trainer',\n",
       "   'base_model:deepseek-ai/deepseek-coder-6.7b-instruct',\n",
       "   'base_model:adapter:deepseek-ai/deepseek-coder-6.7b-instruct',\n",
       "   'license:other',\n",
       "   'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'other'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlibrary_name: peft\\nlicense: other\\nbase_model: deepseek-ai/deepseek-coder-6.7b-instruct\\ntags:\\n- axolotl\\n- generated_from_trainer\\nmodel-index:\\n- name: bb1f33a6-70a4-4d3f-95ca-3096bd3f4697\\n  results: []\\n---\\n\\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n[<img src=\"https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/axolotl-ai-cloud/axolotl)\\n<details><summary>See axolotl config</summary>\\n\\naxolotl version: `0.4.1`\\n```yaml\\nadapter: lora\\nbase_model: deepseek-ai/deepseek-coder-6.7b-instruct\\nbf16: auto\\nchat_template: llama3\\ndataset_prepared_path: null\\ndatasets:\\n- data_files:\\n  - a801c7376be2f96e_train_data.json\\n  ds_type: json\\n  format: custom\\n  path: /workspace/input_data/a801c7376be2f96e_train_data.json\\n  type:\\n    field_instruction: instruction\\n    field_output: output\\n    format: \\'{instruction}\\'\\n    no_input_format: \\'{instruction}\\'\\n    system_format: \\'{system}\\'\\n    system_prompt: \\'\\'\\ndebug: null\\ndeepspeed: null\\nearly_stopping_patience: null\\neval_max_new_tokens: 128\\neval_table_size: null\\nevals_per_epoch: 4\\nflash_attention: false\\nfp16: null\\nfsdp: null\\nfsdp_config: null\\ngradient_accumulation_steps: 4\\ngradient_checkpointing: false\\ngroup_by_length: false\\nhub_model_id: Paladiso/bb1f33a6-70a4-4d3f-95ca-3096bd3f4697\\nhub_repo: null\\nhub_strategy: checkpoint\\nhub_token: null\\nlearning_rate: 0.0002\\nload_in_4bit: false\\nload_in_8bit: false\\nlocal_rank: null\\nlogging_steps: 1\\nlora_alpha: 16\\nlora_dropout: 0.05\\nlora_fan_in_fan_out: null\\nlora_model_dir: null\\nlora_r: 8\\nlora_target_linear: true\\nlr_scheduler: cosine\\nmax_steps: 10\\nmicro_batch_size: 2\\nmlflow_experiment_name: /tmp/a801c7376be2f96e_train_data.json\\nmodel_type: AutoModelForCausalLM\\nnum_epochs: 1\\noptimizer: adamw_bnb_8bit\\noutput_dir: miner_id_24\\npad_to_sequence_len: true\\nresume_from_checkpoint: null\\ns2_attention: null\\nsample_packing: false\\nsaves_per_epoch: 4\\nsequence_len: 512\\nstrict: false\\ntf32: false\\ntokenizer_type: AutoTokenizer\\ntrain_on_inputs: false\\ntrust_remote_code: true\\nval_set_size: 0.05\\nwandb_entity: null\\nwandb_mode: online\\nwandb_name: 4a97ae51-a91e-49f3-b415-2bcb8b1d68e5\\nwandb_project: Gradients-On-Demand\\nwandb_run: your_name\\nwandb_runid: 4a97ae51-a91e-49f3-b415-2bcb8b1d68e5\\nwarmup_steps: 10\\nweight_decay: 0.0\\nxformers_attention: null\\n\\n```\\n\\n</details><br>\\n\\n# bb1f33a6-70a4-4d3f-95ca-3096bd3f4697\\n\\nThis model is a fine-tuned version of [deepseek-ai/deepseek-coder-6.7b-instruct](https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct) on the None dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: nan\\n\\n## Model description\\n\\nMore information needed\\n\\n## Intended uses & limitations\\n\\nMore information needed\\n\\n## Training and evaluation data\\n\\nMore information needed\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 0.0002\\n- train_batch_size: 2\\n- eval_batch_size: 2\\n- seed: 42\\n- gradient_accumulation_steps: 4\\n- total_train_batch_size: 8\\n- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\\n- lr_scheduler_type: cosine\\n- lr_scheduler_warmup_steps: 10\\n- training_steps: 10\\n\\n### Training results\\n\\n| Training Loss | Epoch  | Step | Validation Loss |\\n|:-------------:|:------:|:----:|:---------------:|\\n| 0.0           | 0.0001 | 1    | nan             |\\n| 0.0           | 0.0002 | 3    | nan             |\\n| 0.0           | 0.0004 | 6    | nan             |\\n| 0.0           | 0.0006 | 9    | nan             |\\n\\n\\n### Framework versions\\n\\n- PEFT 0.13.2\\n- Transformers 4.46.0\\n- Pytorch 2.5.0+cu124\\n- Datasets 3.0.1\\n- Tokenizers 0.20.1',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c231fcad08a0d12cca84',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.572000',\n",
       "  'date_modified': '2025-02-27T19:18:30.572000'},\n",
       " {'model_identifier': 'Hishambarakat/checkpoint',\n",
       "  'version': 110,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2024-09-26T10:36:12',\n",
       "  'last_modified': '2025-02-27T17:28:35',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['diffusers',\n",
       "   'safetensors',\n",
       "   'autotrain_compatible',\n",
       "   'endpoints_compatible',\n",
       "   'diffusers:StableDiffusionXLPipeline',\n",
       "   'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 1021.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c22afcad08a0d12cca82',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.569000',\n",
       "  'date_modified': '2025-02-27T19:18:30.569000'},\n",
       " {'model_identifier': 'MrRobotoAI/165',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:24:40',\n",
       "  'last_modified': '2025-02-27T17:28:42',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'llama',\n",
       "   'text-generation',\n",
       "   'mergekit',\n",
       "   'merge',\n",
       "   'conversational',\n",
       "   'arxiv:2403.19522',\n",
       "   'base_model:MrRobotoAI/155',\n",
       "   'base_model:merge:MrRobotoAI/155',\n",
       "   'base_model:MrRobotoAI/156',\n",
       "   'base_model:merge:MrRobotoAI/156',\n",
       "   'base_model:MrRobotoAI/157',\n",
       "   'base_model:merge:MrRobotoAI/157',\n",
       "   'base_model:MrRobotoAI/160',\n",
       "   'base_model:merge:MrRobotoAI/160',\n",
       "   'base_model:MrRobotoAI/163',\n",
       "   'base_model:merge:MrRobotoAI/163',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 14.957527160644531},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nbase_model:\\n- MrRobotoAI/157\\n- MrRobotoAI/160\\n- MrRobotoAI/155\\n- MrRobotoAI/156\\n- MrRobotoAI/163\\nlibrary_name: transformers\\ntags:\\n- mergekit\\n- merge\\n\\n---\\n# merge\\n\\nThis is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).\\n\\n## Merge Details\\n### Merge Method\\n\\nThis model was merged using the [Model Stock](https://arxiv.org/abs/2403.19522) merge method using [MrRobotoAI/160](https://huggingface.co/MrRobotoAI/160) as a base.\\n\\n### Models Merged\\n\\nThe following models were included in the merge:\\n* [MrRobotoAI/157](https://huggingface.co/MrRobotoAI/157)\\n* [MrRobotoAI/155](https://huggingface.co/MrRobotoAI/155)\\n* [MrRobotoAI/156](https://huggingface.co/MrRobotoAI/156)\\n* [MrRobotoAI/163](https://huggingface.co/MrRobotoAI/163)\\n\\n### Configuration\\n\\nThe following YAML configuration was used to produce this model:\\n\\n```yaml\\nmodels:\\n  - model: MrRobotoAI/155\\n  - model: MrRobotoAI/156\\n  - model: MrRobotoAI/157\\n  - model: MrRobotoAI/160\\n  - model: MrRobotoAI/163\\nmerge_method: model_stock\\nbase_model: MrRobotoAI/160\\nnormalize: true\\ndtype: float16\\n```\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c229fcad08a0d12cca81',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.569000',\n",
       "  'date_modified': '2025-02-27T19:18:30.569000'},\n",
       " {'model_identifier': 'cutelemonlili/Qwen2.5-3B-Instruct_Lean_Code',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:27:18',\n",
       "  'last_modified': '2025-02-27T17:28:58',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'llama-factory',\n",
       "   'full',\n",
       "   'generated_from_trainer',\n",
       "   'conversational',\n",
       "   'base_model:Qwen/Qwen2.5-3B-Instruct',\n",
       "   'base_model:finetune:Qwen/Qwen2.5-3B-Instruct',\n",
       "   'license:other',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 5.748008728027344},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'other'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nlibrary_name: transformers\\nlicense: other\\nbase_model: Qwen/Qwen2.5-3B-Instruct\\ntags:\\n- llama-factory\\n- full\\n- generated_from_trainer\\nmodel-index:\\n- name: Lean_Code_data\\n  results: []\\n---\\n\\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n# Lean_Code_data\\n\\nThis model is a fine-tuned version of [Qwen/Qwen2.5-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct) on the Lean_Code_data dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: 0.2278\\n\\n## Model description\\n\\nMore information needed\\n\\n## Intended uses & limitations\\n\\nMore information needed\\n\\n## Training and evaluation data\\n\\nMore information needed\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 1e-05\\n- train_batch_size: 2\\n- eval_batch_size: 1\\n- seed: 42\\n- distributed_type: multi-GPU\\n- num_devices: 4\\n- total_train_batch_size: 8\\n- total_eval_batch_size: 4\\n- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\\n- lr_scheduler_type: cosine\\n- num_epochs: 2\\n\\n### Training results\\n\\n\\n\\n### Framework versions\\n\\n- Transformers 4.46.1\\n- Pytorch 2.6.0+cu124\\n- Datasets 3.1.0\\n- Tokenizers 0.20.3\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c227fcad08a0d12cca80',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.569000',\n",
       "  'date_modified': '2025-02-27T19:18:30.569000'},\n",
       " {'model_identifier': 'CrimsonZockt/MilenaTakuska-FLUXLORA',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:29:19',\n",
       "  'last_modified': '2025-02-27T17:29:40',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['diffusers',\n",
       "   'text-to-image',\n",
       "   'lora',\n",
       "   'template:diffusion-lora',\n",
       "   'base_model:black-forest-labs/FLUX.1-dev',\n",
       "   'base_model:adapter:black-forest-labs/FLUX.1-dev',\n",
       "   'region:us'],\n",
       "  'task': ['text-to-image'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'description': '---\\ntags:\\n  - text-to-image\\n  - lora\\n  - diffusers\\n  - template:diffusion-lora\\nwidget:\\n- text: >-\\n    photoshoot of Milena Takuska, female, woman, solo, black tanktop,\\n    professional headshot.\\n  output:\\n    url: images/photoshoot of Milena Takuska, female, woman, so....png\\nbase_model: black-forest-labs/FLUX.1-dev\\ninstance_prompt: Milena Takuska\\n\\n---\\n# MilenaTakuska\\n\\n<Gallery />\\n\\n## Model description \\n\\nThis is a LORA Model that i have train on Weights.gg\\n\\n## Trigger words\\n\\nYou should use `Milena Takuska` to trigger the image generation.\\n\\n\\n## Download model\\n\\nWeights for this model are available in Safetensors format.\\n\\n[Download](/CrimsonZockt/MilenaTakuska-FLUXLORA/tree/main) them in the Files & versions tab.\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c225fcad08a0d12cca7f',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.462000',\n",
       "  'date_modified': '2025-02-27T19:18:30.462000'},\n",
       " {'model_identifier': 'mradermacher/ChatParts-llama3.1-8b-GGUF',\n",
       "  'version': 16,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-23T00:29:19',\n",
       "  'last_modified': '2025-02-27T17:30:12',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': True,\n",
       "  'tags': ['transformers',\n",
       "   'gguf',\n",
       "   'biology',\n",
       "   'medical',\n",
       "   'en',\n",
       "   'dataset:shellwork/ChatParts_Dataset',\n",
       "   'base_model:shellwork/ChatParts-llama3.1-8b',\n",
       "   'base_model:quantized:shellwork/ChatParts-llama3.1-8b',\n",
       "   'license:llama3.1',\n",
       "   'endpoints_compatible',\n",
       "   'region:us',\n",
       "   'conversational'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 7.478763580322266},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'llama3.1'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': \"---\\nbase_model: shellwork/ChatParts-llama3.1-8b\\ndatasets:\\n- shellwork/ChatParts_Dataset\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: llama3.1\\nquantized_by: mradermacher\\ntags:\\n- biology\\n- medical\\n---\\n## About\\n\\n<!-- ### quantize_version: 2 -->\\n<!-- ### output_tensor_quantised: 1 -->\\n<!-- ### convert_type: hf -->\\n<!-- ### vocab_type:  -->\\n<!-- ### tags:  -->\\nstatic quants of https://huggingface.co/shellwork/ChatParts-llama3.1-8b\\n\\n<!-- provided-files -->\\nweighted/imatrix quants are available at https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-i1-GGUF\\n## Usage\\n\\nIf you are unsure how to use GGUF files, refer to one of [TheBloke's\\nREADMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for\\nmore details, including on how to concatenate multi-part files.\\n\\n## Provided Quants\\n\\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\\n\\n| Link | Type | Size/GB | Notes |\\n|:-----|:-----|--------:|:------|\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.Q2_K.gguf) | Q2_K | 3.3 |  |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.Q3_K_S.gguf) | Q3_K_S | 3.8 |  |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.Q3_K_M.gguf) | Q3_K_M | 4.1 | lower quality |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.Q3_K_L.gguf) | Q3_K_L | 4.4 |  |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.IQ4_XS.gguf) | IQ4_XS | 4.6 |  |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.Q4_K_S.gguf) | Q4_K_S | 4.8 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.Q4_K_M.gguf) | Q4_K_M | 5.0 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.Q5_K_S.gguf) | Q5_K_S | 5.7 |  |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.Q5_K_M.gguf) | Q5_K_M | 5.8 |  |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.Q6_K.gguf) | Q6_K | 6.7 | very good quality |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.Q8_0.gguf) | Q8_0 | 8.6 | fast, best quality |\\n| [GGUF](https://huggingface.co/mradermacher/ChatParts-llama3.1-8b-GGUF/resolve/main/ChatParts-llama3.1-8b.f16.gguf) | f16 | 16.2 | 16 bpw, overkill |\\n\\nHere is a handy graph by ikawrakow comparing some lower-quality quant\\ntypes (lower is better):\\n\\n![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)\\n\\nAnd here are Artefact2's thoughts on the matter:\\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\\n\\n## FAQ / Model Request\\n\\nSee https://huggingface.co/mradermacher/model_requests for some answers to\\nquestions you might have and/or if you want some other model quantized.\\n\\n## Thanks\\n\\nI thank my company, [nethype GmbH](https://www.nethype.de/), for letting\\nme use its servers and providing upgrades to my workstation to enable\\nthis work in my free time.\\n\\n<!-- end -->\\n\",\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 334.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c223fcad08a0d12cca7d',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.348000',\n",
       "  'date_modified': '2025-02-27T19:18:30.348000'},\n",
       " {'model_identifier': 'cutelemonlili/Qwen2.5-1.5B-Instruct_Lean_Code',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:29:10',\n",
       "  'last_modified': '2025-02-27T17:30:19',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'llama-factory',\n",
       "   'full',\n",
       "   'generated_from_trainer',\n",
       "   'conversational',\n",
       "   'base_model:Qwen/Qwen2.5-1.5B-Instruct',\n",
       "   'base_model:finetune:Qwen/Qwen2.5-1.5B-Instruct',\n",
       "   'license:other',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 2.875391960144043},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'other'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nlibrary_name: transformers\\nlicense: other\\nbase_model: Qwen/Qwen2.5-1.5B-Instruct\\ntags:\\n- llama-factory\\n- full\\n- generated_from_trainer\\nmodel-index:\\n- name: Lean_Code_data\\n  results: []\\n---\\n\\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n# Lean_Code_data\\n\\nThis model is a fine-tuned version of [Qwen/Qwen2.5-1.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct) on the Lean_Code_data dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: 0.2442\\n\\n## Model description\\n\\nMore information needed\\n\\n## Intended uses & limitations\\n\\nMore information needed\\n\\n## Training and evaluation data\\n\\nMore information needed\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 1e-05\\n- train_batch_size: 2\\n- eval_batch_size: 1\\n- seed: 42\\n- distributed_type: multi-GPU\\n- num_devices: 4\\n- total_train_batch_size: 8\\n- total_eval_batch_size: 4\\n- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\\n- lr_scheduler_type: cosine\\n- num_epochs: 2\\n\\n### Training results\\n\\n\\n\\n### Framework versions\\n\\n- Transformers 4.46.1\\n- Pytorch 2.6.0+cu124\\n- Datasets 3.1.0\\n- Tokenizers 0.20.3\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c222fcad08a0d12cca7c',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.348000',\n",
       "  'date_modified': '2025-02-27T19:18:30.348000'},\n",
       " {'model_identifier': 'cutelemonlili/Qwen2.5-0.5B-Instruct_Lean_Code',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:30:22',\n",
       "  'last_modified': '2025-02-27T17:30:45',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'llama-factory',\n",
       "   'full',\n",
       "   'generated_from_trainer',\n",
       "   'conversational',\n",
       "   'base_model:Qwen/Qwen2.5-0.5B-Instruct',\n",
       "   'base_model:finetune:Qwen/Qwen2.5-0.5B-Instruct',\n",
       "   'license:other',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.9202077388763428},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'other'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nlibrary_name: transformers\\nlicense: other\\nbase_model: Qwen/Qwen2.5-0.5B-Instruct\\ntags:\\n- llama-factory\\n- full\\n- generated_from_trainer\\nmodel-index:\\n- name: Lean_Code_data\\n  results: []\\n---\\n\\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n# Lean_Code_data\\n\\nThis model is a fine-tuned version of [Qwen/Qwen2.5-0.5B-Instruct](https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct) on the Lean_Code_data dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: 0.2902\\n\\n## Model description\\n\\nMore information needed\\n\\n## Intended uses & limitations\\n\\nMore information needed\\n\\n## Training and evaluation data\\n\\nMore information needed\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 1e-05\\n- train_batch_size: 2\\n- eval_batch_size: 1\\n- seed: 42\\n- distributed_type: multi-GPU\\n- num_devices: 4\\n- total_train_batch_size: 8\\n- total_eval_batch_size: 4\\n- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\\n- lr_scheduler_type: cosine\\n- num_epochs: 2\\n\\n### Training results\\n\\n\\n\\n### Framework versions\\n\\n- Transformers 4.46.1\\n- Pytorch 2.6.0+cu124\\n- Datasets 3.1.0\\n- Tokenizers 0.20.3\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c21efcad08a0d12cca7a',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.347000',\n",
       "  'date_modified': '2025-02-27T19:18:30.347000'},\n",
       " {'model_identifier': 'mradermacher/OpenCoder-8B-Instruct-i1-GGUF',\n",
       "  'version': 30,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2024-11-09T02:29:44',\n",
       "  'last_modified': '2025-02-27T17:31:20',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': True,\n",
       "  'tags': ['transformers',\n",
       "   'gguf',\n",
       "   'en',\n",
       "   'zh',\n",
       "   'dataset:OpenCoder-LLM/opencoder-sft-stage1',\n",
       "   'dataset:OpenCoder-LLM/opencoder-sft-stage2',\n",
       "   'base_model:infly/OpenCoder-8B-Instruct',\n",
       "   'base_model:quantized:infly/OpenCoder-8B-Instruct',\n",
       "   'license:other',\n",
       "   'endpoints_compatible',\n",
       "   'region:us',\n",
       "   'imatrix',\n",
       "   'conversational'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 7.237552642822266},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': \"---\\nbase_model: infly/OpenCoder-8B-Instruct\\ndatasets:\\n- OpenCoder-LLM/opencoder-sft-stage1\\n- OpenCoder-LLM/opencoder-sft-stage2\\nlanguage:\\n- en\\n- zh\\nlibrary_name: transformers\\nlicense: other\\nlicense_link: https://huggingface.co/infly/OpenCoder-8B-Instruct/blob/main/LICENSE\\nlicense_name: inf\\nquantized_by: mradermacher\\n---\\n## About\\n\\n<!-- ### quantize_version: 2 -->\\n<!-- ### output_tensor_quantised: 1 -->\\n<!-- ### convert_type: hf -->\\n<!-- ### vocab_type:  -->\\n<!-- ### tags: nicoboss -->\\nweighted/imatrix quants of https://huggingface.co/infly/OpenCoder-8B-Instruct\\n\\n<!-- provided-files -->\\n## Usage\\n\\nIf you are unsure how to use GGUF files, refer to one of [TheBloke's\\nREADMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for\\nmore details, including on how to concatenate multi-part files.\\n\\n## Provided Quants\\n\\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\\n\\n| Link | Type | Size/GB | Notes |\\n|:-----|:-----|--------:|:------|\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ1_S.gguf) | i1-IQ1_S | 2.0 | for the desperate |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ1_M.gguf) | i1-IQ1_M | 2.1 | mostly desperate |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ2_XXS.gguf) | i1-IQ2_XXS | 2.4 |  |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ2_XS.gguf) | i1-IQ2_XS | 2.6 |  |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ2_S.gguf) | i1-IQ2_S | 2.7 |  |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ2_M.gguf) | i1-IQ2_M | 2.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q2_K.gguf) | i1-Q2_K | 3.1 | IQ3_XXS probably better |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ3_XXS.gguf) | i1-IQ3_XXS | 3.2 | lower quality |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ3_XS.gguf) | i1-IQ3_XS | 3.5 |  |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q3_K_S.gguf) | i1-Q3_K_S | 3.6 | IQ3_XS probably better |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ3_S.gguf) | i1-IQ3_S | 3.6 | beats Q3_K* |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ3_M.gguf) | i1-IQ3_M | 3.7 |  |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q3_K_M.gguf) | i1-Q3_K_M | 4.0 | IQ3_S probably better |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q3_K_L.gguf) | i1-Q3_K_L | 4.3 | IQ3_M probably better |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-IQ4_XS.gguf) | i1-IQ4_XS | 4.4 |  |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q4_0_4_4.gguf) | i1-Q4_0_4_4 | 4.6 | fast on arm, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q4_0_4_8.gguf) | i1-Q4_0_4_8 | 4.6 | fast on arm+i8mm, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q4_0_8_8.gguf) | i1-Q4_0_8_8 | 4.6 | fast on arm+sve, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q4_0.gguf) | i1-Q4_0 | 4.6 | fast, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q4_K_S.gguf) | i1-Q4_K_S | 4.6 | optimal size/speed/quality |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q4_K_M.gguf) | i1-Q4_K_M | 4.8 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q5_K_S.gguf) | i1-Q5_K_S | 5.5 |  |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q5_K_M.gguf) | i1-Q5_K_M | 5.6 |  |\\n| [GGUF](https://huggingface.co/mradermacher/OpenCoder-8B-Instruct-i1-GGUF/resolve/main/OpenCoder-8B-Instruct.i1-Q6_K.gguf) | i1-Q6_K | 6.5 | practically like static Q6_K |\\n\\nHere is a handy graph by ikawrakow comparing some lower-quality quant\\ntypes (lower is better):\\n\\n![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)\\n\\nAnd here are Artefact2's thoughts on the matter:\\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\\n\\n## FAQ / Model Request\\n\\nSee https://huggingface.co/mradermacher/model_requests for some answers to\\nquestions you might have and/or if you want some other model quantized.\\n\\n## Thanks\\n\\nI thank my company, [nethype GmbH](https://www.nethype.de/), for letting\\nme use its servers and providing upgrades to my workstation to enable\\nthis work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.\\n\\n<!-- end -->\\n\",\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 93.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 2.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c21bfcad08a0d12cca78',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.103000',\n",
       "  'date_modified': '2025-02-27T19:18:30.103000'},\n",
       " {'model_identifier': 'fixie-ai/ultravox-v0_4_1-llama-3_3-70b',\n",
       "  'version': 6,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2024-12-16T16:14:37',\n",
       "  'last_modified': '2025-02-27T17:32:11',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'ultravox',\n",
       "   'feature-extraction',\n",
       "   'audio-text-to-text',\n",
       "   'custom_code',\n",
       "   'ar',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'nl',\n",
       "   'pt',\n",
       "   'ru',\n",
       "   'sv',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'zh',\n",
       "   'dataset:fixie-ai/librispeech_asr',\n",
       "   'dataset:fixie-ai/common_voice_17_0',\n",
       "   'dataset:fixie-ai/peoples_speech',\n",
       "   'dataset:fixie-ai/gigaspeech',\n",
       "   'dataset:fixie-ai/multilingual_librispeech',\n",
       "   'dataset:fixie-ai/wenetspeech',\n",
       "   'dataset:fixie-ai/covost2',\n",
       "   'license:mit',\n",
       "   'region:us'],\n",
       "  'task': ['feature-extraction'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.10940933227539062},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'mit'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'audio-text-to-text',\n",
       "  'description': '---\\nlanguage:\\n- ar\\n- de\\n- en\\n- es\\n- fr\\n- hi\\n- it\\n- ja\\n- nl\\n- pt\\n- ru\\n- sv\\n- tr\\n- uk\\n- zh\\nlicense: mit\\nlibrary_name: transformers\\ndatasets:\\n- fixie-ai/librispeech_asr\\n- fixie-ai/common_voice_17_0\\n- fixie-ai/peoples_speech\\n- fixie-ai/gigaspeech\\n- fixie-ai/multilingual_librispeech\\n- fixie-ai/wenetspeech\\n- fixie-ai/covost2\\nmetrics:\\n- bleu\\npipeline_tag: audio-text-to-text\\n---\\n\\n# Model Card for Ultravox\\n\\nUltravox is a multimodal Speech LLM built around a pretrained [Llama3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) and [whisper-large-v3-turbo](https://huggingface.co/openai/whisper-large-v3-turbo) backbone.\\n\\nSee https://ultravox.ai for the GitHub repo and more information.\\n\\n\\n## Model Details\\n\\n### Model Description\\n\\nUltravox is a multimodal model that can consume both speech and text as input (e.g., a text system prompt and voice user message). \\nThe input to the model is given as a text prompt with a special `<|audio|>` pseudo-token, and the model processor will replace this magic token with embeddings derived from the input audio.\\nUsing the merged embeddings as input, the model will then generate output text as usual. \\n\\nIn a future revision of Ultravox, we plan to expand the token vocabulary to support generation of semantic and acoustic audio tokens, which can then be fed to a vocoder to produce voice output.\\nNo preference tuning has been applied to this revision of the model.\\n\\n- **Developed by:** Fixie.ai\\n- **License:** MIT\\n\\n### Model Sources\\n\\n- **Repository:** https://ultravox.ai\\n- **Demo:** See repo\\n\\n## Usage\\n\\nThink of the model as an LLM that can also hear and understand speech. As such, it can be used as a voice agent, and also to do speech-to-speech translation, analysis of spoken audio, etc.\\n\\nTo use the model, try the following:\\n```python\\n# pip install transformers peft librosa\\n\\nimport transformers\\nimport numpy as np\\nimport librosa\\n\\npipe = transformers.pipeline(model=\\'fixie-ai/ultravox-v0_4_1-llama-3_1-70b\\', trust_remote_code=True)\\n\\npath = \"<path-to-input-audio>\"  # TODO: pass the audio here\\naudio, sr = librosa.load(path, sr=16000)\\n\\n\\nturns = [\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You are a friendly and helpful character. You love to answer questions for people.\"\\n  },\\n]\\npipe({\\'audio\\': audio, \\'turns\\': turns, \\'sampling_rate\\': sr}, max_new_tokens=30)\\n```\\n\\n\\n## Training Details\\n\\nThe model uses a pre-trained [Llama3.3-70B-Instruct](https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct) backbone as well as the encoder part of [whisper-large-v3-turbo](https://huggingface.co/openai/whisper-large-v3-turbo).\\n\\nOnly the multi-modal adapter is trained, while Whisper encoder and Llama are kept frozen.\\n\\nWe use a knowledge-distillation loss where Ultravox is trying to match the logits of the text-based Llama backbone.\\n\\n### Training Data\\n\\nThe training dataset is a mix of ASR datasets, extended with continuations generated by Llama 3.1 8B, and speech translation datasets, which yield a modest improvement in translation evaluations.\\n\\n### Training Procedure\\n\\nSupervised speech instruction finetuning via knowledge-distillation. For more info, see [training code in Ultravox repo](https://github.com/fixie-ai/ultravox/blob/main/ultravox/training/train.py).\\n\\n\\n#### Training Hyperparameters\\n\\n- **Training regime:** BF16 mixed precision training\\n- **Hardward used:** 8x H100 GPUs\\n\\n#### Speeds, Sizes, Times\\n\\nThe current version of Ultravox, when invoked with audio content, has a time-to-first-token (TTFT) of approximately 150ms, and a tokens-per-second rate of ~50-100 when using an A100-40GB GPU, all using a Llama 3.3 70B backbone.\\n\\nCheck out the audio tab on [TheFastest.ai](https://thefastest.ai/?m=audio) for daily benchmarks and a comparison with other existing models.\\n\\n## Evaluation\\n\\n|  | Ultravox 0.4 70B | Ultravox 0.4.1 70B |\\n| --- |  ---: | ---: |\\n| **en_ar** | 14.97 | 19.64 |\\n| **en_de** | 30.30 | 32.47 |\\n| **es_en** | 39.55 | 40.76 |\\n| **ru_en** | 44.16 | 45.07 |\\n| **en_ca** | 35.02 | 37.58 |\\n| **zh_en** | 12.16 | 17.98 |',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 1096.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 10.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c216fcad08a0d12cca77',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.093000',\n",
       "  'date_modified': '2025-02-27T19:18:30.093000'},\n",
       " {'model_identifier': 'enxhi/ship-intents-model',\n",
       "  'version': 14,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-24T14:14:27',\n",
       "  'last_modified': '2025-02-27T17:32:23',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'distilbert',\n",
       "   'text-classification',\n",
       "   'generated_from_trainer',\n",
       "   'base_model:distilbert/distilbert-base-uncased',\n",
       "   'base_model:finetune:distilbert/distilbert-base-uncased',\n",
       "   'license:apache-2.0',\n",
       "   'autotrain_compatible',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-classification'],\n",
       "  'architecture': 'DistilBertForSequenceClassification',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.24943257868289948},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-classification',\n",
       "  'description': '---\\nlibrary_name: transformers\\nlicense: apache-2.0\\nbase_model: distilbert-base-uncased\\ntags:\\n- generated_from_trainer\\nmodel-index:\\n- name: ship-intents-model\\n  results: []\\n---\\n\\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n# ship-intents-model\\n\\nThis model is a fine-tuned version of [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased) on an unknown dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: 1.3524\\n\\n## Model description\\n\\nMore information needed\\n\\n## Intended uses & limitations\\n\\nMore information needed\\n\\n## Training and evaluation data\\n\\nMore information needed\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 5e-05\\n- train_batch_size: 8\\n- eval_batch_size: 8\\n- seed: 42\\n- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\\n- lr_scheduler_type: linear\\n- num_epochs: 3.0\\n\\n### Training results\\n\\n| Training Loss | Epoch | Step | Validation Loss |\\n|:-------------:|:-----:|:----:|:---------------:|\\n| No log        | 1.0   | 4    | 1.4064          |\\n| No log        | 2.0   | 8    | 1.3716          |\\n| No log        | 3.0   | 12   | 1.3524          |\\n\\n\\n### Framework versions\\n\\n- Transformers 4.49.0\\n- Pytorch 2.6.0+cu124\\n- Datasets 3.3.2\\n- Tokenizers 0.21.0\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 1.0,\n",
       "    'downloads': 14.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c215fcad08a0d12cca76',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.093000',\n",
       "  'date_modified': '2025-02-27T19:18:30.093000'},\n",
       " {'model_identifier': 'fixie-ai/ultravox-v0_4_1-llama-3_1-8b',\n",
       "  'version': 11,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2024-11-05T03:24:47',\n",
       "  'last_modified': '2025-02-27T17:32:29',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'ultravox',\n",
       "   'feature-extraction',\n",
       "   'audio-text-to-text',\n",
       "   'custom_code',\n",
       "   'ar',\n",
       "   'de',\n",
       "   'en',\n",
       "   'es',\n",
       "   'fr',\n",
       "   'hi',\n",
       "   'it',\n",
       "   'ja',\n",
       "   'nl',\n",
       "   'pt',\n",
       "   'ru',\n",
       "   'sv',\n",
       "   'tr',\n",
       "   'uk',\n",
       "   'zh',\n",
       "   'dataset:fixie-ai/librispeech_asr',\n",
       "   'dataset:fixie-ai/common_voice_17_0',\n",
       "   'dataset:fixie-ai/peoples_speech',\n",
       "   'dataset:fixie-ai/gigaspeech',\n",
       "   'dataset:fixie-ai/multilingual_librispeech',\n",
       "   'dataset:fixie-ai/wenetspeech',\n",
       "   'dataset:fixie-ai/covost2',\n",
       "   'license:mit',\n",
       "   'region:us'],\n",
       "  'task': ['feature-extraction'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.09377670288085938},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'mit'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'audio-text-to-text',\n",
       "  'description': '---\\ndatasets:\\n- fixie-ai/librispeech_asr\\n- fixie-ai/common_voice_17_0\\n- fixie-ai/peoples_speech\\n- fixie-ai/gigaspeech\\n- fixie-ai/multilingual_librispeech\\n- fixie-ai/wenetspeech\\n- fixie-ai/covost2\\nlanguage:\\n- ar\\n- de\\n- en\\n- es\\n- fr\\n- hi\\n- it\\n- ja\\n- nl\\n- pt\\n- ru\\n- sv\\n- tr\\n- uk\\n- zh\\nlibrary_name: transformers\\nlicense: mit\\nmetrics:\\n- bleu\\npipeline_tag: audio-text-to-text\\n---\\n\\n# Model Card for Ultravox\\n\\nUltravox is a multimodal Speech LLM built around a pretrained [Llama3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B) and [whisper-large-v3-turbo](https://huggingface.co/openai/whisper-large-v3-turbo) backbone.\\n\\nSee https://ultravox.ai for the GitHub repo and more information.\\n\\n\\n## Model Details\\n\\n### Model Description\\n\\nUltravox is a multimodal model that can consume both speech and text as input (e.g., a text system prompt and voice user message). \\nThe input to the model is given as a text prompt with a special `<|audio|>` pseudo-token, and the model processor will replace this magic token with embeddings derived from the input audio.\\nUsing the merged embeddings as input, the model will then generate output text as usual. \\n\\nIn a future revision of Ultravox, we plan to expand the token vocabulary to support generation of semantic and acoustic audio tokens, which can then be fed to a vocoder to produce voice output.\\nNo preference tuning has been applied to this revision of the model.\\n\\n- **Developed by:** Fixie.ai\\n- **License:** MIT\\n\\n### Model Sources\\n\\n- **Repository:** https://ultravox.ai\\n- **Demo:** See repo\\n\\n## Usage\\n\\nThink of the model as an LLM that can also hear and understand speech. As such, it can be used as a voice agent, and also to do speech-to-speech translation, analysis of spoken audio, etc.\\n\\nTo use the model, try the following:\\n```python\\n# pip install transformers peft librosa\\n\\nimport transformers\\nimport numpy as np\\nimport librosa\\n\\npipe = transformers.pipeline(model=\\'fixie-ai/ultravox-v0_4_1-llama-3_1-8b\\', trust_remote_code=True)\\n\\npath = \"<path-to-input-audio>\"  # TODO: pass the audio here\\naudio, sr = librosa.load(path, sr=16000)\\n\\n\\nturns = [\\n  {\\n    \"role\": \"system\",\\n    \"content\": \"You are a friendly and helpful character. You love to answer questions for people.\"\\n  },\\n]\\npipe({\\'audio\\': audio, \\'turns\\': turns, \\'sampling_rate\\': sr}, max_new_tokens=30)\\n```\\n\\n\\n## Training Details\\n\\nThe model uses a pre-trained [Llama3.1-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B) backbone as well as the encoder part of [whisper-large-v3-turbo](https://huggingface.co/openai/whisper-large-v3-turbo).\\n\\nOnly the multi-modal adapter is trained, while Whisper encoder and Llama are kept frozen.\\n\\nWe use a knowledge-distillation loss where Ultravox is trying to match the logits of the text-based Llama backbone.\\n\\n### Training Data\\n\\nThe training dataset is a mix of ASR datasets, extended with continuations generated by Llama 3.1 8B, and speech translation datasets, which yield a modest improvement in translation evaluations.\\n\\n### Training Procedure\\n\\nSupervised speech instruction finetuning via knowledge-distillation. For more info, see [training code in Ultravox repo](https://github.com/fixie-ai/ultravox/blob/main/ultravox/training/train.py).\\n\\n\\n#### Training Hyperparameters\\n\\n- **Training regime:** BF16 mixed precision training\\n- **Hardward used:** 8x H100 GPUs\\n\\n#### Speeds, Sizes, Times\\n\\nThe current version of Ultravox, when invoked with audio content, has a time-to-first-token (TTFT) of approximately 150ms, and a tokens-per-second rate of ~50-100 when using an A100-40GB GPU, all using a Llama 3.1 8B backbone.\\n\\nCheck out the audio tab on [TheFastest.ai](https://thefastest.ai/?m=audio) for daily benchmarks and a comparison with other existing models.\\n\\n## Evaluation\\n\\n|  | Ultravox 0.4 8B | **Ultravox 0.4.1 8B** |\\n| --- |  ---: | ---: |\\n| **en_ar** | 11.17 | 12.28 |\\n| **en_de** | 25.47 | 27.13 |\\n| **es_en** | 37.11 | 39.16 |\\n| **ru_en** | 38.96 | 39.65 |\\n| **en_ca** | 27.46 | 29.94 |\\n| **zh_en** | 10.08 | 14.55 |',\n",
       "  'popularity': {'huggingface': {'spaces_count': 1.0,\n",
       "    'downloads': 14668.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 96.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c214fcad08a0d12cca75',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.093000',\n",
       "  'date_modified': '2025-02-27T19:18:30.093000'},\n",
       " {'model_identifier': 'Charmainemahachi/distilbert-base-uncased',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:32:39',\n",
       "  'last_modified': '2025-02-27T17:32:39',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c213fcad08a0d12cca74',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:30.093000',\n",
       "  'date_modified': '2025-02-27T19:18:30.093000'},\n",
       " {'model_identifier': 'mradermacher/LemonP-8B-Model_Stock-i1-GGUF',\n",
       "  'version': 31,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2024-11-08T23:29:34',\n",
       "  'last_modified': '2025-02-27T17:32:50',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': True,\n",
       "  'tags': ['transformers',\n",
       "   'gguf',\n",
       "   'mergekit',\n",
       "   'merge',\n",
       "   'en',\n",
       "   'base_model:DreadPoor/LemonP-8B-Model_Stock',\n",
       "   'base_model:quantized:DreadPoor/LemonP-8B-Model_Stock',\n",
       "   'license:apache-2.0',\n",
       "   'endpoints_compatible',\n",
       "   'region:us',\n",
       "   'imatrix',\n",
       "   'conversational'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 7.478763580322266},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': \"---\\nbase_model: DreadPoor/LemonP-8B-Model_Stock\\nlanguage:\\n- en\\nlibrary_name: transformers\\nlicense: apache-2.0\\nquantized_by: mradermacher\\ntags:\\n- mergekit\\n- merge\\n---\\n## About\\n\\n<!-- ### quantize_version: 2 -->\\n<!-- ### output_tensor_quantised: 1 -->\\n<!-- ### convert_type: hf -->\\n<!-- ### vocab_type:  -->\\n<!-- ### tags: nicoboss -->\\nweighted/imatrix quants of https://huggingface.co/DreadPoor/LemonP-8B-Model_Stock\\n\\n<!-- provided-files -->\\n## Usage\\n\\nIf you are unsure how to use GGUF files, refer to one of [TheBloke's\\nREADMEs](https://huggingface.co/TheBloke/KafkaLM-70B-German-V0.1-GGUF) for\\nmore details, including on how to concatenate multi-part files.\\n\\n## Provided Quants\\n\\n(sorted by size, not necessarily quality. IQ-quants are often preferable over similar sized non-IQ quants)\\n\\n| Link | Type | Size/GB | Notes |\\n|:-----|:-----|--------:|:------|\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ1_S.gguf) | i1-IQ1_S | 2.1 | for the desperate |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ1_M.gguf) | i1-IQ1_M | 2.3 | mostly desperate |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ2_XXS.gguf) | i1-IQ2_XXS | 2.5 |  |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ2_XS.gguf) | i1-IQ2_XS | 2.7 |  |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ2_S.gguf) | i1-IQ2_S | 2.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ2_M.gguf) | i1-IQ2_M | 3.0 |  |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q2_K.gguf) | i1-Q2_K | 3.3 | IQ3_XXS probably better |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ3_XXS.gguf) | i1-IQ3_XXS | 3.4 | lower quality |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ3_XS.gguf) | i1-IQ3_XS | 3.6 |  |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q3_K_S.gguf) | i1-Q3_K_S | 3.8 | IQ3_XS probably better |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ3_S.gguf) | i1-IQ3_S | 3.8 | beats Q3_K* |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ3_M.gguf) | i1-IQ3_M | 3.9 |  |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q3_K_M.gguf) | i1-Q3_K_M | 4.1 | IQ3_S probably better |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q3_K_L.gguf) | i1-Q3_K_L | 4.4 | IQ3_M probably better |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-IQ4_XS.gguf) | i1-IQ4_XS | 4.5 |  |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q4_0_4_4.gguf) | i1-Q4_0_4_4 | 4.8 | fast on arm, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q4_0_4_8.gguf) | i1-Q4_0_4_8 | 4.8 | fast on arm+i8mm, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q4_0_8_8.gguf) | i1-Q4_0_8_8 | 4.8 | fast on arm+sve, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q4_0.gguf) | i1-Q4_0 | 4.8 | fast, low quality |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q4_K_S.gguf) | i1-Q4_K_S | 4.8 | optimal size/speed/quality |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q4_K_M.gguf) | i1-Q4_K_M | 5.0 | fast, recommended |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q5_K_S.gguf) | i1-Q5_K_S | 5.7 |  |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q5_K_M.gguf) | i1-Q5_K_M | 5.8 |  |\\n| [GGUF](https://huggingface.co/mradermacher/LemonP-8B-Model_Stock-i1-GGUF/resolve/main/LemonP-8B-Model_Stock.i1-Q6_K.gguf) | i1-Q6_K | 6.7 | practically like static Q6_K |\\n\\nHere is a handy graph by ikawrakow comparing some lower-quality quant\\ntypes (lower is better):\\n\\n![image.png](https://www.nethype.de/huggingface_embed/quantpplgraph.png)\\n\\nAnd here are Artefact2's thoughts on the matter:\\nhttps://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\\n\\n## FAQ / Model Request\\n\\nSee https://huggingface.co/mradermacher/model_requests for some answers to\\nquestions you might have and/or if you want some other model quantized.\\n\\n## Thanks\\n\\nI thank my company, [nethype GmbH](https://www.nethype.de/), for letting\\nme use its servers and providing upgrades to my workstation to enable\\nthis work in my free time. Additional thanks to [@nicoboss](https://huggingface.co/nicoboss) for giving me access to his private supercomputer, enabling me to provide many more imatrix quants, at much higher quality, than I would otherwise be able to.\\n\\n<!-- end -->\\n\",\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 83.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 1.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c20ffcad08a0d12cca72',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.971000',\n",
       "  'date_modified': '2025-02-27T19:18:29.971000'},\n",
       " {'model_identifier': 'PrunaAI/Qwen-Qwen1.5-0.5B-HQQ-4bit-smashed',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:32:11',\n",
       "  'last_modified': '2025-02-27T17:33:02',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['qwen2', 'pruna-ai', 'hqq', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nthumbnail: \"https://assets-global.website-files.com/646b351987a8d8ce158d1940/64ec9e96b4334c0e1ac41504_Logo%20with%20white%20text.svg\"\\nbase_model: ORIGINAL_REPO_NAME\\nmetrics:\\n- memory_disk\\n- memory_inference\\n- inference_latency\\n- inference_throughput\\n- inference_CO2_emissions\\n- inference_energy_consumption\\ntags:\\n- pruna-ai\\n---\\n<!-- header start -->\\n<!-- 200823 -->\\n<div style=\"width: auto; margin-left: auto; margin-right: auto\">\\n    <a href=\"https://www.pruna.ai/\" target=\"_blank\" rel=\"noopener noreferrer\">\\n        <img src=\"https://i.imgur.com/eDAlcgk.png\" alt=\"PrunaAI\" style=\"width: 100%; min-width: 400px; display: block; margin: auto;\">\\n    </a>\\n</div>\\n<!-- header end -->\\n\\n[![Twitter](https://img.shields.io/twitter/follow/PrunaAI?style=social)](https://twitter.com/PrunaAI)\\n[![GitHub](https://img.shields.io/github/followers/PrunaAI?label=Follow%20%40PrunaAI&style=social)](https://github.com/PrunaAI)\\n[![LinkedIn](https://img.shields.io/badge/LinkedIn-Connect-blue)](https://www.linkedin.com/company/93832878/admin/feed/posts/?feedType=following)\\n[![Discord](https://img.shields.io/badge/Discord-Join%20Us-blue?style=social&logo=discord)](https://discord.gg/rskEr4BZJx)\\n\\n# Simply make AI models cheaper, smaller, faster, and greener!\\n\\n- Give a thumbs up if you like this model!\\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\\n- Request access to easily compress your *own* AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\\n- Read the documentations to know more [here](https://pruna-ai-pruna.readthedocs-hosted.com/en/latest/)\\n- Join Pruna AI community on Discord [here](https://discord.gg/CP4VSgck) to share feedback/suggestions or get help.\\n\\n## Results\\n\\n![image info](./plots.png)\\n\\n**Frequently Asked Questions**\\n- ***How does the compression work?*** The model is compressed with hqq.\\n- ***How does the model quality change?*** The quality of the model output might vary compared to the base model.\\n- ***How is the model efficiency evaluated?*** These results were obtained with configuration described in `model/smash_config.json` and are obtained after a hardware warmup. The smashed model is directly compared to the original base model. Efficiency results may vary in other settings (e.g. other hardware, image size, batch size, ...). We recommend to directly run them in the use-case conditions to know if the smashed model can benefit you.\\n- ***What is the model format?*** We use safetensors.\\n- ***What calibration data has been used?*** If needed by the compression method, we used WikiText as the calibration data.\\n- ***What is the naming convention for Pruna Huggingface models?*** We take the original model name and append \"turbo\", \"tiny\", or \"green\" if the smashed model has a measured inference speed, inference memory, or inference energy consumption which is less than 90% of the original base model.\\n- ***How to compress my own models?*** You can request premium access to more compression methods and tech support for your specific use-cases [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).\\n- ***What are \"first\" metrics?*** Results mentioning \"first\" are obtained after the first run of the model. The first run might take more memory or be slower than the subsequent runs due cuda overheads.\\n- ***What are \"Sync\" and \"Async\" metrics?*** \"Sync\" metrics are obtained by syncing all GPU processes and stop measurement when all of them are executed. \"Async\" metrics are obtained without syncing all GPU processes and stop when the model output can be used by the CPU. We provide both metrics since both could be relevant depending on the use-case. We recommend to test the efficiency gains directly in your use-cases.\\n\\n## Setup\\n\\nYou can run the smashed model with these steps:\\n\\n0. Check requirements from the original repo ORIGINAL_REPO_NAME installed. In particular, check python, cuda, and transformers versions.\\n1. Make sure that you have installed quantization related packages.\\n    ```bash\\n    pip install hqq\\n    ```\\n2. Load & run the model.\\n    ```python \\n   from transformers import AutoModelForCausalLM, AutoTokenizer\\n    from hqq.engine.hf import HQQModelForCausalLM\\n from hqq.models.hf.base import AutoHQQHFModel\\n\\n   try:\\n     model = HQQModelForCausalLM.from_quantized(\"PrunaAI/Qwen-Qwen1.5-0.5B-HQQ-4bit-smashed\", device_map=\\'auto\\')\\n    except: \\n     model = AutoHQQHFModel.from_quantized(\"PrunaAI/Qwen-Qwen1.5-0.5B-HQQ-4bit-smashed\")\\n   tokenizer = AutoTokenizer.from_pretrained(\"ORIGINAL_REPO_NAME\")\\n    \\n   input_ids = tokenizer(\"What is the color of prunes?,\", return_tensors=\\'pt\\').to(model.device)[\"input_ids\"]\\n    \\n   outputs = model.generate(input_ids, max_new_tokens=216)\\n   tokenizer.decode(outputs[0])\\n    ```\\n\\n## Configurations\\n\\nThe configuration info are in `smash_config.json`.\\n\\n## Credits & License\\n\\nThe license of the smashed model follows the license of the original model. Please check the license of the original model ORIGINAL_REPO_NAME before using this model which provided the base model. The license  of the `pruna-engine` is [here](https://pypi.org/project/pruna-engine/) on Pypi.\\n\\n## Want to compress other models?\\n\\n- Contact us and tell us which model to compress next [here](https://www.pruna.ai/contact).\\n- Request access to easily compress your own AI models [here](https://z0halsaff74.typeform.com/pruna-access?typeform-source=www.pruna.ai).',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c20efcad08a0d12cca71',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.971000',\n",
       "  'date_modified': '2025-02-27T19:18:29.971000'},\n",
       " {'model_identifier': 'Hasnain200605/roznama',\n",
       "  'version': 32,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2024-01-28T12:07:40',\n",
       "  'last_modified': '2025-02-27T17:33:25',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['license:openrail', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'openrail'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: openrail\\n---\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c20cfcad08a0d12cca6f',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.762000',\n",
       "  'date_modified': '2025-02-27T19:18:29.762000'},\n",
       " {'model_identifier': 'lkoenig/BBAI_375_QwenDyancabs',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:28:50',\n",
       "  'last_modified': '2025-02-27T17:33:31',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'mergekit',\n",
       "   'merge',\n",
       "   'conversational',\n",
       "   'base_model:Xiaojian9992024/Qwen2.5-Dyanka-7B-Preview',\n",
       "   'base_model:merge:Xiaojian9992024/Qwen2.5-Dyanka-7B-Preview',\n",
       "   'base_model:gz987/qwen2.5-7b-cabs-v0.3',\n",
       "   'base_model:merge:gz987/qwen2.5-7b-cabs-v0.3',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A mid-range GPU with 16GB VRAM (e.g., NVIDIA RTX 3080, A2000).',\n",
       "   'memory_in_gb': 14.18519115447998},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nbase_model:\\n- gz987/qwen2.5-7b-cabs-v0.3\\n- Xiaojian9992024/Qwen2.5-Dyanka-7B-Preview\\nlibrary_name: transformers\\ntags:\\n- mergekit\\n- merge\\n\\n---\\n# merge\\n\\nThis is a merge of pre-trained language models created using [mergekit](https://github.com/cg123/mergekit).\\n\\n## Merge Details\\n### Merge Method\\n\\nThis model was merged using the [SLERP](https://en.wikipedia.org/wiki/Slerp) merge method.\\n\\n### Models Merged\\n\\nThe following models were included in the merge:\\n* [gz987/qwen2.5-7b-cabs-v0.3](https://huggingface.co/gz987/qwen2.5-7b-cabs-v0.3)\\n* [Xiaojian9992024/Qwen2.5-Dyanka-7B-Preview](https://huggingface.co/Xiaojian9992024/Qwen2.5-Dyanka-7B-Preview)\\n\\n### Configuration\\n\\nThe following YAML configuration was used to produce this model:\\n\\n```yaml\\nslices:\\n- sources:\\n  - model: Xiaojian9992024/Qwen2.5-Dyanka-7B-Preview\\n    layer_range:\\n    - 0\\n    - 28\\n  - model: gz987/qwen2.5-7b-cabs-v0.3\\n    layer_range:\\n    - 0\\n    - 28\\nmerge_method: slerp\\nbase_model: Xiaojian9992024/Qwen2.5-Dyanka-7B-Preview\\nparameters:\\n  t:\\n  - filter: self_attn\\n    value: 0.5\\n  - filter: mlp\\n    value: 0.5\\n  - value: 0.5\\ndtype: bfloat16\\nallow_different_vocabulary: true\\nignore_unexpected_keys: true\\n```\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c20bfcad08a0d12cca6e',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.762000',\n",
       "  'date_modified': '2025-02-27T19:18:29.762000'},\n",
       " {'model_identifier': 'cutelemonlili/Llama3.2-1B-Instruct_Lean_Code',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:32:46',\n",
       "  'last_modified': '2025-02-27T17:33:38',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'llama',\n",
       "   'text-generation',\n",
       "   'llama-factory',\n",
       "   'full',\n",
       "   'generated_from_trainer',\n",
       "   'conversational',\n",
       "   'base_model:meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'base_model:finetune:meta-llama/Llama-3.2-1B-Instruct',\n",
       "   'license:other',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 2.3018836975097656},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'other'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nlibrary_name: transformers\\nlicense: other\\nbase_model: meta-llama/Llama-3.2-1B-Instruct\\ntags:\\n- llama-factory\\n- full\\n- generated_from_trainer\\nmodel-index:\\n- name: Lean_Code_data\\n  results: []\\n---\\n\\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n# Lean_Code_data\\n\\nThis model is a fine-tuned version of [meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct) on the Lean_Code_data dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: 0.2471\\n\\n## Model description\\n\\nMore information needed\\n\\n## Intended uses & limitations\\n\\nMore information needed\\n\\n## Training and evaluation data\\n\\nMore information needed\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 1e-05\\n- train_batch_size: 2\\n- eval_batch_size: 1\\n- seed: 42\\n- distributed_type: multi-GPU\\n- num_devices: 4\\n- total_train_batch_size: 8\\n- total_eval_batch_size: 4\\n- optimizer: Use adamw_torch with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\\n- lr_scheduler_type: cosine\\n- num_epochs: 2\\n\\n### Training results\\n\\n\\n\\n### Framework versions\\n\\n- Transformers 4.46.1\\n- Pytorch 2.6.0+cu124\\n- Datasets 3.1.0\\n- Tokenizers 0.20.3\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c20afcad08a0d12cca6d',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.762000',\n",
       "  'date_modified': '2025-02-27T19:18:29.762000'},\n",
       " {'model_identifier': 'Kaileh57/Ursa_Minor-Q4_K_M-GGUF',\n",
       "  'version': 3,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:33:35',\n",
       "  'last_modified': '2025-02-27T17:33:42',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': True,\n",
       "  'tags': ['gguf',\n",
       "   'llama-cpp',\n",
       "   'gguf-my-repo',\n",
       "   'base_model:Kaileh57/Ursa_Minor',\n",
       "   'base_model:quantized:Kaileh57/Ursa_Minor',\n",
       "   'license:apache-2.0',\n",
       "   'endpoints_compatible',\n",
       "   'region:us',\n",
       "   'conversational'],\n",
       "  'task': [],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 1.4376959800720215},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: apache-2.0\\nbase_model: Kaileh57/Ursa_Minor\\ntags:\\n- llama-cpp\\n- gguf-my-repo\\n---\\n\\n# Kaileh57/Ursa_Minor-Q4_K_M-GGUF\\nThis model was converted to GGUF format from [`Kaileh57/Ursa_Minor`](https://huggingface.co/Kaileh57/Ursa_Minor) using llama.cpp via the ggml.ai\\'s [GGUF-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) space.\\nRefer to the [original model card](https://huggingface.co/Kaileh57/Ursa_Minor) for more details on the model.\\n\\n## Use with llama.cpp\\nInstall llama.cpp through brew (works on Mac and Linux)\\n\\n```bash\\nbrew install llama.cpp\\n\\n```\\nInvoke the llama.cpp server or the CLI.\\n\\n### CLI:\\n```bash\\nllama-cli --hf-repo Kaileh57/Ursa_Minor-Q4_K_M-GGUF --hf-file ursa_minor-q4_k_m.gguf -p \"The meaning to life and the universe is\"\\n```\\n\\n### Server:\\n```bash\\nllama-server --hf-repo Kaileh57/Ursa_Minor-Q4_K_M-GGUF --hf-file ursa_minor-q4_k_m.gguf -c 2048\\n```\\n\\nNote: You can also use this checkpoint directly through the [usage steps](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#usage) listed in the Llama.cpp repo as well.\\n\\nStep 1: Clone llama.cpp from GitHub.\\n```\\ngit clone https://github.com/ggerganov/llama.cpp\\n```\\n\\nStep 2: Move into the llama.cpp folder and build it with `LLAMA_CURL=1` flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\\n```\\ncd llama.cpp && LLAMA_CURL=1 make\\n```\\n\\nStep 3: Run inference through the main binary.\\n```\\n./llama-cli --hf-repo Kaileh57/Ursa_Minor-Q4_K_M-GGUF --hf-file ursa_minor-q4_k_m.gguf -p \"The meaning to life and the universe is\"\\n```\\nor \\n```\\n./llama-server --hf-repo Kaileh57/Ursa_Minor-Q4_K_M-GGUF --hf-file ursa_minor-q4_k_m.gguf -c 2048\\n```\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c209fcad08a0d12cca6c',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.655000',\n",
       "  'date_modified': '2025-02-27T19:18:29.655000'},\n",
       " {'model_identifier': 'GoofysNutsacc/AustrianPainter',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:32:12',\n",
       "  'last_modified': '2025-02-27T17:34:07',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['license:openrail', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'openrail'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: openrail\\n---\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c208fcad08a0d12cca6b',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.552000',\n",
       "  'date_modified': '2025-02-27T19:18:29.552000'},\n",
       " {'model_identifier': 'kaitchup/Llama-3.3-70B-Instruct-AutoRoundGPTQ-2bit',\n",
       "  'version': 3,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:29:17',\n",
       "  'last_modified': '2025-02-27T17:34:26',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'llama',\n",
       "   'text-generation',\n",
       "   'conversational',\n",
       "   'arxiv:1910.09700',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   '2-bit',\n",
       "   'gptq',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A workstation GPU with 40GB VRAM (e.g., NVIDIA A100 40GB).',\n",
       "   'memory_in_gb': 24.359634399414062},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nlibrary_name: transformers\\ntags: []\\n---\\n\\n# Model Card for Model ID\\n\\n<!-- Provide a quick summary of what the model is/does. -->\\n\\n\\n\\n## Model Details\\n\\n### Model Description\\n\\n<!-- Provide a longer summary of what this model is. -->\\n\\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\\n\\n- **Developed by:** [More Information Needed]\\n- **Funded by [optional]:** [More Information Needed]\\n- **Shared by [optional]:** [More Information Needed]\\n- **Model type:** [More Information Needed]\\n- **Language(s) (NLP):** [More Information Needed]\\n- **License:** [More Information Needed]\\n- **Finetuned from model [optional]:** [More Information Needed]\\n\\n### Model Sources [optional]\\n\\n<!-- Provide the basic links for the model. -->\\n\\n- **Repository:** [More Information Needed]\\n- **Paper [optional]:** [More Information Needed]\\n- **Demo [optional]:** [More Information Needed]\\n\\n## Uses\\n\\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\\n\\n### Direct Use\\n\\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\\n\\n[More Information Needed]\\n\\n### Downstream Use [optional]\\n\\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\\n\\n[More Information Needed]\\n\\n### Out-of-Scope Use\\n\\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\\n\\n[More Information Needed]\\n\\n## Bias, Risks, and Limitations\\n\\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\\n\\n[More Information Needed]\\n\\n### Recommendations\\n\\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\\n\\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n## How to Get Started with the Model\\n\\nUse the code below to get started with the model.\\n\\n[More Information Needed]\\n\\n## Training Details\\n\\n### Training Data\\n\\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\\n\\n[More Information Needed]\\n\\n### Training Procedure\\n\\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\\n\\n#### Preprocessing [optional]\\n\\n[More Information Needed]\\n\\n\\n#### Training Hyperparameters\\n\\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\\n\\n#### Speeds, Sizes, Times [optional]\\n\\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\\n\\n[More Information Needed]\\n\\n## Evaluation\\n\\n<!-- This section describes the evaluation protocols and provides the results. -->\\n\\n### Testing Data, Factors & Metrics\\n\\n#### Testing Data\\n\\n<!-- This should link to a Dataset Card if possible. -->\\n\\n[More Information Needed]\\n\\n#### Factors\\n\\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\\n\\n[More Information Needed]\\n\\n#### Metrics\\n\\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\\n\\n[More Information Needed]\\n\\n### Results\\n\\n[More Information Needed]\\n\\n#### Summary\\n\\n\\n\\n## Model Examination [optional]\\n\\n<!-- Relevant interpretability work for the model goes here -->\\n\\n[More Information Needed]\\n\\n## Environmental Impact\\n\\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\\n\\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n\\n- **Hardware Type:** [More Information Needed]\\n- **Hours used:** [More Information Needed]\\n- **Cloud Provider:** [More Information Needed]\\n- **Compute Region:** [More Information Needed]\\n- **Carbon Emitted:** [More Information Needed]\\n\\n## Technical Specifications [optional]\\n\\n### Model Architecture and Objective\\n\\n[More Information Needed]\\n\\n### Compute Infrastructure\\n\\n[More Information Needed]\\n\\n#### Hardware\\n\\n[More Information Needed]\\n\\n#### Software\\n\\n[More Information Needed]\\n\\n## Citation [optional]\\n\\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\\n\\n**BibTeX:**\\n\\n[More Information Needed]\\n\\n**APA:**\\n\\n[More Information Needed]\\n\\n## Glossary [optional]\\n\\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\\n\\n[More Information Needed]\\n\\n## More Information [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Authors [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Contact\\n\\n[More Information Needed]',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c207fcad08a0d12cca6a',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.552000',\n",
       "  'date_modified': '2025-02-27T19:18:29.552000'},\n",
       " {'model_identifier': 'lesso16/8696ca2f-fcf9-469b-823b-f9aee8f17dbb',\n",
       "  'version': 16,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:20:25',\n",
       "  'last_modified': '2025-02-27T17:34:26',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['peft',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'axolotl',\n",
       "   'generated_from_trainer',\n",
       "   'base_model:Qwen/Qwen1.5-1.8B',\n",
       "   'base_model:adapter:Qwen/Qwen1.5-1.8B',\n",
       "   'license:other',\n",
       "   'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'other'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlibrary_name: peft\\nlicense: other\\nbase_model: Qwen/Qwen1.5-1.8B\\ntags:\\n- axolotl\\n- generated_from_trainer\\nmodel-index:\\n- name: 8696ca2f-fcf9-469b-823b-f9aee8f17dbb\\n  results: []\\n---\\n\\n<!-- This model card has been generated automatically according to the information the Trainer had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n[<img src=\"https://raw.githubusercontent.com/axolotl-ai-cloud/axolotl/main/image/axolotl-badge-web.png\" alt=\"Built with Axolotl\" width=\"200\" height=\"32\"/>](https://github.com/axolotl-ai-cloud/axolotl)\\n<details><summary>See axolotl config</summary>\\n\\naxolotl version: `0.4.1`\\n```yaml\\nadapter: lora\\nauto_find_batch_size: true\\nbase_model: Qwen/Qwen1.5-1.8B\\nbf16: auto\\nchat_template: llama3\\ndataset_prepared_path: null\\ndatasets:\\n- data_files:\\n  - 5f0e56427db2855a_train_data.json\\n  ds_type: json\\n  format: custom\\n  path: /workspace/input_data/5f0e56427db2855a_train_data.json\\n  type:\\n    field_input: context\\n    field_instruction: prompt_serial\\n    field_output: hypothesis\\n    format: \\'{instruction} {input}\\'\\n    no_input_format: \\'{instruction}\\'\\n    system_format: \\'{system}\\'\\n    system_prompt: \\'\\'\\ndebug: null\\ndeepspeed: null\\ndo_eval: true\\nearly_stopping_patience: 3\\neval_max_new_tokens: 128\\neval_steps: 50\\nevals_per_epoch: null\\nflash_attention: true\\nfp16: false\\nfsdp: null\\nfsdp_config: null\\ngradient_accumulation_steps: 2\\ngradient_checkpointing: false\\ngroup_by_length: true\\nhub_model_id: lesso16/8696ca2f-fcf9-469b-823b-f9aee8f17dbb\\nhub_repo: null\\nhub_strategy: checkpoint\\nhub_token: null\\nlearning_rate: 0.000216\\nload_in_4bit: false\\nload_in_8bit: false\\nlocal_rank: null\\nlogging_steps: 10\\nlora_alpha: 32\\nlora_dropout: 0.05\\nlora_fan_in_fan_out: null\\nlora_model_dir: null\\nlora_r: 16\\nlora_target_linear: true\\nlr_scheduler: cosine\\nmax_grad_norm: 1.0\\nmax_steps: 500\\nmicro_batch_size: 4\\nmlflow_experiment_name: /tmp/5f0e56427db2855a_train_data.json\\nmodel_type: AutoModelForCausalLM\\nnum_epochs: 1\\noptimizer: adamw_bnb_8bit\\noutput_dir: miner_id_24\\npad_to_sequence_len: true\\nresume_from_checkpoint: null\\ns2_attention: null\\nsample_packing: false\\nsave_steps: 50\\nsaves_per_epoch: null\\nseed: 160\\nsequence_len: 512\\nstrict: false\\ntf32: true\\ntokenizer_type: AutoTokenizer\\ntrain_on_inputs: false\\ntrust_remote_code: true\\nval_set_size: 0.05\\nwandb_entity: null\\nwandb_mode: online\\nwandb_name: 4cd3a91f-f18d-4fa3-8e9d-2eba69d2fd9d\\nwandb_project: 16a\\nwandb_run: your_name\\nwandb_runid: 4cd3a91f-f18d-4fa3-8e9d-2eba69d2fd9d\\nwarmup_steps: 50\\nweight_decay: 0.0\\nxformers_attention: null\\n\\n```\\n\\n</details><br>\\n\\n# 8696ca2f-fcf9-469b-823b-f9aee8f17dbb\\n\\nThis model is a fine-tuned version of [Qwen/Qwen1.5-1.8B](https://huggingface.co/Qwen/Qwen1.5-1.8B) on the None dataset.\\nIt achieves the following results on the evaluation set:\\n- Loss: 0.0065\\n\\n## Model description\\n\\nMore information needed\\n\\n## Intended uses & limitations\\n\\nMore information needed\\n\\n## Training and evaluation data\\n\\nMore information needed\\n\\n## Training procedure\\n\\n### Training hyperparameters\\n\\nThe following hyperparameters were used during training:\\n- learning_rate: 0.000216\\n- train_batch_size: 4\\n- eval_batch_size: 4\\n- seed: 160\\n- gradient_accumulation_steps: 2\\n- total_train_batch_size: 8\\n- optimizer: Use OptimizerNames.ADAMW_BNB with betas=(0.9,0.999) and epsilon=1e-08 and optimizer_args=No additional optimizer arguments\\n- lr_scheduler_type: cosine\\n- lr_scheduler_warmup_steps: 50\\n- training_steps: 500\\n\\n### Training results\\n\\n| Training Loss | Epoch  | Step | Validation Loss |\\n|:-------------:|:------:|:----:|:---------------:|\\n| No log        | 0.0002 | 1    | 2.2145          |\\n| 0.0344        | 0.0106 | 50   | 0.0212          |\\n| 0.0004        | 0.0212 | 100  | 0.0182          |\\n| 0.0004        | 0.0318 | 150  | 0.0616          |\\n| 0.0004        | 0.0424 | 200  | 0.0012          |\\n| 0.0092        | 0.0530 | 250  | 0.0059          |\\n| 0.001         | 0.0636 | 300  | 0.0063          |\\n| 0.0021        | 0.0742 | 350  | 0.0065          |\\n\\n\\n### Framework versions\\n\\n- PEFT 0.13.2\\n- Transformers 4.46.0\\n- Pytorch 2.5.0+cu124\\n- Datasets 3.0.1\\n- Tokenizers 0.20.1',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c206fcad08a0d12cca69',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.434000',\n",
       "  'date_modified': '2025-02-27T19:18:29.434000'},\n",
       " {'model_identifier': 'kyoungmiin/style_12',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:28:11',\n",
       "  'last_modified': '2025-02-27T17:34:35',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['diffusers',\n",
       "   'text-to-image',\n",
       "   'diffusers-training',\n",
       "   'lora',\n",
       "   'template:sd-lora',\n",
       "   'stable-diffusion-xl',\n",
       "   'stable-diffusion-xl-diffusers',\n",
       "   'base_model:stabilityai/stable-diffusion-xl-base-1.0',\n",
       "   'base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0',\n",
       "   'license:openrail++',\n",
       "   'region:us'],\n",
       "  'task': ['text-to-image'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'description': '---\\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\\nlibrary_name: diffusers\\nlicense: openrail++\\ninstance_prompt: sks\\nwidget: []\\ntags:\\n- text-to-image\\n- text-to-image\\n- diffusers-training\\n- diffusers\\n- lora\\n- template:sd-lora\\n- stable-diffusion-xl\\n- stable-diffusion-xl-diffusers\\n---\\n\\n<!-- This model card has been generated automatically according to the information the training script had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n\\n# SDXL LoRA DreamBooth - kyoungmiin/style_12\\n\\n<Gallery />\\n\\n## Model description\\n\\nThese are kyoungmiin/style_12 LoRA adaption weights for stabilityai/stable-diffusion-xl-base-1.0.\\n\\nThe weights were trained  using [DreamBooth](https://dreambooth.github.io/).\\n\\nLoRA for the text encoder was enabled: False.\\n\\nSpecial VAE used for training: madebyollin/sdxl-vae-fp16-fix.\\n\\n## Trigger words\\n\\nYou should use sks to trigger the image generation.\\n\\n## Download model\\n\\nWeights for this model are available in Safetensors format.\\n\\n[Download](kyoungmiin/style_12/tree/main) them in the Files & versions tab.\\n\\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\n```python\\n# TODO: add an example code snippet for running this diffusion pipeline\\n```\\n\\n#### Limitations and bias\\n\\n[TODO: provide examples of latent issues and potential remediations]\\n\\n## Training details\\n\\n[TODO: describe the data used to train the model]',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c205fcad08a0d12cca68',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.434000',\n",
       "  'date_modified': '2025-02-27T19:18:29.434000'},\n",
       " {'model_identifier': 'kartikeyp011/PlantX-model',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:29:37',\n",
       "  'last_modified': '2025-02-27T17:34:40',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['keras', 'license:cc-by-4.0', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'cc-by-4.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: cc-by-4.0\\n---\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c204fcad08a0d12cca67',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.330000',\n",
       "  'date_modified': '2025-02-27T19:18:29.330000'},\n",
       " {'model_identifier': 'Zolotomeo/mashamodelforzolotomeo',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:35:33',\n",
       "  'last_modified': '2025-02-27T17:35:33',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c200fcad08a0d12cca65',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.330000',\n",
       "  'date_modified': '2025-02-27T19:18:29.330000'},\n",
       " {'model_identifier': 'YasserEmad/propertyModel',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:35:48',\n",
       "  'last_modified': '2025-02-27T17:35:48',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1fffcad08a0d12cca64',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.329000',\n",
       "  'date_modified': '2025-02-27T19:18:29.329000'},\n",
       " {'model_identifier': 'Maslionok/lang-detect',\n",
       "  'version': 4,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:35:38',\n",
       "  'last_modified': '2025-02-27T17:35:52',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'floret',\n",
       "   'custom_code',\n",
       "   'arxiv:1910.09700',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlibrary_name: transformers\\ntags: []\\n---\\n\\n# Model Card for Model ID\\n\\n<!-- Provide a quick summary of what the model is/does. -->\\n\\n\\n\\n## Model Details\\n\\n### Model Description\\n\\n<!-- Provide a longer summary of what this model is. -->\\n\\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\\n\\n- **Developed by:** [More Information Needed]\\n- **Funded by [optional]:** [More Information Needed]\\n- **Shared by [optional]:** [More Information Needed]\\n- **Model type:** [More Information Needed]\\n- **Language(s) (NLP):** [More Information Needed]\\n- **License:** [More Information Needed]\\n- **Finetuned from model [optional]:** [More Information Needed]\\n\\n### Model Sources [optional]\\n\\n<!-- Provide the basic links for the model. -->\\n\\n- **Repository:** [More Information Needed]\\n- **Paper [optional]:** [More Information Needed]\\n- **Demo [optional]:** [More Information Needed]\\n\\n## Uses\\n\\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\\n\\n### Direct Use\\n\\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\\n\\n[More Information Needed]\\n\\n### Downstream Use [optional]\\n\\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\\n\\n[More Information Needed]\\n\\n### Out-of-Scope Use\\n\\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\\n\\n[More Information Needed]\\n\\n## Bias, Risks, and Limitations\\n\\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\\n\\n[More Information Needed]\\n\\n### Recommendations\\n\\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\\n\\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n## How to Get Started with the Model\\n\\nUse the code below to get started with the model.\\n\\n[More Information Needed]\\n\\n## Training Details\\n\\n### Training Data\\n\\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\\n\\n[More Information Needed]\\n\\n### Training Procedure\\n\\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\\n\\n#### Preprocessing [optional]\\n\\n[More Information Needed]\\n\\n\\n#### Training Hyperparameters\\n\\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\\n\\n#### Speeds, Sizes, Times [optional]\\n\\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\\n\\n[More Information Needed]\\n\\n## Evaluation\\n\\n<!-- This section describes the evaluation protocols and provides the results. -->\\n\\n### Testing Data, Factors & Metrics\\n\\n#### Testing Data\\n\\n<!-- This should link to a Dataset Card if possible. -->\\n\\n[More Information Needed]\\n\\n#### Factors\\n\\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\\n\\n[More Information Needed]\\n\\n#### Metrics\\n\\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\\n\\n[More Information Needed]\\n\\n### Results\\n\\n[More Information Needed]\\n\\n#### Summary\\n\\n\\n\\n## Model Examination [optional]\\n\\n<!-- Relevant interpretability work for the model goes here -->\\n\\n[More Information Needed]\\n\\n## Environmental Impact\\n\\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\\n\\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n\\n- **Hardware Type:** [More Information Needed]\\n- **Hours used:** [More Information Needed]\\n- **Cloud Provider:** [More Information Needed]\\n- **Compute Region:** [More Information Needed]\\n- **Carbon Emitted:** [More Information Needed]\\n\\n## Technical Specifications [optional]\\n\\n### Model Architecture and Objective\\n\\n[More Information Needed]\\n\\n### Compute Infrastructure\\n\\n[More Information Needed]\\n\\n#### Hardware\\n\\n[More Information Needed]\\n\\n#### Software\\n\\n[More Information Needed]\\n\\n## Citation [optional]\\n\\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\\n\\n**BibTeX:**\\n\\n[More Information Needed]\\n\\n**APA:**\\n\\n[More Information Needed]\\n\\n## Glossary [optional]\\n\\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\\n\\n[More Information Needed]\\n\\n## More Information [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Authors [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Contact\\n\\n[More Information Needed]',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1fefcad08a0d12cca63',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.223000',\n",
       "  'date_modified': '2025-02-27T19:18:29.223000'},\n",
       " {'model_identifier': 'hugosousa/smol-135-interval-c-4b664fbd',\n",
       "  'version': 7,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T15:58:38',\n",
       "  'last_modified': '2025-02-27T17:36:04',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'llama', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'ContextClassifier',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.26608696579933167},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1fdfcad08a0d12cca62',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.223000',\n",
       "  'date_modified': '2025-02-27T19:18:29.223000'},\n",
       " {'model_identifier': 'Yangyang127/image_fusion',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:36:07',\n",
       "  'last_modified': '2025-02-27T17:36:07',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1fcfcad08a0d12cca61',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.223000',\n",
       "  'date_modified': '2025-02-27T19:18:29.223000'},\n",
       " {'model_identifier': 'greger43wg/1w21',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:36:49',\n",
       "  'last_modified': '2025-02-27T17:36:49',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['license:llama3.3', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'llama3.3'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: llama3.3\\n---\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1fafcad08a0d12cca60',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.010000',\n",
       "  'date_modified': '2025-02-27T19:18:29.010000'},\n",
       " {'model_identifier': 'FelipeMahlow/1740677843857916',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:37:33',\n",
       "  'last_modified': '2025-02-27T17:37:33',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1f8fcad08a0d12cca5e',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:29.009000',\n",
       "  'date_modified': '2025-02-27T19:18:29.009000'},\n",
       " {'model_identifier': 'TOMFORD79/RTX_C36002',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:28:59',\n",
       "  'last_modified': '2025-02-27T17:38:05',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['any-to-any',\n",
       "   'omega',\n",
       "   'omegalabs',\n",
       "   'bittensor',\n",
       "   'agi',\n",
       "   'license:mit',\n",
       "   'region:us'],\n",
       "  'task': ['any-to-any'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'mit'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'any-to-any',\n",
       "  'description': '---\\nlicense: mit\\ntags:\\n- any-to-any\\n- omega\\n- omegalabs\\n- bittensor\\n- agi\\n---\\n\\nThis is an Any-to-Any model checkpoint for the OMEGA Labs x Bittensor Any-to-Any subnet.\\n\\nCheck out the [git repo](https://github.com/omegalabsinc/omegalabs-anytoany-bittensor) and find OMEGA on X: [@omegalabsai](https://x.com/omegalabsai).\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1f6fcad08a0d12cca5c',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.796000',\n",
       "  'date_modified': '2025-02-27T19:18:28.796000'},\n",
       " {'model_identifier': 'TOMFORD79/RTX_C36003',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:29:05',\n",
       "  'last_modified': '2025-02-27T17:39:11',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['any-to-any',\n",
       "   'omega',\n",
       "   'omegalabs',\n",
       "   'bittensor',\n",
       "   'agi',\n",
       "   'license:mit',\n",
       "   'region:us'],\n",
       "  'task': ['any-to-any'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'mit'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'any-to-any',\n",
       "  'description': '---\\nlicense: mit\\ntags:\\n- any-to-any\\n- omega\\n- omegalabs\\n- bittensor\\n- agi\\n---\\n\\nThis is an Any-to-Any model checkpoint for the OMEGA Labs x Bittensor Any-to-Any subnet.\\n\\nCheck out the [git repo](https://github.com/omegalabsinc/omegalabs-anytoany-bittensor) and find OMEGA on X: [@omegalabsai](https://x.com/omegalabsai).\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1f2fcad08a0d12cca5a',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.692000',\n",
       "  'date_modified': '2025-02-27T19:18:28.692000'},\n",
       " {'model_identifier': 'jmalejandrob79/flrncrsshwf02',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:39:26',\n",
       "  'last_modified': '2025-02-27T17:39:26',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: other\\nlicense_name: flux-1-dev-non-commercial-license\\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md\\nlanguage:\\n- en\\ntags:\\n- flux\\n- diffusers\\n- lora\\n- replicate\\nbase_model: \"black-forest-labs/FLUX.1-dev\"\\npipeline_tag: text-to-image\\n# widget:\\n#   - text: >-\\n#       prompt\\n#     output:\\n#       url: https://...\\ninstance_prompt: flrncrsshwf02\\n---\\n\\n# Flrncrsshwf02\\n\\n<Gallery />\\n\\nTrained on Replicate using:\\n\\nhttps://replicate.com/ostris/flux-dev-lora-trainer/train\\n\\n\\n## Trigger words\\nYou should use `flrncrsshwf02` to trigger the image generation.\\n\\n\\n## Use it with the [🧨 diffusers library](https://github.com/huggingface/diffusers)\\n\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2Image.from_pretrained(\\'black-forest-labs/FLUX.1-dev\\', torch_dtype=torch.float16).to(\\'cuda\\')\\npipeline.load_lora_weights(\\'jmalejandrob79/flrncrsshwf02\\', weight_name=\\'lora.safetensors\\')\\nimage = pipeline(\\'your prompt\\').images[0]\\n```\\n\\nFor more details, including weighting, merging and fusing LoRAs, check the [documentation on loading LoRAs in diffusers](https://huggingface.co/docs/diffusers/main/en/using-diffusers/loading_adapters)\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1f1fcad08a0d12cca59',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.692000',\n",
       "  'date_modified': '2025-02-27T19:18:28.692000'},\n",
       " {'model_identifier': 'sasdvilka/talga_lora_model',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:39:34',\n",
       "  'last_modified': '2025-02-27T17:39:34',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1effcad08a0d12cca57',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.583000',\n",
       "  'date_modified': '2025-02-27T19:18:28.583000'},\n",
       " {'model_identifier': 'FelipeMahlow/17406779727787323',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:39:40',\n",
       "  'last_modified': '2025-02-27T17:39:40',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1eefcad08a0d12cca56',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.583000',\n",
       "  'date_modified': '2025-02-27T19:18:28.583000'},\n",
       " {'model_identifier': 'jmalejandrob79/flrncrsshwf03',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:39:49',\n",
       "  'last_modified': '2025-02-27T17:39:49',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: other\\nlicense_name: flux-1-dev-non-commercial-license\\nlicense_link: https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md\\nlanguage:\\n- en\\ntags:\\n- flux\\n- diffusers\\n- lora\\n- replicate\\nbase_model: \"black-forest-labs/FLUX.1-dev\"\\npipeline_tag: text-to-image\\n# widget:\\n#   - text: >-\\n#       prompt\\n#     output:\\n#       url: https://...\\ninstance_prompt: flrncrsshwf03\\n---\\n\\n# Flrncrsshwf03\\n\\n<Gallery />\\n\\nTrained on Replicate using:\\n\\nhttps://replicate.com/ostris/flux-dev-lora-trainer/train\\n\\n\\n## Trigger words\\nYou should use `flrncrsshwf03` to trigger the image generation.\\n\\n\\n## Use it with the [🧨 diffusers library](https://github.com/huggingface/diffusers)\\n\\n```py\\nfrom diffusers import AutoPipelineForText2Image\\nimport torch\\n\\npipeline = AutoPipelineForText2Image.from_pretrained(\\'black-forest-labs/FLUX.1-dev\\', torch_dtype=torch.float16).to(\\'cuda\\')\\npipeline.load_lora_weights(\\'jmalejandrob79/flrncrsshwf03\\', weight_name=\\'lora.safetensors\\')\\nimage = pipeline(\\'your prompt\\').images[0]\\n```\\n\\nFor more details, including weighting, merging and fusing LoRAs, check the [documentation on loading LoRAs in diffusers](https://huggingface.co/docs/diffusers/main/en/using-diffusers/loading_adapters)\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1edfcad08a0d12cca55',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.583000',\n",
       "  'date_modified': '2025-02-27T19:18:28.583000'},\n",
       " {'model_identifier': 'k0t1k/dubl20',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:33:08',\n",
       "  'last_modified': '2025-02-27T17:40:02',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'qwen2', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'Multiple GPUs or a high-memory GPU (e.g., NVIDIA A100 80GB, H100).',\n",
       "   'memory_in_gb': 61.02747344970703},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1ecfcad08a0d12cca54',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.582000',\n",
       "  'date_modified': '2025-02-27T19:18:28.582000'},\n",
       " {'model_identifier': 'TOMFORD79/RTX_C36004',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:29:10',\n",
       "  'last_modified': '2025-02-27T17:40:03',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['any-to-any',\n",
       "   'omega',\n",
       "   'omegalabs',\n",
       "   'bittensor',\n",
       "   'agi',\n",
       "   'license:mit',\n",
       "   'region:us'],\n",
       "  'task': ['any-to-any'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'mit'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'any-to-any',\n",
       "  'description': '---\\nlicense: mit\\ntags:\\n- any-to-any\\n- omega\\n- omegalabs\\n- bittensor\\n- agi\\n---\\n\\nThis is an Any-to-Any model checkpoint for the OMEGA Labs x Bittensor Any-to-Any subnet.\\n\\nCheck out the [git repo](https://github.com/omegalabsinc/omegalabs-anytoany-bittensor) and find OMEGA on X: [@omegalabsai](https://x.com/omegalabsai).\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1ebfcad08a0d12cca53',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.582000',\n",
       "  'date_modified': '2025-02-27T19:18:28.582000'},\n",
       " {'model_identifier': 'thiomajid/moxe_v0',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:40:42',\n",
       "  'last_modified': '2025-02-27T17:40:42',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1eafcad08a0d12cca52',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.582000',\n",
       "  'date_modified': '2025-02-27T19:18:28.582000'},\n",
       " {'model_identifier': 'TOMFORD79/RTX_C36005',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:29:15',\n",
       "  'last_modified': '2025-02-27T17:40:45',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['any-to-any',\n",
       "   'omega',\n",
       "   'omegalabs',\n",
       "   'bittensor',\n",
       "   'agi',\n",
       "   'license:mit',\n",
       "   'region:us'],\n",
       "  'task': ['any-to-any'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'mit'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'any-to-any',\n",
       "  'description': '---\\nlicense: mit\\ntags:\\n- any-to-any\\n- omega\\n- omegalabs\\n- bittensor\\n- agi\\n---\\n\\nThis is an Any-to-Any model checkpoint for the OMEGA Labs x Bittensor Any-to-Any subnet.\\n\\nCheck out the [git repo](https://github.com/omegalabsinc/omegalabs-anytoany-bittensor) and find OMEGA on X: [@omegalabsai](https://x.com/omegalabsai).\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1e9fcad08a0d12cca51',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.581000',\n",
       "  'date_modified': '2025-02-27T19:18:28.581000'},\n",
       " {'model_identifier': 'jimbowyer123/OtterCountdown',\n",
       "  'version': 48,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-04T23:15:31',\n",
       "  'last_modified': '2025-02-27T17:40:53',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'unsloth', 'license:mit', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'mit'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: mit\\ntags:\\n- unsloth\\n---\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1e8fcad08a0d12cca50',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.475000',\n",
       "  'date_modified': '2025-02-27T19:18:28.475000'},\n",
       " {'model_identifier': 'kyoungmiin/style_13',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:35:05',\n",
       "  'last_modified': '2025-02-27T17:41:35',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['diffusers',\n",
       "   'text-to-image',\n",
       "   'diffusers-training',\n",
       "   'lora',\n",
       "   'template:sd-lora',\n",
       "   'stable-diffusion-xl',\n",
       "   'stable-diffusion-xl-diffusers',\n",
       "   'base_model:stabilityai/stable-diffusion-xl-base-1.0',\n",
       "   'base_model:adapter:stabilityai/stable-diffusion-xl-base-1.0',\n",
       "   'license:openrail++',\n",
       "   'region:us'],\n",
       "  'task': ['text-to-image'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-to-image',\n",
       "  'description': '---\\nbase_model: stabilityai/stable-diffusion-xl-base-1.0\\nlibrary_name: diffusers\\nlicense: openrail++\\ninstance_prompt: sks\\nwidget: []\\ntags:\\n- text-to-image\\n- text-to-image\\n- diffusers-training\\n- diffusers\\n- lora\\n- template:sd-lora\\n- stable-diffusion-xl\\n- stable-diffusion-xl-diffusers\\n---\\n\\n<!-- This model card has been generated automatically according to the information the training script had access to. You\\nshould probably proofread and complete it, then remove this comment. -->\\n\\n\\n# SDXL LoRA DreamBooth - kyoungmiin/style_13\\n\\n<Gallery />\\n\\n## Model description\\n\\nThese are kyoungmiin/style_13 LoRA adaption weights for stabilityai/stable-diffusion-xl-base-1.0.\\n\\nThe weights were trained  using [DreamBooth](https://dreambooth.github.io/).\\n\\nLoRA for the text encoder was enabled: False.\\n\\nSpecial VAE used for training: madebyollin/sdxl-vae-fp16-fix.\\n\\n## Trigger words\\n\\nYou should use sks to trigger the image generation.\\n\\n## Download model\\n\\nWeights for this model are available in Safetensors format.\\n\\n[Download](kyoungmiin/style_13/tree/main) them in the Files & versions tab.\\n\\n\\n\\n## Intended uses & limitations\\n\\n#### How to use\\n\\n```python\\n# TODO: add an example code snippet for running this diffusion pipeline\\n```\\n\\n#### Limitations and bias\\n\\n[TODO: provide examples of latent issues and potential remediations]\\n\\n## Training details\\n\\n[TODO: describe the data used to train the model]',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1e6fcad08a0d12cca4f',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.331000',\n",
       "  'date_modified': '2025-02-27T19:18:28.331000'},\n",
       " {'model_identifier': 'StevenZhang/Wan2.1-T2V-1.3B-Diff',\n",
       "  'version': 3,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T16:09:02',\n",
       "  'last_modified': '2025-02-27T17:41:42',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['diffusers', 'safetensors', 'license:apache-2.0', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: apache-2.0\\n---\\n\\n```\\nimport torch\\nfrom transformers import AutoTokenizer, UMT5EncoderModel\\nfrom diffusers import AutoencoderKLWan, WanPipeline, WanTransformer3DModel, FlowMatchEulerDiscreteScheduler\\nfrom diffusers.schedulers.scheduling_unipc_multistep import UniPCMultistepScheduler\\nfrom diffusers.utils import export_to_video\\nfrom torchvision import transforms\\nimport os\\nimport cv2\\nimport numpy as np\\n\\n\\nfrom pathlib import Path\\nimport json\\nfrom safetensors.torch import safe_open\\n\\ndevice = \"cuda\"\\nseed = 0\\n\\n# TODO: impl AutoencoderKLWan\\nvae = vae.from_pretrained(\"StevenZhang/Wan2.1-VAE_Diff\")\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nvae = vae.to(device)\\n\\n# TODO: impl FlowDPMSolverMultistepScheduler\\nscheduler = UniPCMultistepScheduler(prediction_type=\\'flow_prediction\\', use_flow_sigmas=True, num_train_timesteps=1000, flow_shift=1.0)\\n\\ntext_encoder = UMT5EncoderModel.from_pretrained(\"google/umt5-xxl\", torch_dtype=torch.bfloat16)\\ntokenizer = AutoTokenizer.from_pretrained(\"google/umt5-xxl\")\\n\\n# 14B\\ntransformer = WanTransformer3DModel.from_pretrained(\\'StevenZhang/Wan2.1-T2V-14B-Diff\\', torch_dtype=torch.bfloat16)\\n# transformer = WanTransformer3DModel.from_pretrained(\\'StevenZhang/Wan2.1-T2V-1.3B-Diff\\', torch_dtype=torch.bfloat16)\\n\\ncomponents = {\\n    \"transformer\": transformer,\\n    \"vae\": vae,\\n    \"scheduler\": scheduler,\\n    \"text_encoder\": text_encoder,\\n    \"tokenizer\": tokenizer,\\n}\\npipe = WanPipeline(**components)\\n\\npipe.to(device)\\n\\nnegative_prompt = \\'色调艳丽，过曝，静态，细节模糊不清，字幕，风格，作品，画作，画面，静止，整体发灰，最差质量，低质量，JPEG压缩残留，丑陋的，残缺的，多余的手指，画得不好的手部，画得不好的脸部，畸形的，毁容的，形态畸形的肢体，手指融合，静止不动的画面，杂乱的背景，三条腿，背景人很多，倒着走\\'\\n\\ngenerator = torch.Generator(device=device).manual_seed(seed)\\ninputs = {\\n    \"prompt\": \"两只拟人化的猫咪身穿舒适的拳击装备，戴着鲜艳的手套，在聚光灯照射的舞台上激烈对战\",\\n    \"negative_prompt\": negative_prompt, # TODO\\n    \"generator\": generator,\\n    \"num_inference_steps\": 50,\\n    \"flow_shift\": 3.0,\\n    \"guidance_scale\": 5.0,\\n    \"height\": 480,\\n    \"width\": 832,\\n    \"num_frames\": 81,\\n    \"max_sequence_length\": 512,\\n    \"output_type\": \"np\"\\n}\\n\\nvideo = pipe(**inputs).frames[0]\\n\\nprint(video.shape)\\n\\nexport_to_video(video, \"output.mp4\", fps=16)\\n```\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1e4fcad08a0d12cca4e',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.224000',\n",
       "  'date_modified': '2025-02-27T19:18:28.224000'},\n",
       " {'model_identifier': 'kaitchup/Phi-4-mini-instruct-AutoRoundGPTQ-8bit',\n",
       "  'version': 3,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:39:26',\n",
       "  'last_modified': '2025-02-27T17:41:44',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'phi3',\n",
       "   'text-generation',\n",
       "   'conversational',\n",
       "   'custom_code',\n",
       "   'arxiv:1910.09700',\n",
       "   'autotrain_compatible',\n",
       "   'text-generation-inference',\n",
       "   'endpoints_compatible',\n",
       "   '8-bit',\n",
       "   'gptq',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 4.217535018920898},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nlibrary_name: transformers\\ntags: []\\n---\\n\\n# Model Card for Model ID\\n\\n<!-- Provide a quick summary of what the model is/does. -->\\n\\n\\n\\n## Model Details\\n\\n### Model Description\\n\\n<!-- Provide a longer summary of what this model is. -->\\n\\nThis is the model card of a 🤗 transformers model that has been pushed on the Hub. This model card has been automatically generated.\\n\\n- **Developed by:** [More Information Needed]\\n- **Funded by [optional]:** [More Information Needed]\\n- **Shared by [optional]:** [More Information Needed]\\n- **Model type:** [More Information Needed]\\n- **Language(s) (NLP):** [More Information Needed]\\n- **License:** [More Information Needed]\\n- **Finetuned from model [optional]:** [More Information Needed]\\n\\n### Model Sources [optional]\\n\\n<!-- Provide the basic links for the model. -->\\n\\n- **Repository:** [More Information Needed]\\n- **Paper [optional]:** [More Information Needed]\\n- **Demo [optional]:** [More Information Needed]\\n\\n## Uses\\n\\n<!-- Address questions around how the model is intended to be used, including the foreseeable users of the model and those affected by the model. -->\\n\\n### Direct Use\\n\\n<!-- This section is for the model use without fine-tuning or plugging into a larger ecosystem/app. -->\\n\\n[More Information Needed]\\n\\n### Downstream Use [optional]\\n\\n<!-- This section is for the model use when fine-tuned for a task, or when plugged into a larger ecosystem/app -->\\n\\n[More Information Needed]\\n\\n### Out-of-Scope Use\\n\\n<!-- This section addresses misuse, malicious use, and uses that the model will not work well for. -->\\n\\n[More Information Needed]\\n\\n## Bias, Risks, and Limitations\\n\\n<!-- This section is meant to convey both technical and sociotechnical limitations. -->\\n\\n[More Information Needed]\\n\\n### Recommendations\\n\\n<!-- This section is meant to convey recommendations with respect to the bias, risk, and technical limitations. -->\\n\\nUsers (both direct and downstream) should be made aware of the risks, biases and limitations of the model. More information needed for further recommendations.\\n\\n## How to Get Started with the Model\\n\\nUse the code below to get started with the model.\\n\\n[More Information Needed]\\n\\n## Training Details\\n\\n### Training Data\\n\\n<!-- This should link to a Dataset Card, perhaps with a short stub of information on what the training data is all about as well as documentation related to data pre-processing or additional filtering. -->\\n\\n[More Information Needed]\\n\\n### Training Procedure\\n\\n<!-- This relates heavily to the Technical Specifications. Content here should link to that section when it is relevant to the training procedure. -->\\n\\n#### Preprocessing [optional]\\n\\n[More Information Needed]\\n\\n\\n#### Training Hyperparameters\\n\\n- **Training regime:** [More Information Needed] <!--fp32, fp16 mixed precision, bf16 mixed precision, bf16 non-mixed precision, fp16 non-mixed precision, fp8 mixed precision -->\\n\\n#### Speeds, Sizes, Times [optional]\\n\\n<!-- This section provides information about throughput, start/end time, checkpoint size if relevant, etc. -->\\n\\n[More Information Needed]\\n\\n## Evaluation\\n\\n<!-- This section describes the evaluation protocols and provides the results. -->\\n\\n### Testing Data, Factors & Metrics\\n\\n#### Testing Data\\n\\n<!-- This should link to a Dataset Card if possible. -->\\n\\n[More Information Needed]\\n\\n#### Factors\\n\\n<!-- These are the things the evaluation is disaggregating by, e.g., subpopulations or domains. -->\\n\\n[More Information Needed]\\n\\n#### Metrics\\n\\n<!-- These are the evaluation metrics being used, ideally with a description of why. -->\\n\\n[More Information Needed]\\n\\n### Results\\n\\n[More Information Needed]\\n\\n#### Summary\\n\\n\\n\\n## Model Examination [optional]\\n\\n<!-- Relevant interpretability work for the model goes here -->\\n\\n[More Information Needed]\\n\\n## Environmental Impact\\n\\n<!-- Total emissions (in grams of CO2eq) and additional considerations, such as electricity usage, go here. Edit the suggested text below accordingly -->\\n\\nCarbon emissions can be estimated using the [Machine Learning Impact calculator](https://mlco2.github.io/impact#compute) presented in [Lacoste et al. (2019)](https://arxiv.org/abs/1910.09700).\\n\\n- **Hardware Type:** [More Information Needed]\\n- **Hours used:** [More Information Needed]\\n- **Cloud Provider:** [More Information Needed]\\n- **Compute Region:** [More Information Needed]\\n- **Carbon Emitted:** [More Information Needed]\\n\\n## Technical Specifications [optional]\\n\\n### Model Architecture and Objective\\n\\n[More Information Needed]\\n\\n### Compute Infrastructure\\n\\n[More Information Needed]\\n\\n#### Hardware\\n\\n[More Information Needed]\\n\\n#### Software\\n\\n[More Information Needed]\\n\\n## Citation [optional]\\n\\n<!-- If there is a paper or blog post introducing the model, the APA and Bibtex information for that should go in this section. -->\\n\\n**BibTeX:**\\n\\n[More Information Needed]\\n\\n**APA:**\\n\\n[More Information Needed]\\n\\n## Glossary [optional]\\n\\n<!-- If relevant, include terms and calculations in this section that can help readers understand the model or model card. -->\\n\\n[More Information Needed]\\n\\n## More Information [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Authors [optional]\\n\\n[More Information Needed]\\n\\n## Model Card Contact\\n\\n[More Information Needed]',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1e3fcad08a0d12cca4d',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.224000',\n",
       "  'date_modified': '2025-02-27T19:18:28.224000'},\n",
       " {'model_identifier': 'whiteapple8222/bee8cc68-b77b-4b40-9b35-82cd87a31107_private',\n",
       "  'version': 101,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T14:22:27',\n",
       "  'last_modified': '2025-02-27T17:41:57',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'gemma', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'GemmaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1e2fcad08a0d12cca4c',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.222000',\n",
       "  'date_modified': '2025-02-27T19:18:28.222000'},\n",
       " {'model_identifier': 'asigalov61/Guided-Accompaniment-Transformer',\n",
       "  'version': 9,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-01-22T12:12:23',\n",
       "  'last_modified': '2025-02-27T17:42:14',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['music',\n",
       "   'MIDI',\n",
       "   'accompaniment',\n",
       "   'music ai',\n",
       "   'music transformer',\n",
       "   'en',\n",
       "   'license:apache-2.0',\n",
       "   'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'apache-2.0'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nlicense: apache-2.0\\nlanguage:\\n- en\\ntags:\\n- music\\n- MIDI\\n- accompaniment\\n- music ai\\n- music transformer\\n---\\n\\n# Guided Accompaniment Transformer\\n\\n![Guided-Accompaniment-Transformer-Artwork-1.jpg](https://cdn-uploads.huggingface.co/production/uploads/5f57ea2d3f32f12a3c0692e6/FN9GXN8ZpKqzO0lum8bOs.jpeg)\\n\\n***\\n\\n### Project Los Angeles\\n### Tegridy Code 2025',\n",
       "  'popularity': {'huggingface': {'spaces_count': 2.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 1.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1e1fcad08a0d12cca4b',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.112000',\n",
       "  'date_modified': '2025-02-27T19:18:28.112000'},\n",
       " {'model_identifier': 'FelipeMahlow/17406781757725437',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:43:03',\n",
       "  'last_modified': '2025-02-27T17:43:03',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1e0fcad08a0d12cca4a',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.111000',\n",
       "  'date_modified': '2025-02-27T19:18:28.111000'},\n",
       " {'model_identifier': 'Yunika/muril-squad-nphi',\n",
       "  'version': 3,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T13:30:28',\n",
       "  'last_modified': '2025-02-27T17:43:16',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['tensorboard', 'safetensors', 'bert', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'BertForQuestionAnswering',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 0.8827714920043945},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1dffcad08a0d12cca49',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.111000',\n",
       "  'date_modified': '2025-02-27T19:18:28.111000'},\n",
       " {'model_identifier': 'prithivMLmods/Tadpole-Opus-14B-Exp',\n",
       "  'version': 6,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-25T07:03:38',\n",
       "  'last_modified': '2025-02-27T17:43:28',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['transformers',\n",
       "   'safetensors',\n",
       "   'qwen2',\n",
       "   'text-generation',\n",
       "   'text-generation-inference',\n",
       "   'conversational',\n",
       "   'en',\n",
       "   'zh',\n",
       "   'de',\n",
       "   'base_model:prithivMLmods/Sombrero-Opus-14B-Elite5',\n",
       "   'base_model:finetune:prithivMLmods/Sombrero-Opus-14B-Elite5',\n",
       "   'license:apache-2.0',\n",
       "   'model-index',\n",
       "   'autotrain_compatible',\n",
       "   'endpoints_compatible',\n",
       "   'region:us'],\n",
       "  'task': ['text-generation'],\n",
       "  'architecture': 'Qwen2ForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A workstation GPU with 40GB VRAM (e.g., NVIDIA A100 40GB).',\n",
       "   'memory_in_gb': 27.503721237182617},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': 'text-generation',\n",
       "  'description': '---\\nlicense: apache-2.0\\nlanguage:\\n- en\\n- zh\\n- de\\nbase_model:\\n- prithivMLmods/Sombrero-Opus-14B-Elite5\\npipeline_tag: text-generation\\nlibrary_name: transformers\\ntags:\\n- text-generation-inference\\nmodel-index:\\n- name: Tadpole-Opus-14B-Exp\\n  results:\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: IFEval (0-Shot)\\n      type: wis-k/instruction-following-eval\\n      split: train\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: inst_level_strict_acc and prompt_level_strict_acc\\n      value: 57.5\\n      name: averaged accuracy\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods%2FTadpole-Opus-14B-Exp\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: BBH (3-Shot)\\n      type: SaylorTwift/bbh\\n      split: test\\n      args:\\n        num_few_shot: 3\\n    metrics:\\n    - type: acc_norm\\n      value: 47.78\\n      name: normalized accuracy\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods%2FTadpole-Opus-14B-Exp\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MATH Lvl 5 (4-Shot)\\n      type: lighteval/MATH-Hard\\n      split: test\\n      args:\\n        num_few_shot: 4\\n    metrics:\\n    - type: exact_match\\n      value: 31.34\\n      name: exact match\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods%2FTadpole-Opus-14B-Exp\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: GPQA (0-shot)\\n      type: Idavidrein/gpqa\\n      split: train\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: acc_norm\\n      value: 18.12\\n      name: acc_norm\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods%2FTadpole-Opus-14B-Exp\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MuSR (0-shot)\\n      type: TAUR-Lab/MuSR\\n      args:\\n        num_few_shot: 0\\n    metrics:\\n    - type: acc_norm\\n      value: 18.51\\n      name: acc_norm\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods%2FTadpole-Opus-14B-Exp\\n      name: Open LLM Leaderboard\\n  - task:\\n      type: text-generation\\n      name: Text Generation\\n    dataset:\\n      name: MMLU-PRO (5-shot)\\n      type: TIGER-Lab/MMLU-Pro\\n      config: main\\n      split: test\\n      args:\\n        num_few_shot: 5\\n    metrics:\\n    - type: acc\\n      value: 48.03\\n      name: accuracy\\n    source:\\n      url: https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/?search=prithivMLmods%2FTadpole-Opus-14B-Exp\\n      name: Open LLM Leaderboard\\n---\\n\\n![11.png](https://cdn-uploads.huggingface.co/production/uploads/65bb837dbfb878f46c77de4c/Rfmg8smo9sorEjIQBfGjL.png)\\n\\n# **Tadpole-Opus-14B-Exp**\\n\\nTadpole-Opus-14B-Exp is based on the Qwen 2.5 14B modality architecture, designed to enhance the reasoning capabilities of 14B-parameter models. This model is optimized for general-purpose reasoning and answering, excelling in contextual understanding, logical deduction, and multi-step problem-solving. It has been fine-tuned using a long chain-of-thought reasoning model and specialized datasets to improve comprehension, structured responses, and conversational intelligence.\\n\\nKey improvements include:  \\n1. **Enhanced General Knowledge**: The model provides broad knowledge across various domains, improving capabilities in answering questions accurately and generating coherent responses.  \\n2. **Improved Instruction Following**: Significant advancements in understanding and following complex instructions, generating structured responses, and maintaining coherence over extended interactions.  \\n3. **Versatile Adaptability**: More resilient to diverse prompts, enhancing its ability to handle a wide range of topics and conversation styles, including open-ended and structured inquiries.  \\n4. **Long-Context Support**: Supports up to 128K tokens for input context and can generate up to 8K tokens in a single output, making it ideal for detailed responses.  \\n5. **Multilingual Proficiency**: Supports over 29 languages, including English, Chinese, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.  \\n\\n# **Quickstart with transformers**\\n\\nHere is a code snippet with `apply_chat_template` to show you how to load the tokenizer and model and generate content:\\n\\n```python\\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\\n\\nmodel_name = \"prithivMLmods/Tadpole-Opus-14B-Exp\"\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    torch_dtype=\"auto\",\\n    device_map=\"auto\"\\n)\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\n\\nprompt = \"What are the key principles of general-purpose AI?\"\\nmessages = [\\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant capable of answering a wide range of questions.\"},\\n    {\"role\": \"user\", \"content\": prompt}\\n]\\ntext = tokenizer.apply_chat_template(\\n    messages,\\n    tokenize=False,\\n    add_generation_prompt=True\\n)\\nmodel_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\\n\\ngenerated_ids = model.generate(\\n    **model_inputs,\\n    max_new_tokens=512\\n)\\ngenerated_ids = [\\n    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\\n]\\n\\nresponse = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\\n```\\n\\n# **Intended Use**  \\n1. **General-Purpose Reasoning**:  \\n   Designed for broad applicability, assisting with logical reasoning, answering diverse questions, and solving general knowledge problems.  \\n\\n2. **Educational and Informational Assistance**:  \\n   Suitable for providing explanations, summaries, and research-based responses for students, educators, and general users.  \\n\\n3. **Conversational AI and Chatbots**:  \\n   Ideal for building intelligent conversational agents that require contextual understanding and dynamic response generation.  \\n\\n4. **Multilingual Applications**:  \\n   Supports global communication, translations, and multilingual content generation.  \\n\\n5. **Structured Data Processing**:  \\n   Capable of analyzing and generating structured outputs, such as tables and JSON, useful for data science and automation.  \\n\\n6. **Long-Form Content Generation**:  \\n   Can generate extended responses, including articles, reports, and guides, maintaining coherence over large text outputs.  \\n\\n# **Limitations**  \\n1. **Hardware Requirements**:  \\n   Requires high-memory GPUs or TPUs due to its large parameter size and long-context support.  \\n\\n2. **Potential Bias in Responses**:  \\n   While designed to be neutral, outputs may still reflect biases present in training data.  \\n\\n3. **Inconsistent Outputs in Creative Tasks**:  \\n   May produce variable results in storytelling and highly subjective topics.  \\n\\n4. **Limited Real-World Awareness**:  \\n   Does not have access to real-time events beyond its training cutoff.  \\n\\n5. **Error Propagation in Extended Outputs**:  \\n   Minor errors in early responses may affect overall coherence in long-form outputs.  \\n\\n6. **Prompt Sensitivity**:  \\n   The effectiveness of responses may depend on how well the input prompt is structured.\\n# [Open LLM Leaderboard Evaluation Results](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)\\nDetailed results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/prithivMLmods__Tadpole-Opus-14B-Exp-details)!\\nSummarized results can be found [here](https://huggingface.co/datasets/open-llm-leaderboard/contents/viewer/default/train?q=prithivMLmods%2FTadpole-Opus-14B-Exp&sort[column]=Average%20%E2%AC%86%EF%B8%8F&sort[direction]=desc)!\\n\\n|      Metric       |Value (%)|\\n|-------------------|--------:|\\n|**Average**        |    36.88|\\n|IFEval (0-Shot)    |    57.50|\\n|BBH (3-Shot)       |    47.78|\\n|MATH Lvl 5 (4-Shot)|    31.34|\\n|GPQA (0-shot)      |    18.12|\\n|MuSR (0-shot)      |    18.51|\\n|MMLU-PRO (5-shot)  |    48.03|\\n\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 11.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 10.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1defcad08a0d12cca48',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.111000',\n",
       "  'date_modified': '2025-02-27T19:18:28.111000'},\n",
       " {'model_identifier': 'mrferr3t/1ef09e6c-4fee-4c44-883f-05113007e696',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:52:52',\n",
       "  'last_modified': '2025-02-27T17:43:31',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'mistral', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'MistralForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1ddfcad08a0d12cca47',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.110000',\n",
       "  'date_modified': '2025-02-27T19:18:28.110000'},\n",
       " {'model_identifier': 'Grogros/dmWM-llama-3.2-1B-Instruct-kgw_wmtoken-OWT2WT-DistillationWM-Al4-wmToken2-d4-v1',\n",
       "  'version': 3,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T13:33:52',\n",
       "  'last_modified': '2025-02-27T17:43:37',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['tensorboard', 'safetensors', 'llama', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 4.603782653808594},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1dcfcad08a0d12cca46',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:28.110000',\n",
       "  'date_modified': '2025-02-27T19:18:28.110000'},\n",
       " {'model_identifier': 'MrRobotoAI/165-Q4_K_M-GGUF',\n",
       "  'version': 3,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:43:27',\n",
       "  'last_modified': '2025-02-27T17:43:50',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': True,\n",
       "  'tags': ['transformers',\n",
       "   'gguf',\n",
       "   'mergekit',\n",
       "   'merge',\n",
       "   'llama-cpp',\n",
       "   'gguf-my-repo',\n",
       "   'base_model:MrRobotoAI/165',\n",
       "   'base_model:quantized:MrRobotoAI/165',\n",
       "   'endpoints_compatible',\n",
       "   'region:us',\n",
       "   'conversational'],\n",
       "  'task': [],\n",
       "  'architecture': 'LlamaForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': 'A consumer-grade GPU with 8GB VRAM (e.g., NVIDIA GTX 1080, RTX 3060).',\n",
       "   'memory_in_gb': 7.478763580322266},\n",
       "  'license': {'link': 'None', 'name': 'None', 'type': 'None'},\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': '---\\nbase_model: MrRobotoAI/165\\nlibrary_name: transformers\\ntags:\\n- mergekit\\n- merge\\n- llama-cpp\\n- gguf-my-repo\\n---\\n\\n# MrRobotoAI/165-Q4_K_M-GGUF\\nThis model was converted to GGUF format from [`MrRobotoAI/165`](https://huggingface.co/MrRobotoAI/165) using llama.cpp via the ggml.ai\\'s [GGUF-my-repo](https://huggingface.co/spaces/ggml-org/gguf-my-repo) space.\\nRefer to the [original model card](https://huggingface.co/MrRobotoAI/165) for more details on the model.\\n\\n## Use with llama.cpp\\nInstall llama.cpp through brew (works on Mac and Linux)\\n\\n```bash\\nbrew install llama.cpp\\n\\n```\\nInvoke the llama.cpp server or the CLI.\\n\\n### CLI:\\n```bash\\nllama-cli --hf-repo MrRobotoAI/165-Q4_K_M-GGUF --hf-file 165-q4_k_m.gguf -p \"The meaning to life and the universe is\"\\n```\\n\\n### Server:\\n```bash\\nllama-server --hf-repo MrRobotoAI/165-Q4_K_M-GGUF --hf-file 165-q4_k_m.gguf -c 2048\\n```\\n\\nNote: You can also use this checkpoint directly through the [usage steps](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#usage) listed in the Llama.cpp repo as well.\\n\\nStep 1: Clone llama.cpp from GitHub.\\n```\\ngit clone https://github.com/ggerganov/llama.cpp\\n```\\n\\nStep 2: Move into the llama.cpp folder and build it with `LLAMA_CURL=1` flag along with other hardware-specific flags (for ex: LLAMA_CUDA=1 for Nvidia GPUs on Linux).\\n```\\ncd llama.cpp && LLAMA_CURL=1 make\\n```\\n\\nStep 3: Run inference through the main binary.\\n```\\n./llama-cli --hf-repo MrRobotoAI/165-Q4_K_M-GGUF --hf-file 165-q4_k_m.gguf -p \"The meaning to life and the universe is\"\\n```\\nor \\n```\\n./llama-server --hf-repo MrRobotoAI/165-Q4_K_M-GGUF --hf-file 165-q4_k_m.gguf -c 2048\\n```\\n',\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1dbfcad08a0d12cca45',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:27.991000',\n",
       "  'date_modified': '2025-02-27T19:18:27.991000'},\n",
       " {'model_identifier': 'drdreaddd/895f865a-06fe-4a0d-bacc-ea723ee61bd4',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:49:08',\n",
       "  'last_modified': '2025-02-27T17:44:15',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'mistral', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'MistralForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1dafcad08a0d12cca44',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:27.991000',\n",
       "  'date_modified': '2025-02-27T19:18:27.991000'},\n",
       " {'model_identifier': 'idyrobert/doncorleone-replicate',\n",
       "  'version': 1,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:44:31',\n",
       "  'last_modified': '2025-02-27T17:44:31',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1d9fcad08a0d12cca43',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:27.991000',\n",
       "  'date_modified': '2025-02-27T19:18:27.991000'},\n",
       " {'model_identifier': '3morrrrr/handwriting-synthesis-api',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:25:51',\n",
       "  'last_modified': '2025-02-27T17:44:50',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['region:us'],\n",
       "  'task': [],\n",
       "  'architecture': None,\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1d7fcad08a0d12cca42',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:27.991000',\n",
       "  'date_modified': '2025-02-27T19:18:27.991000'},\n",
       " {'model_identifier': 'mayyteee/328ddebc-a0f7-42c1-b586-349b4cb1bf81',\n",
       "  'version': 2,\n",
       "  'repository': 'https://huggingface.co',\n",
       "  'created_at': '2025-02-27T17:46:07',\n",
       "  'last_modified': '2025-02-27T17:44:58',\n",
       "  'config_version': '2.0.0',\n",
       "  'gguf': False,\n",
       "  'tags': ['safetensors', 'mistral', 'region:us'],\n",
       "  'task': [],\n",
       "  'architecture': 'MistralForCausalLM',\n",
       "  'framework': 'PyTorch',\n",
       "  'domains': None,\n",
       "  'use_cases': None,\n",
       "  'performance': None,\n",
       "  'model_size_parameters': None,\n",
       "  'hardware_requirements': {'recommended_hardware': None,\n",
       "   'memory_in_gb': None},\n",
       "  'license': None,\n",
       "  'status': 'pending_review',\n",
       "  'security': None,\n",
       "  'privacy': None,\n",
       "  'pipeline_tag': None,\n",
       "  'description': None,\n",
       "  'popularity': {'huggingface': {'spaces_count': 0.0,\n",
       "    'downloads': 0.0,\n",
       "    'popularity_score': None,\n",
       "    'likes': 0.0}},\n",
       "  'power_automate_exposed': False,\n",
       "  'ui_path_exposed': False,\n",
       "  'pypi_exposed': False,\n",
       "  'id': '67c0c1d6fcad08a0d12cca41',\n",
       "  'deleted_at': None,\n",
       "  'date_created': '2025-02-27T19:18:27.990000',\n",
       "  'date_modified': '2025-02-27T19:18:27.990000'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.json()['items']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n",
      "PyTorch\n"
     ]
    }
   ],
   "source": [
    "for model in response.json()['items']:\n",
    "    print(model['framework'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:585: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  np.object,\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/utils/import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/pipelines/__init__.py:26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedFeatureExtractor\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseImageProcessor\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/image_processing_utils.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_processing_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchFeature, ImageProcessingMixin\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_transforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m center_crop, normalize, rescale\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChannelDimension, get_image_size\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/image_transforms.py:48\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_flax_available():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/__init__.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LazyLoader \u001b[38;5;28;01mas\u001b[39;00m _LazyLoader\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/__init__.py:46\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/data/__init__.py:25\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AUTOTUNE\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/data/experimental/__init__.py:97\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m service\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatching\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dense_to_ragged_batch\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/data/experimental/service/__init__.py:353\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_service_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_dataset_id\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compat\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compression_ops\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexperimental\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistribute_options\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoShardPolicy\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m structure\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[38;5;28;01mas\u001b[39;00m ged_ops\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwrapt\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nest\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/data/util/nest.py:40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_six\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sparse_tensor \u001b[38;5;28;01mas\u001b[39;00m _sparse_tensor\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_utils\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/sparse_tensor.py:28\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m composite_tensor\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py:29\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m context\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m execute\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meager\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m core\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:585\u001b[0m\n\u001b[1;32m    559\u001b[0m TF_VALUE_DTYPES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(_NP_TO_TF\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    561\u001b[0m _TF_TO_NP \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    562\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_HALF:\n\u001b[1;32m    563\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m    564\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_FLOAT:\n\u001b[1;32m    565\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m    566\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_DOUBLE:\n\u001b[1;32m    567\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[1;32m    568\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT32:\n\u001b[1;32m    569\u001b[0m         np\u001b[38;5;241m.\u001b[39mint32,\n\u001b[1;32m    570\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT8:\n\u001b[1;32m    571\u001b[0m         np\u001b[38;5;241m.\u001b[39muint8,\n\u001b[1;32m    572\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT16:\n\u001b[1;32m    573\u001b[0m         np\u001b[38;5;241m.\u001b[39muint16,\n\u001b[1;32m    574\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT32:\n\u001b[1;32m    575\u001b[0m         np\u001b[38;5;241m.\u001b[39muint32,\n\u001b[1;32m    576\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT64:\n\u001b[1;32m    577\u001b[0m         np\u001b[38;5;241m.\u001b[39muint64,\n\u001b[1;32m    578\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT16:\n\u001b[1;32m    579\u001b[0m         np\u001b[38;5;241m.\u001b[39mint16,\n\u001b[1;32m    580\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT8:\n\u001b[1;32m    581\u001b[0m         np\u001b[38;5;241m.\u001b[39mint8,\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# NOTE(touts): For strings we use np.object as it supports variable length\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;66;03m# strings.\u001b[39;00m\n\u001b[1;32m    584\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_STRING:\n\u001b[0;32m--> 585\u001b[0m         \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobject\u001b[49m,\n\u001b[1;32m    586\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX64:\n\u001b[1;32m    587\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex64,\n\u001b[1;32m    588\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX128:\n\u001b[1;32m    589\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex128,\n\u001b[1;32m    590\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT64:\n\u001b[1;32m    591\u001b[0m         np\u001b[38;5;241m.\u001b[39mint64,\n\u001b[1;32m    592\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BOOL:\n\u001b[1;32m    593\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool_,\n\u001b[1;32m    594\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT8:\n\u001b[1;32m    595\u001b[0m         _np_qint8,\n\u001b[1;32m    596\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT8:\n\u001b[1;32m    597\u001b[0m         _np_quint8,\n\u001b[1;32m    598\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT16:\n\u001b[1;32m    599\u001b[0m         _np_qint16,\n\u001b[1;32m    600\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT16:\n\u001b[1;32m    601\u001b[0m         _np_quint16,\n\u001b[1;32m    602\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT32:\n\u001b[1;32m    603\u001b[0m         _np_qint32,\n\u001b[1;32m    604\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BFLOAT16:\n\u001b[1;32m    605\u001b[0m         _np_bfloat16,\n\u001b[1;32m    606\u001b[0m \n\u001b[1;32m    607\u001b[0m     \u001b[38;5;66;03m# Ref types\u001b[39;00m\n\u001b[1;32m    608\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_HALF_REF:\n\u001b[1;32m    609\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m    610\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_FLOAT_REF:\n\u001b[1;32m    611\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat32,\n\u001b[1;32m    612\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_DOUBLE_REF:\n\u001b[1;32m    613\u001b[0m         np\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[1;32m    614\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT32_REF:\n\u001b[1;32m    615\u001b[0m         np\u001b[38;5;241m.\u001b[39mint32,\n\u001b[1;32m    616\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT32_REF:\n\u001b[1;32m    617\u001b[0m         np\u001b[38;5;241m.\u001b[39muint32,\n\u001b[1;32m    618\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT8_REF:\n\u001b[1;32m    619\u001b[0m         np\u001b[38;5;241m.\u001b[39muint8,\n\u001b[1;32m    620\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT16_REF:\n\u001b[1;32m    621\u001b[0m         np\u001b[38;5;241m.\u001b[39muint16,\n\u001b[1;32m    622\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT16_REF:\n\u001b[1;32m    623\u001b[0m         np\u001b[38;5;241m.\u001b[39mint16,\n\u001b[1;32m    624\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT8_REF:\n\u001b[1;32m    625\u001b[0m         np\u001b[38;5;241m.\u001b[39mint8,\n\u001b[1;32m    626\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_STRING_REF:\n\u001b[1;32m    627\u001b[0m         np\u001b[38;5;241m.\u001b[39mobject,\n\u001b[1;32m    628\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX64_REF:\n\u001b[1;32m    629\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex64,\n\u001b[1;32m    630\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_COMPLEX128_REF:\n\u001b[1;32m    631\u001b[0m         np\u001b[38;5;241m.\u001b[39mcomplex128,\n\u001b[1;32m    632\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_INT64_REF:\n\u001b[1;32m    633\u001b[0m         np\u001b[38;5;241m.\u001b[39mint64,\n\u001b[1;32m    634\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_UINT64_REF:\n\u001b[1;32m    635\u001b[0m         np\u001b[38;5;241m.\u001b[39muint64,\n\u001b[1;32m    636\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BOOL_REF:\n\u001b[1;32m    637\u001b[0m         np\u001b[38;5;241m.\u001b[39mbool,\n\u001b[1;32m    638\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT8_REF:\n\u001b[1;32m    639\u001b[0m         _np_qint8,\n\u001b[1;32m    640\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT8_REF:\n\u001b[1;32m    641\u001b[0m         _np_quint8,\n\u001b[1;32m    642\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT16_REF:\n\u001b[1;32m    643\u001b[0m         _np_qint16,\n\u001b[1;32m    644\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QUINT16_REF:\n\u001b[1;32m    645\u001b[0m         _np_quint16,\n\u001b[1;32m    646\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_QINT32_REF:\n\u001b[1;32m    647\u001b[0m         _np_qint32,\n\u001b[1;32m    648\u001b[0m     types_pb2\u001b[38;5;241m.\u001b[39mDT_BFLOAT16_REF:\n\u001b[1;32m    649\u001b[0m         _np_bfloat16,\n\u001b[1;32m    650\u001b[0m }\n\u001b[1;32m    652\u001b[0m _QUANTIZED_DTYPES_NO_REF \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfrozenset\u001b[39m([qint8, quint8, qint16, quint16, qint32])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/numpy/__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline \n\u001b[1;32m      4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m pipeline(task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage-classification\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvit-base-patch16-224/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m pipe \n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/utils/import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1849\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/transformers/utils/import_utils.py:1865\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1866\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1867\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1868\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nmodule 'numpy' has no attribute 'object'.\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "\n",
    "pipe = pipeline(task='image-classification', model='vit-base-patch16-224/')\n",
    "\n",
    "pipe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
